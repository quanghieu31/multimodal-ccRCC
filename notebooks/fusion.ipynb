{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PatientClinicalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    from csv, so getitem would be something like .loc[idx]\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file_path):\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.df = pd.read_csv(self.csv_file_path).drop([\"time\", \"event\"], axis=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_series = self.df.iloc[idx]\n",
    "        return patient_series # includes the submitter_id!\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "class PatientRNASeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    a csv file, 534 rows and ~20000 columns for normalized RNA-seq counts\n",
    "    \"\"\"\n",
    "    def __init__(self, rna_file_path):\n",
    "        self.rna_file_path = rna_file_path\n",
    "        self.df = pd.read_csv(self.rna_file_path)\n",
    "        self.df.set_index(\"submitter_id\", inplace=True)\n",
    "\n",
    "    def __getitem__(self, case_id):\n",
    "        gene_expressions = list(self.df.loc[case_id])\n",
    "        tensor_gene_expressions = torch.tensor(gene_expressions, dtype=torch.float32).unsqueeze(0)\n",
    "        return tensor_gene_expressions # [1, 19962]\n",
    "\n",
    "\n",
    "class PatientWSIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for accessing a patient's list of patches features, each is of shape (1, n_patches, n_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, wsi_dir):\n",
    "\n",
    "        self.wsi_dir = wsi_dir\n",
    "        self.case_ids = list(os.listdir(self.wsi_dir))\n",
    "        self.dict_case_id_path = {\n",
    "            c: os.path.join(self.wsi_dir, c) + \"/patches_features.npy\" for c in self.case_ids\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, case_id):\n",
    "        # grab the list of 5 clusters for this case_id\n",
    "\n",
    "        case_npy_file = self.dict_case_id_path[case_id]\n",
    "        patches_features = np.load(case_npy_file, allow_pickle=True).item()\n",
    "        \n",
    "        cluster_ids = self.clustering(patches_features)\n",
    "\n",
    "        features_list = list(patches_features.values())\n",
    "        unique_clusters = np.unique(cluster_ids)\n",
    "        n_clusters = len(unique_clusters)\n",
    "\n",
    "        list_phenotype_tensors = [] # list of tensors, each tensor is a cluster's features of shape i.e. (1, 15 patches in this cluster, 512 as output of resnet18)\n",
    "\n",
    "        for cluster in unique_clusters:\n",
    "            cluster_features = [features for features, c in zip(features_list, cluster_ids) if c == cluster]\n",
    "            tensor_cluster_features = torch.from_numpy(np.array(cluster_features)).float().unsqueeze(0) # (1, n_patches, n_features)\n",
    "\n",
    "            list_phenotype_tensors.append(tensor_cluster_features.to(device))\n",
    "\n",
    "        return list_phenotype_tensors # [t1,t2,t3,t4,t5]\n",
    "\n",
    "    def clustering(self, patches_features, n_clusters=5):\n",
    "        feature_vectors = list(patches_features.values())\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        cluster_ids = kmeans.fit_predict(feature_vectors)\n",
    "        return cluster_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.case_ids)\n",
    "\n",
    "\n",
    "\n",
    "## Fusion multimodal\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    takes three data paths (clinical, rna-seq, histopath images)\n",
    "    build a data out of 'em\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        clinical_data_path, \n",
    "        rna_seq_data_path, \n",
    "        wsi_data_path\n",
    "    ):\n",
    "        # prepare labels from the clinical data path\n",
    "        self.LABELS_DF = pd.read_csv(clinical_data_path)[[\"submitter_id\", \"event\", \"time\"]]\n",
    "        # then by initializing the clinical_dataset, remove the time and event from the clinical features:\n",
    "        self.clinical_dataset = PatientClinicalDataset(clinical_data_path)\n",
    "\n",
    "        # initialize the datasets for each modality\n",
    "        self.wsi_dataset = PatientWSIDataset(wsi_data_path)\n",
    "        self.rna_dataset = PatientRNASeqDataset(rna_seq_data_path)\n",
    "\n",
    "        # label dictionary with key=submitter_id and value=(event,time) for easy lookup\n",
    "        self.labels_dict = {}\n",
    "        for submitter_id, event, time in zip(self.LABELS_DF[\"submitter_id\"], self.LABELS_DF[\"event\"], self.LABELS_DF[\"time\"]):\n",
    "            self.labels_dict[submitter_id] = {\"event\": event, \"time\": time}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clinical_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # (1) start from clinical dataset\n",
    "        patient_series = self.clinical_dataset[idx]\n",
    "        case_id = patient_series[\"submitter_id\"]\n",
    "        clinical_features = list(patient_series.drop([\"submitter_id\"]))\n",
    "        tensor_clinical_features = torch.tensor(clinical_features, dtype=torch.float32).unsqueeze(0) \n",
    "        # above: add batch dim (1, 13) instead of (13)\n",
    "\n",
    "        # (2) grab the tensor for 20000 (processed) gene counts for that case id\n",
    "        tensor_rna_genes = self.rna_dataset[case_id] # (1, 19962)\n",
    "\n",
    "        # (2.5) NOTE: to save time for this moment, I will concat the clinical and rna together \n",
    "        # and build one feed-forward for the combined\n",
    "        tensor_clinical_rna = torch.cat((tensor_clinical_features, tensor_rna_genes), dim=1) # (1, 19975)\n",
    "\n",
    "        # (3) collect the list of phenotype tensor for that case id\n",
    "        list_of_phenotype_tensors = self.wsi_dataset[case_id]\n",
    "\n",
    "        # (4) labels\n",
    "        time = self.labels_dict[case_id][\"time\"]\n",
    "        event = self.labels_dict[case_id][\"event\"]\n",
    "\n",
    "        return (\n",
    "            tensor_clinical_rna,\n",
    "            list_of_phenotype_tensors,\n",
    "            time,\n",
    "            event\n",
    "        )\n",
    "\n",
    "        # return (\n",
    "        #     tensor_clinical_features, \n",
    "        #     tensor_rna_genes, \n",
    "        #     list_of_phenotype_tensors,\n",
    "        #     time,\n",
    "        #     event\n",
    "        # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = MultimodalDataset(\n",
    "    config[\"clinical\"][\"cleaned_clinical_json\"],\n",
    "    config[\"rna\"][\"cleaned_rna\"],\n",
    "    config[\"wsi\"][\"wsi_slides\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 first gene counts:\n",
      "tensor(2.4332)\n",
      "tensor(0.)\n",
      "tensor(3.4071)\n",
      "tensor(2.7216)\n",
      "tensor(1.6839)\n",
      "tensor(1.7558)\n",
      "tensor(3.9939)\n",
      "clinical:\n",
      "tensor(0.)\n",
      "tensor(1.1109)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "case0 = check_data[5]\n",
    "clin_rna, list_tensors = case0[0], case0[1]\n",
    "\n",
    "print(\"5 first gene counts:\")\n",
    "for i in clin_rna.flatten()[13:20]:\n",
    "    print(i)\n",
    "\n",
    "print(\"clinical:\")\n",
    "for i in clin_rna.flatten()[0:13]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1148, 512])\n",
      "torch.Size([1, 1078, 512])\n",
      "torch.Size([1, 984, 512])\n",
      "torch.Size([1, 966, 512])\n",
      "torch.Size([1, 824, 512])\n",
      "\n",
      "torch.Size([1, 1148, 512])\n",
      "torch.Size([1, 1078, 512])\n",
      "torch.Size([1, 984, 512])\n",
      "torch.Size([1, 966, 512])\n",
      "torch.Size([1, 824, 512])\n"
     ]
    }
   ],
   "source": [
    "for tensor in list_tensors:\n",
    "    print(tensor.shape)\n",
    "\n",
    "print()\n",
    "check_wsi = PatientWSIDataset(config[\"wsi\"][\"wsi_slides\"])[\"TCGA-BP-4352\"]\n",
    "for tensor in check_wsi:\n",
    "    print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class WSI_FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2009.11169\n",
    "    fully convolutional network for WSI\n",
    "    takes 1 phenotype tensor/cluster of shape (1, n_patches, 512)\n",
    "    outputs a local representation of that phenotype tensor of shape (1, 64)\n",
    "    why FCN? on the numerical vectors? \n",
    "        - utilize the kernel, and especially kernel_size=1 because we can't have kernel_size>1 for randomly picked patches from the histopathology slides\n",
    "        - so why not a simple fully connected network (MLP)? it's because it requires inputs with fixed dimension and we have varying number of patches for each cluster\n",
    "    also, note that a patch -> FCN -> (1,64) shape. So if we have 300 patches or (300,64) shape, we would use avgpooling and get (1,64) as the final output for that cluster\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features=64):\n",
    "        super(WSI_FCN, self).__init__()\n",
    "        # conv1d because we only have a tensor of shape (N, C, L) = (1, 512, i.e. 272)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, \n",
    "            kernel_size=1 # kernel size = 1 is extremely important because we only want to the a single patch to be learned, \n",
    "            # doing i.e. 3x3 is no use because the patches are picked randomly, so can't use spatial relationship here\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        # adaptive avg pooling to get a local representation of the phenotype tensor\n",
    "        # NOTE: adapative pooling from (64, 300 patches) to (64,1) as the final output of that cluster\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, n_patches, n_features)\n",
    "        # permute to (1, n_features, n_patches) so that n_features become channels why? because tensor in pytorch reads () https://stackoverflow.com/questions/51541532/which-part-of-pytorch-tensor-represents-channels\n",
    "        # n_patches is the length of the sequence. why?\n",
    "        # FYI: for a conv2D, input should be in (N, C, H, W) format. N is the number of samples/batch_size. C is the channels. H and W are height and width resp: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \n",
    "        # but here we have conv1d: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "\n",
    "        x = x.permute(0, 2, 1) # (1, 512, 300 patches)\n",
    "        x = self.conv(x) # (1, 64, 300 patches)\n",
    "        x = self.relu(x) # (1, 64, 300 patches)\n",
    "        x = self.pool(x) # (1, 64, 1)\n",
    "        x = x.view(x.size()[0], -1) # (1, 64)\n",
    "        return x # (1, 64)\n",
    "\n",
    "\n",
    "class WSI_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2009.11169 \n",
    "    pooling attention mechanism for WSI\n",
    "    takes a local representation of the phenotype tensor of shape (5, 64) in which 5 is the number of clusters\n",
    "    outputs a global representation of the phenotype tensors of shape (64-dim) which is a weighted sum across 5 clusters for 64 features\n",
    "        each case has a global representation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features=64):\n",
    "        super(WSI_Attention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Tanh(),  # tanh because we want to normalize the weights\n",
    "            # why tanh() >> output values in range (-1,1), allowing both neg and pos values, often used in attention scores\n",
    "            nn.Linear(out_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply softmax because we have different number of clusters for each case\n",
    "        # x: (5, 64) stack representation of 5 clusters/phenotypes\n",
    "        scores = self.attention(x) # (5, 1)\n",
    "        att_weights = torch.softmax(scores, dim=0).T # (1,5) which is probabilities\n",
    "        # weighted sum across the 5 clusters:\n",
    "        weights_applied = att_weights @ x  # (5, 64) = (1,5) @ (5,64)\n",
    "        # weighted_sum_vector = torch.sum(weights_applied, dim=0) # (1, 64) or (64)\n",
    "        return weights_applied, att_weights\n",
    "\n",
    "\n",
    "class Clinical_RNA_FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=32, dropout_ratio=dropout_ratio):\n",
    "        # https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "        # https://arxiv.org/pdf/1207.0580\n",
    "        # For fully connected layers, dropout in all hidden layers works\n",
    "        # better than dropout in only one hidden layer and more extreme probabilities tend to be worse,\n",
    "        # which is why we have used 0.5 throughout this paper\n",
    "        \n",
    "        super(Clinical_RNA_FeedForward, self).__init__()\n",
    "\n",
    "        hidden = [512, 256, 256, 64, 64, 32]\n",
    "        hidden = [1024, 512, 256, 128, 64, 32]\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[1], hidden[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[2], hidden[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[3], hidden[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),   \n",
    "            nn.Linear(hidden[4], hidden[5]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),      \n",
    "            nn.Linear(hidden[5], output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_ratio),    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feedforward(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# FusionFeedForward\n",
    "class FusionNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "        input_dim_clinical_rna=19975, \n",
    "        input_dim_wsi_fcn=512, \n",
    "        input_dim_wsi_attention=64, \n",
    "        input_dim_final=96  # as from 32+64 = (out_dim of clinical_RNA) + (out_dim of WSI)\n",
    "    ): \n",
    "        # NOTE: no dropout for now\n",
    "        super(FusionNetwork, self).__init__()\n",
    "\n",
    "        # Clinical+RNA\n",
    "        self.clinical_rna_feedforward = Clinical_RNA_FeedForward(input_dim_clinical_rna, output_dim=32, dropout_ratio=dropout_ratio)\n",
    "        # WSI_FCN and WSI_Attention\n",
    "        self.wsi_fcn = WSI_FCN(input_dim_wsi_fcn, out_features=64)\n",
    "        self.attention = WSI_Attention(input_dim_wsi_attention, out_features=64)\n",
    "\n",
    "        # after fusion:\n",
    "        # TODO: rational -> book: many hidden neurons are good -> with regularization like dropout/weight decay\n",
    "        # for no. layers -> background knowledge and experimentation, for now 4 layers\n",
    "        hidden = [64, 32, 16, 8]\n",
    "\n",
    "        self.baby_feed_forward = nn.Sequential(\n",
    "            nn.Linear(input_dim_final, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[1], hidden[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[2], hidden[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[3], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_clinical_rna, list_of_phenotype_tensors):\n",
    "\n",
    "        # Clinical+RNA:\n",
    "        extracted_clinical_rna = self.clinical_rna_feedforward(tensor_clinical_rna.to(device)) # (1, 32) shape\n",
    "        \n",
    "        # WSI_FCN\n",
    "        local_reps = [] # len=5\n",
    "        # here since tensors have different no. images in each of them\n",
    "        # we use the \"flexiblity\" of the FCN to output 1x64 for each cluster\n",
    "        for tensor in list_of_phenotype_tensors:\n",
    "            tensor = tensor.to(device)\n",
    "            cluster_rep = self.wsi_fcn(tensor) # each of shape (1,64) by pooling from tensors with varying dim\n",
    "            local_reps.append(cluster_rep)\n",
    "        # stack 5 local representation of shape (1,64) >> tensor of shape (5,64)\n",
    "        tensor_local_reps = torch.cat(local_reps)\n",
    "\n",
    "        # WSI_Attention:\n",
    "        wsi_aggregated_vector, att_weights = self.attention(tensor_local_reps) # from (5,64) to weighted vector (1,64)\n",
    "\n",
    "        # concantenate:\n",
    "        concatenated_features = torch.cat((extracted_clinical_rna, wsi_aggregated_vector), dim=1) # shape (1, 96)\n",
    "\n",
    "        risk_score = self.baby_feed_forward(concatenated_features)\n",
    "        return risk_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2094]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " torch.Size([1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "x = torch.rand(5, 64)\n",
    "a2 = WSI_Attention(64)(x)\n",
    "print(a2[0].shape)\n",
    "\n",
    "x = torch.rand(1, 300, 512) # (1, n_patches, 512)\n",
    "a1 = WSI_FCN(512)(x)\n",
    "print(a1.shape)\n",
    "\n",
    "x = torch.rand(1, 19975)\n",
    "a3 = Clinical_RNA_FeedForward(19975)(x)\n",
    "print(a3.shape)\n",
    "\n",
    "x1 = torch.rand(1, 19975)\n",
    "x2 = [torch.rand(1, 300, 512), torch.rand(1, 200, 512), torch.rand(1, 50, 512), torch.rand(1, 150, 512), torch.rand(1, 25, 512)]\n",
    "m = FusionNetwork().to(device)\n",
    "a4 = m(x1,x2)\n",
    "a4, a4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "begin to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [14:20<00:00, 71.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 1.13406045238177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [15:24<00:00, 77.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 1.163498858610789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [09:55<00:00, 49.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 1.1341417630513508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [10:22<00:00, 51.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 1.1359817037979762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [10:33<00:00, 52.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 1.0893761763970058\n",
      "finished training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from lifelines.utils import concordance_index\n",
    "from utils import display_km_curves_fusion\n",
    "\n",
    "# from models import *\n",
    "# from data_utils import *\n",
    "\n",
    "# import configparser\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read(\"config.ini\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "########################## LOSS ###############################################\n",
    "\n",
    "def negative_partial_log_likelihood(hazard_preds, times, events, device, eps=1e-8):\n",
    "\n",
    "    # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet\n",
    "    # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data\n",
    "    # flatten predictions\n",
    "\n",
    "    hazard_preds = hazard_preds.view(-1)\n",
    "    times = times.to(device, dtype=torch.float).view(-1)\n",
    "    events = events.to(device, dtype=torch.float).view(-1)\n",
    "\n",
    "    if events.sum() == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    # compute risk set: R[i, j] = 1 if times[j] >= times[i]\n",
    "    # https://stackoverflow.com/questions/56646261/can-someone-please-explain-np-less-equal-outerrange1-18-range1-13\n",
    "    R_mat = torch.tensor(\n",
    "        np.greater_equal.outer(times.cpu(), times.cpu()).T.astype(np.float32), \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # standardize theta/hazard prediction\n",
    "    theta = (hazard_preds - hazard_preds.mean()) / (hazard_preds.std(unbiased=False) + eps)\n",
    "\n",
    "    # compute the log risk set using the correct formula\n",
    "    # NOTE: use theta directly without an extra exp()\n",
    "    # First, mask the non-risk set entries by multiplying exp(theta) with R_mat,\n",
    "    # then take the log of the sum\n",
    "    log_risk_set = torch.log(torch.sum(torch.exp(theta) * R_mat, dim=1) + eps)\n",
    "\n",
    "    # negative partial likelihood only for events\n",
    "    loss = -torch.mean((theta - log_risk_set) * events)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# one-batch\n",
    "# hazard_pred = torch.tensor([2.1, 1.8, 3.0, 0.5, 2.5], device=device)\n",
    "# time = torch.tensor([5, 3, 6, 2, 4], device=device)\n",
    "# event = torch.tensor([1, 1, 0, 1, 0], device=device)\n",
    "\n",
    "# one_batch_loss = negative_partial_log_likelihood(hazard_pred, time, event, device)\n",
    "# print(one_batch_loss)\n",
    "\n",
    "################## HYPERPARAMS ################################################\n",
    "\n",
    "# https://arxiv.org/pdf/1206.5533 (guide to choose hyperparams)\n",
    "n_epochs = 5\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "# regularizations:\n",
    "dropout_ratio = 0.5\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# since we have relatively small dataset (~300 for training), high weidght decay may lead to udnerfitting\n",
    "# but we might have many interactions between parameters in the final feedforward, so let's try different ones\n",
    "# https://medium.com/towards-data-science/this-thing-called-weight-decay-a7cd4bcfccab\n",
    "# https://stackoverflow.com/questions/44452571/what-is-the-proper-way-to-weight-decay-for-adam-optimizer\n",
    "\n",
    "################### TRAIN #####################################################\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders \n",
    "    i.e. 32 batch_size\n",
    "    batch = [\n",
    "        (list_of_phenotype_tensors, time1, event1),    => case 1\n",
    "        (list_of_phenotype_tensors, time2, event2),    => case 2\n",
    "        ...                                            => case 32\n",
    "    ]\n",
    "\n",
    "    TODO: more explanation to come\n",
    "    \"\"\"\n",
    "    # each element in batch is a tuple: \n",
    "    # (clinical_rna_tensor, list_of_phenotype_tensors, time, event)\n",
    "    list_of_clinical_rna_features, list_of_lists_of_5_tensors, times, events = zip(*batch)\n",
    "    # i.e. [patient_1_clinical_rna_t1, patient_2_clinical_rna_t2,..., patient32_clinical_rna_t32]\n",
    "    # i.e. [[t1,t2,t3,t4,t5],[t1,t2,t3,t4,t5],...,[t1,t2,t3,t4,t5]] = [32 lists]\n",
    "    \n",
    "    return (\n",
    "        list(list_of_clinical_rna_features),\n",
    "        list(list_of_lists_of_5_tensors),\n",
    "        torch.tensor(times),\n",
    "        torch.tensor(events)\n",
    "    )\n",
    "\n",
    "# dataset and splitting\n",
    "dataset = MultimodalDataset(\n",
    "    config[\"clinical\"][\"cleaned_clinical_json\"],\n",
    "    config[\"rna\"][\"cleaned_rna\"],\n",
    "    config[\"wsi\"][\"wsi_slides\"]\n",
    ")\n",
    "\n",
    "train_size, val_size = int(0.7 * len(dataset)), int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train, val, test = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# loading data\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val, batch_size=val_size, shuffle=False, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test, batch_size=test_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# initiate model\n",
    "model = FusionNetwork(\n",
    "    input_dim_clinical_rna=19975,\n",
    "    input_dim_wsi_fcn=512,\n",
    "    input_dim_wsi_attention=64,\n",
    "    input_dim_final=96\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "print(\"begin to train\")\n",
    "# training loops\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0 \n",
    "\n",
    "    # each batch contains 32 cases!\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_clinical_rna_features, batch_lists_phenotype_clusters, batch_times, batch_events = batch\n",
    "\n",
    "        risk_scores = [] # list of  32 risk scores\n",
    "        for (clinical_rna_features, list_of_phenotype_tensors) in zip(batch_clinical_rna_features, batch_lists_phenotype_clusters):\n",
    "            # process each sample in the batch of 32\n",
    "            risk_score = model(clinical_rna_features, list_of_phenotype_tensors)\n",
    "            risk_scores.append(risk_score)\n",
    "\n",
    "        # convert to tensor type\n",
    "        risk_scores = torch.stack(risk_scores) # of shape (batch_size, 1) or (32,1) \n",
    "\n",
    "        # TODO: explain in detail: meaning of loss of 32 cases in the batch\n",
    "        optimizer.zero_grad() # zero the parameter gradients\n",
    "        loss = negative_partial_log_likelihood(risk_scores, batch_times.to(device), batch_events.to(device), device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch}, loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"finished training\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:11<00:00, 131.09s/it]\n"
     ]
    }
   ],
   "source": [
    "####################### VALIDATION ############################################\n",
    "\n",
    "print(\"begin to validate\")\n",
    "model.eval()\n",
    "\n",
    "val_risks = []\n",
    "val_times = []\n",
    "val_events = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        # unpack the batch\n",
    "        batch_clinical_rna_features, batch_lists_phenotype_clusters, batch_times, batch_events = batch\n",
    "        \n",
    "        # move times and events to the device\n",
    "        batch_times = batch_times.to(device)\n",
    "        batch_events = batch_events.to(device)\n",
    "        \n",
    "        # tterate over each sample in the batch\n",
    "        for idx, (clinical_rna_features, list_of_phenotype_tensors) in enumerate(zip(batch_clinical_rna_features, batch_lists_phenotype_clusters)):\n",
    "            \n",
    "            risk_score = model(clinical_rna_features, list_of_phenotype_tensors)\n",
    "            \n",
    "            val_risks.append(risk_score.item())\n",
    "            val_times.append(batch_times[idx].item())\n",
    "            val_events.append(batch_events[idx].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation c-index: 0.6721470019342359\n",
      "validation c-index custom: 0.6724806201550387\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAK9CAYAAAAT0TyCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcK5JREFUeJzt3Ql8E2X+x/FfeqYFCqXlFgERoSJQ8ARXcRVEWHXxRHEFUfHvAV7ruasguAoe6wWou6i7664orveqiweCroqKVESxgAcIKkc5C23T9Jj/6/eElLRNO0nJ0SSf9+sVkk4mmSeTSZhvnmd+47AsyxIAAAAAQIOSGr4LAAAAAKAITgAAAABgg+AEAAAAADYITgAAAABgg+AEAAAAADYITgAAAABgg+AEAAAAADYITgAAAABgg+AEAAAAADYITgBsrVu3ThwOh/z973+XeKSvS1+fvk4Eb8GCBZKfny9Op9Osx507d4Z1eUuXLpUhQ4ZIixYtzPKWL18ud9xxh7kdTosXLzbL0Ot4/bzGymtsKn/bSffu3eWiiy6KyvdEvH+3AvGG4ATECO9/2p9//nmt6bt27ZKjjjrK7LTqDmw80tetl0svvdTv/X/84x9r5tm6dWvE25fItm3bJueee65kZGTInDlz5J///KcJNOFSUVEh55xzjmzfvl0efPBBs7xu3bqFbXlAKMybN08eeughiVfx/voAr5SaWwBiTnFxsZx88smyYsUKefnll+WUU06ReKXB8MUXX5RHH31U0tLSat337LPPmvtdLleTnvvCCy+U8847T9LT00PU2sShvT+7d++WO++8U4YNGxb25X3//ffy448/yty5c2sF6dtuu01uueWWsC8f8Wf16tWSlJQU9mDx9ddfy7XXXltruob+srIySU1NlVjW0OsD4g09TkCM0p3VESNGmGFKGihGjhwp8UxDoQbF//73v7Wmf/zxx7J27Vr5zW9+0+TnTk5OrhlmFgolJSUSLZZlmR2xSNmyZYu5btOmTcies7H119DyUlJSzHsIBEt/MIlWcNHvHN1u9TsIQPNHcAJi0J49e0yQKCgoMKGpbmh49dVXzbTOnTubnYKePXuaHoGqqqpa851wwgly2GGHybJly8wxIzrcqkePHvL444/btkF7ufS4gIMOOsj8x9+xY0e5+OKLzdAtf8cUfPfdd2Z+3eFt3bq1TJgwQUpLSwN+zV26dJHjjz/e/LLp65lnnpF+/fqZ1+HPp59+ataVLjMzM1OGDh0qH330UUDHLmhIO+6448zQs1atWpl1unLlylrz6Gtq2bKl6QkZNWqUme+CCy5o9LX8/PPPcskll9S8P7rOr7jiCnG73bXWWV3+2qnHZ5x66qny1ltvyRFHHGHew7/85S9mffz617+u9xzV1dVmXZ599tm1pukwm759+5r3skOHDvJ///d/smPHjkZfh24/48ePN7ePPPJI0zbfY0X+/e9/y+GHH27alJubK7/73e/Ma2/q+tN59f1TOlxPl6dtaGid6d+TJk2SV155xawPXdf6GusOadUerCuvvFJ69+5t2pqTk2Oev6nHsnjbsmbNGvOaddtr166d3H777SbYbtiwQX77299KVlaW+dz8+c9/9hsQdRvR90LfkwEDBsg//vGPevPp8WS6XnQZ+tnS96OhY8xWrVpl3ve2bdua59Tt5bXXXpOm+uKLL8wPNvo69D086aST5JNPPvG7zepn7vrrrzfrQT9PZ5xxhhQVFTX6/Pfff795rL4/dd16662m59m7jf7vf/8z79mBBx5o3ueuXbvKddddF9CPCP6OcdLP+Yknnmi2hwMOOED+9Kc/mc9JXYF81+o2+sYbb5jX4R1SrMts7Bin9957r+a7R99X3V4KCwtD+t367bffyllnnWW2Qd0e9HVqz7sO//b1r3/9q+ZzrNuOzqPbcCCvD4g3DNUDYoz+Gq87KzpE6oUXXjA7zXXpf8K6I6M7Knqt/wlPmTLF9Njcd999tebVHQ/dYdXjVM4//3x5/vnnzU687pRoEGrIO++8Iz/88IP5T1r/49Udjb/+9a/mWnee6u7E6vNrQJgxY4YJfE888YS0b99e7rnnnoBf+9ixY+Waa64xwVFfV2Vlpdk519fpb5ievm5dV/qf/tSpU81wnL/97W9mh0h3tPTYsIbosTO6E6q9etpG3RF57LHH5Fe/+pXZYfTdMdB26Hx6n+7saUBryC+//GKWqzu3l112mfTp08eECX0vdRl1hyEGOtRI3zsNOxMnTjQBYMyYMWbHatOmTeb98frwww9NG3Tnx0sfp9uMvpdXX3216cGbPXu2eZ26w9vQr/F6bJkuS9/36dOnm/dXdxyV9/k0UOl7vnnzZnn44YfN8+nz+vYYBbr+tJ0a+u6++27TTn1uDRaN0df70ksvmWCkoeyRRx4xO4vr1683AUnpZ0l7LnWd6M6j7szqe607hN98802j72dj9D3Iy8uTmTNnmh1L3fnWHU8NtroN6nalwf+GG24wr0V/GFC6s6/L1h1iDX66XnU7151j3W70M6A0hOkOtb7Gyy+/3CxLh+x6w6wv/Vwee+yxZv3pkEbdIdfP+ujRo82PLxpkgqHPpzv2Gppuuukms43o69J2v//++3L00UfXmn/y5MmSnZ1tPoe6fjWo62ubP39+g8vQ7wx9bm3njTfeWOs+nabDlPU5la4f/fzod5e+r5999pnMmjVLfvrpJ3NfMPQzoz866HbpXVe6jWtwaMp3rX5ONIxoW/S4PKXzNuTdd98131v6o5R+hnV70Nei759+d9YNJU35btUfafQzV15ebt4b/Y7Q76HXX3/dbGMawNRdd91lAr8uQ4fGatjVtui26v0cB/v6gJhmAYgJf/vb3yz9yHbr1s1KTU21XnnllQbnLS0trTft//7v/6zMzEzL5XLVTBs6dKh5zj//+c8108rLy638/Hyrffv2ltvtNtPWrl1r5tM2NLaMZ5991sz3wQcf1EybOnWqmXbxxRfXmveMM86wcnJyAnrt+virrrrK2r59u5WWlmb985//NNPfeOMNy+FwWOvWratZTlFRkbmvurra6tWrlzVixAhz27fdPXr0sIYPH15v3errVLt377batGljTZw4sVY7Nm3aZLVu3brW9PHjx5vH3nLLLQG9lnHjxllJSUnW0qVL693nbaf3tdRVt51KtwedtmDBglrzrl692kyfNWtWrelXXnml1bJly5r373//+5+Z75lnnqk1nz6fv+kNtcn39eh2o9vPYYcdZpWVldVMf/311828U6ZMafL6W7RokZn/3//+d63p/taZ/q3by3fffVcz7csvv6y3Xvxty0uWLDHzPf300/WWrdeN8bblsssuq5lWWVlpHXDAAWZ7nTlzZs30HTt2WBkZGWY9eD300EPm8f/6179qrdPBgweb9664uNhM0+8Ane/ee++ttZzjjjuu3uf1pJNOsvr161fr86/b25AhQ8znJNjXOHr0aLNuv//++5ppv/zyi9WqVSvr+OOPr7d9DBs2rNbn8LrrrrOSk5OtnTt3Nrocfc2HH354rWmfffZZvffG33s4Y8YMs75//PHHRrcT/Qz5rv9rr73WzPPpp5/WTNuyZYv57Nf9/AX6Xfub3/zGLKcuf9+t3u/fbdu21dpu9XtDvz9C8d36xRdf+P0c+dLvVX2P7rrrrlrTv/rqKyslJaXW9IZeHxBvGKoHxBj95V6HVehQlIb4/jKqx0JppTn9dVh/kdXhOnWPDdFf8r20x0P/1qFCOoQvkGVob48u45hjjjF/66+edekv4r60PTqsT3+ZDZT+uqzD7rQYhNJhezrE0F9VNT32S4eiaC+VLkfbpxftsdMhRR988IHfoTfe3jT91VV7cbyP04seh6C/pC9atKjeY/SXbju6PB02dtppp5lhUnU19Rgr/bVZfz32dcghh5gS4b6/6OvwIe3Z0uV73z/9NV5/XR4+fHit16q9dPqrsb/XakcrP+r2o708vscd6ZAm7WHT3pemrL+m0IIV3l4w1b9/f9NLor2l/rZlrdqn28vBBx9sfk33ty0Hyrd4hW47+p5rntMheF66DO21823Pm2++aXoAdPvz0h4d7WXT3lbt0fHOp59f33Wny9EeBF9agVB7QrTXwPt9oBd9nbrd6Oek7hDKxuh29Pbbb5veKu0V8erUqZP5vGkPWN3Ptfau+m7f+vnX5/E3DK9ur51+D+lQTi/dpnVYnPa2+XsP9TOur0+/G3R9a89IMHS96neZb4+0DjH0N4Q0mO/aQGzcuNF8d2nvovZO+m63+hnVtoXiu9Xbo6RDfBsa1qc9tfqdpduN73eDbpu9evVq0ncDEOsITkCM0eEwGm40QOgQrYaG0ejQG/3PUXcS9T99PdZC1R2/rmPz65aP1p1u1dgxHrozpkOGdKiU7jzoMnQH3t8ylB574Ms7xMZ7jII+nw6R8V78PYfSHTMNNjrUSkOI/u2P7gwqHbakbfO96FAWHaLS0DK8j9XhVHUfqzuM3gIFXrrzqkO87OgwF92Zaeh4rKbyrnd/O506NM67U6zn5tG263Tf16rrQYf21H2tupNe97UGwrszrIGgLg1OdXeWA11/TVF3u/Nue77Hb+lQKB1epT9G6A65Ho+lr1/Dc0PbSFOWrZ9HDZL6/HWn+7ZH14/umNat9KZD8bz3e681rNQdFlV3veuQPw0QOuSq7nusQ+dUMO+zbse6s+3v/dU26s627zEwgXz+G6LHLel68P4AoK9Dw7732Cov/T7whg1dH/ravMfDBfseetd/Xf5ebzDftYEuu6Fl6br1/vizv+tWvzN0eKF+F+r2qAFaTyfg22b9btD1reui7najx1s15bsBiHUc4wTEmEMPPdT86qi9JvoLpO4Y+/Y+6c6e7jDof+J63In+2q47a/rL+c0339xgL0uw9FdIPS5Ejz3Qng3dWdHn1kDnbxkNVY3yjKgSOfPMM2t+SfcGHn8nhTz99NPNzq3er+FH2+GPtw16nIG2z5+GxuF7H6vHOfkeH+S7o+9L2xPKcsYN9TzVLe7h5e/YC6UBSQ+i1x1NLROsx4XoDp5v2Xp9rRqa9Fgbf3QnKdxCvf6C2e6U9tDosW+6jgYPHmzWkb4HeszT/nxe/C07kPaEmvc16LFUdXsmvbSHLZya+rr1hx3tQdFt9w9/+IM5flJDku/xO/q50O9C/fFFv+M0nOuPQfqDgYapUH3n1RWp79pwrVstSqLrRwtc6A9C2qOpx0npOtYfMrT9+jnQIjn+lsFxTEhEBCcgBukQEu1t0aFPusOghQ68O7jaq6DDNHSYhfdgc6UH/PujhQL0F0zfXietBqYaqoykv2QuXLhQpk2bZn6pr9tT0xT6n7jvL6S6w9RQSNAhQlrpSX91rvvrvZd3eJbu1AR7fiHvYzVQhPLcRPoeaXv0fCeN8f5irDtmvkUU7IY1+ftVWbcV/bVeD8TXbULXne/5qvS16sHoeuB5QwEsWN6hk9ojqr12vnRaczthrQ5f1CDuW91Oh582VJ0u3HT9aNVK3XH1DZTeoV/e9afX+jn0FkvxqtsT7R1Op8P9QrE963asBTP89XhrG7XNjQ0lDpb+AKDDPnV5ui3rsnW4qddXX31lvrO06uC4ceNqpmvPdFPoevX3XVb39QbzXRvoMFzfz46/davfd6E8wbRWJNWLngdNfwjT7wGtqqqFTPS7QcOXfo94RyE0JFSncgCaO4bqATFKe5z0WB8dhuM9x5Hy/jLo+2ujVlDSE8f6o5WjdPif77z6t+4c6XEu/vhbhtqfM8frsnSnznvRnrWG6C/nOsRIhx419nz6H79WadMdy7oaK4Wsv8prwNHqbXrMSzCPbYzuUGpw+c9//mOOA6rLuz69wU2Pw/LScOuvHHUgO536C/JTTz1lhvn4DtNT2mOnv9hrCWV/20ZTwoMey6OhU3fAtFfQS3+51iE++3POrXDQ7bnutqyVwxrq4Qs3rXKpw1V9j0/T90LbpAHJOwRN59PpWgHQS9us8/nS90Kr3ennWo+h2d/tWdeXVrTTngrf4bx6/KUed6jVEX2H0e0vrYKoy9TvO+091UqivuHB3/eR3tYqjk2h61U/M1qZz3cd1e2VDea7VtsbyNA9HXqpPeT6Wff97OmPLdorpG0LBf3/QrcdXxqg9DvK+5nVUQD6GvUHsrqfD/3b99QTgb4+INbR4wTEMB1bP3fuXFM2XIew6flp9IBo7bHQX9B16IX+EqhDzhoatqE9OzrsRXeA9FdF3VnTg5O1/G5DZah1p0h/Yb333ntNsNASx/qfekO9WqGm57TRS2N0B0DH72uvlJ67R0tjazt1+I4e1KyvQQOMP3qf7oxeeOGFMmjQIDNkS4OkDhHSwgb6q6yW624KDWO6rnTnVw+Y1+MWdGdWdwj1oHrtYdKdUj1uQYsI6FBI3XnR4ONtQzA0GGnQ1Ise/1G3x0HbocVAdIiOvu+6bH3f9Rd3bZPufPqe8ykQ+njdpnSd6/NrkQNvOXLtxdTz6zQnuiOunxEdoqeBfcmSJaYXzluuPNJ0u9CQo8OotDCCrjPtFdNhufrjhJZVV9rrotuilszWz6+2XXs//O3A6vErGmh051hL1msvlL4n+lq1jPSXX34ZVBu1R0J7dPQ5tTdIh69qm3WnW78XQkmDn5YHf+CBB0wBhrrhX4fm6Y8Nuo3r51s/v1pi3e74qYZoCXTdHvQHKT2O01uO3NsT6BXMd63+kKPfrXpckZae1wDs22vmS4cX6/eWDhvV7wBvOXLdPrU8eShosRDthdZjyPR7X0OUtl2/azSoKl2n+j7rcF/dvvRHH9329Htey97rdqrrPNjXB8S0aJf1AxAYf2Wfve6//35z36mnnmpVVFRYH330kXXMMceYMsedO3e2brrpJuutt96qV2ZYy5H37dvX+vzzz03ZX6fTaUrKzp4927Zk7k8//WTK3mrZbi3Te84555hyxDqflsn1qlsmvO7r8S3ta1eOvDENLUfL7p555pmmPG96erp5feeee661cOFC27boutJy5vr6dN307NnTuuiii8z68tIyxi1atLCCoeWRtaxwu3btTJsOOugg8/q0FLzXsmXLrKOPPtqUfD7wwAOtBx54oMFy5FoKuDHHHnusedyll17a4Dx//etfTdln3Wa0pLSWrtbtRt/Tpm6X8+fPtwYOHGheY9u2ba0LLrjAbDe+gl1/wZYj97fd1C0/rSXBJ0yYYOXm5ppy3/qer1q1qt58wZYjr7stNvRavZ9DX5s3b65pk24D+n74fv68tGT1hRdeaGVlZZntVG97S03XnV9Lh+t217FjR3NKgy5dupjvjBdeeCHo16gKCgrMutJ1puW3f/3rX1sff/xxQNtHMMtRc+fONfPrtulb4t7rm2++MSXPtS26zvSUAd7S877rIZBy5GrFihXmfdHPva6nO++803ryySfrff4C/a7ds2ePNXbsWPN96T2tREPfrerdd981n1t9Xn1vTzvtNPMafe3Pd+sPP/xgypjrd5q+Rv186vuny63rxRdftH71q1+ZbVcvffr0MZ8rPeWB3esD4o1D/4l2eAMQHTp8R4dv2R1zAwAAkOg4xgkAAAAAbBCcAAAAAMAGwQkAAAAAbHCMEwAAAADYoMcJAAAAAGwQnAAAAADARsKdALe6ulp++eUXcxI3PVkdAAAAgMRkWZY5uXbnzp0lKanxPqWEC04amrp27RrtZgAAAABoJjZs2CAHHHBAo/MkXHDSnibvysnKyop2cwAAAABESXFxselU8WaExiRccPIOz9PQRHACAAAA4AjgEB6KQwAAAACADYITAAAAANggOAEAAACAjYQ7xgkAAADYn/LVlZWVUlVVFe2mIECpqamSnJws+4vgBAAAAATA7XbLxo0bpbS0NNpNQZCFH7TUeMuWLWV/EJwAAAAAG9XV1bJ27VrTc6EnS01LSwuoEhui30NYVFQkP/30k/Tq1Wu/ep4ITgAAAEAAvU0anvScP5mZmdFuDoLQrl07WbdunVRUVOxXcKI4BAAAABCgpCR2n2NNqHoGeecBAAAAwAbBCQAAAABsEJwAAAAARNTixYvNELqdO3eGdN5wIjgBAAAAceyiiy6S0aNHS3MyZMgQU9q9devWEiuoqgcAAAAgYioqKkw5944dO0osoccJAAAAaArLEqksic5Flx0i77//vhx11FGSnp4unTp1kltuuUUqKyvNfa+//rq0adNGqqqqzN/Lly83w+Z0Hq9LL71Ufve73zX4/Dr/Y489Jqeffrq0aNFC7rrrrnrD73788Uc57bTTJDs728zTt29fefPNN/0+n56AeOTIkXLsscdGdPgePU4AAABAU1SVijzfMjrLPnePSEqL/X6an3/+WUaNGmWG8z399NOyatUqmThxojidTrnjjjvkuOOOk927d8sXX3whRxxxhAlZubm5Jvh46bSbb7650eXoc82cOVMeeughSUlJkR9++KHW/VdddZU5V9YHH3xggtM333wjLVvWX7calH7zm9+Y+955552InlOL4AQAAAAkqEcffdSc1Hf27NmmB6hPnz7yyy+/mCA0ZcoUcwxSfn6+CUoanPT6uuuuk2nTpsmePXtk165d8t1338nQoUMbXc7YsWNlwoQJNX/XDU7r16+Xs846S/r162f+Puigg+o9x6ZNm2TMmDHSq1cvmTdvnhnuF0kEJwAAAKApkjM9PT/RWnYIFBYWyuDBg2udJFaHwGko+umnn+TAAw80oWjx4sXy+9//Xv73v//JjBkz5Pnnn5cPP/xQtm/fLp07dzZhpjEauhpz9dVXyxVXXCFvv/22DBs2zISo/v3715pn+PDhZkjh/PnzJTk5WSKNY5yibcdykXdOENn4jsiKO0TKNka7RQAAAAiEhg0dLheNi0/QCbcTTjjBhKQvv/xSUlNTTa+UTtMwpcP07HqblA6/a4weJ6W9UBdeeKF89dVXJmjNmjWr1jw6RE+H8ukwvmggOEXbzpUiRe+LbP1U5OtpBCcAAABETF5enixZskQsn2ITH330kbRq1UoOOOAA87f3OKcHH3ywJiR5g5Ne9HYo6JDByy+/XF566SXTuzV37txa9+sxUuPHj5eTTjopKuGJoXoAAABAnNNjkbQinq+cnBy58sorTcGGyZMny6RJk2T16tUydepUuf766yUpydPHopXu+vfvL88884w5Fkodf/zxcu6555rS4oH0ONm59tprTaW8Qw45RHbs2CGLFi0yoa6u+++/31T4O/HEE01o096vSCE4AQAAAHFOQ8bAgQNrTbvkkkvkiSeeMGW/b7zxRhkwYIC0bdvWTL/ttttqzTt06FATvLy9SzrfoYceKps3b5bevXvvd/s0DGllPT2uKisrS0455RTTw+WPTvcNTxq2IsFh+fbLRZiOUbzvvvtk2bJl5szBL7/8su1ZjXXlaAJeuXKl6c7TN1XLJwaquLjYVAfR1K1vStStfUZkye9E+t0p8tXtIqcsE2k7KNqtAgAAgA+XyyVr166VHj16mFLdiI/3LphsENVjnEpKSkyynTNnTkDz6wvWg8J+/etfm8SrXXp6INlbb70V9rYCAAAASFxRHaqn4xj1EqjHH3/cJMU///nP5m8d96gVPrS7bsSIERJztLOvyuW5Xe32XFeWec4GbVd+MoKVVAAAAIBEF1PHOGnFD63r7ksDk/Y8NaS8vNxcfLvjolJyXKvnuXeIuHfum65haeWdntve63d/Zf98uYNFhn9EeAIAAAAiJKaCk54tuEOHDrWm6d8ahsrKyiQjI6PeY/QEXXpm46j6/FpPyfFQ2bpEpLxIxNk+dM8JAAAAID6CU1PceuutppiEl4YsLSoRUUc85L/HSYfqFX0ksvktqWo9SJJ3FUhll/PEyqzdPiujm0jLnmKlZkn6+8eaaeUlpSLV3p40h0hyWoOL146ptIbvBgAAABBPwaljx46m5KEv/VsrYPjrbVLp6enmElXZ+Z6LHxXfPiOpm9+Sn5POkAOlQAodN0ppdZ2qenrIU4lIUnWJHLl30oqlW6Tascfzh4amFt1FkvynIy0eomXwCU8AAABAAgSnwYMHmzrzvt555x0zPdYlJ3uuM5wijkz/8ziq993OzHCIlZIuUl2hRRZF9DF7n8OX260lGD2dWwAAAABiMDjt2bNHvvvuu1rlxrXMuJ5Q68ADDzTD7H7++Wd5+umnzf2XX365OVvxTTfdJBdffLG899578vzzz8sbb7whsS55b2H41FSR6gZ6hhxV+26nOVPFSk0RqbI84SnNf3BSFZqtAAAAADRZVM/j9Pnnn5szGHvPYqzHIuntKVOmmL/1pLjr16+vmV9LkWtI0l4mPf+TliXXsx3HZClyAAAAADEjqj1OJ5xwgliNjCH7+9//7vcxX3zxRZhbBgAAACQW3ffW0/zs3OlTzMzGRRddZOZ/5ZVX9nv5up+fn58vDz30UEjnjYseJwAAAADhpeFm9OjR9aYvXrxYHA5HTVAaM2aMrFmzRqLlpZdekjvv3Htu02YopopDxKPqVn2l2DlUSjOPlm2dpkplaqcmPpHb/3Q9LqpapLzU8twOECXMY1HjZekBAAAao1WqG6pUHU5ut1vS0tJMnYPmjOAUbdn5UthlsbRoIVKZNjz4xzs0HJWL7Pne//0VDnFtT5HCYp+SfAFwpldL3iEuSUujHF/MSHaKZOURngAAiBA94qS0NDrLzsz0/NAd7qF6f/rTn+SRRx6RsrIy0yOVm5srCxYsMAXdfN1///2m/oCGoPPOO88MoUvVqmd+3HHHHWZo36RJk+Suu+6SH3/8Uaqrq+sNv3v00UflwQcflA0bNkjr1q3luOOOkxdeeMHvc2odhLFjx5rHXHDBBRIOBKdYl5QqktZGpIF8k5YukpOrn+zAP1lut0NclQ6xkpJFkglOMUF7HKtc+kZHuyUAACQMDU0tW0Zn2Xv2iPnhPZyeeeYZE2w0jBx77LHy3HPPmXCkBdt8LVq0SDp16mSutWK2BiwNQBMnTmzwuXW+F1980QzPS/ael6dOEbmrr75a/vnPf8qQIUNk+/bt8r///c/vc82bN89U39brU089VcKF4BQv4akRaQ2UKW9QskhFaZKn54LgFDuqqDsPAAD8e/3116VlnZRXVdX4cRyzZs2SSy65RCZMmGD+1srXb7/9tjmlkK/s7GxzyiANQH369JHf/OY3snDhwkaDk/ZM6SmH2rVr5/d+razdokULE4RatWol3bp1q6nE7WvOnDnyxz/+Uf7zn//I0KFDJZwITgAAAEATh8vVyRARXXYwfv3rX8tjjz1Wa9qnn34qv/vd7xp8zOrVq+XKK6+sNe2oo44y51L11bdv31q9Rtr79NVXXzXaHg1CDYUmNXz4cDPPQQcdJKeccoq5nHHGGZLp88J12N6WLVvko48+kiOPPFLCjeAEAAAANIEeYxTu4XKhor03Bx98cK1pP/30U0ieO7XOsUxaqU+PWbJrT2O0l6mgoMBU/tNeLu3t0mOjli5dKm3atDHzaA+UzvPUU0/JEUccYZYbTpQjBwAAAFBP7969TVDxtbTO3+GUkpIiw4YNk3vvvVdWrFgh69atq9Xb1bNnT3Nc1auvviqTJ08Of3vCvgQ0WomlpESkrMzzi0VlZePzO6pESlye7snSsiSxKgNL1RlOK+RVVwAAABDfNIzocUramzNkyBCZP3++CTA6fC4Sx2T98MMPcvzxx5tjqN58803Ti6VhztchhxxiwpNW5NOgFc4T4hKcolyJJScnmEdol2ZJ0MsZNMAl857cGHR4Ki8PfdpyOCzODwUAABADtKy3hpcbbrhBXC6XnHvuueZkup999lnYl63D8bTing7P02X36tVLnn32WXM8VV0aprQnSsOTHmullf/CwWFZ2u+ROIqLi00d+F27dklWVlZU26K9TZEqYfnFh+skMyOwt9pdIbJtW4o4ncGd+ykQznRL8nrrSc5C/tSJrUrLkZeY84JJcnq0WwMAQNzRnfe1a9eaUtxOp1MS1fDhw6Vjx46mTHg8vHfBZAN6nKJIi4Js2yayYoXntm2YsCw5YM0wySj5pN5d3/VaKlZS7fIqZWUOGTK8W9DtSkvVnrDKkJ8SyJwfqtwhljmnVELldQAAgJhTWloqjz/+uIwYMcL05GiPz7vvvivvvPOOJCKCUzOoxJKREWBwEods7/emOHZ9IZLklCRHhfT87ihzj/YmWUmhCyMankLPkooAj8sCAABAdGmVOj22SE+C63K5zJA4PWmtFmxIRASnWONwiJWUIZKUIdUOTniKOqrLg5jZ4TnJMQAAgB8ZGRmmhwkeBCcgHmhHXpVLZFdh4I9Jdopk5RGeAAAAAkBwAuJBUppIek7gh45VazEJF8eaAQAQpASrqxYXrBC9ZwQnIJ7CUzCqGOoJAECgUlNTawom6BA2xA63222utcDF/iA4AQAAADZ0p1vPLbRlyxbzd2ZmpimegOZNT5pbVFRk3i89Qe7+IDgBAAAAAdDzFylveEJsSEpKkgMPPHC/gy7BCRFXXu4Qh8PiJLgAACCm6I53p06dpH379lJRwZD3WJGWlmbC0/4iOCFyHHrm5iQpXJMqznRL8nq7CU8AACAmh+3t7/EyiD37H72AIE6qm5NTKakplrjKHWJZjAsGAABAbCA4IaIyHFvkgD0PSmrVFpHyLSKrZ4m4fMYJu3ymuWzuD0ZTHxfM8wW7jLrzB/L4UL8OAAAABITghIhKqdwi7Xc+IqnVW8RRXiTy7WwRvfbynWZ3fzCa+rhgni/YZdSdP5DHh/p1AAAAICAEJwAAAACwQXACAAAAABtU1UsQZWXBFWLIcFrCOd0AAAAAD4JTghgyvFtQ8w8a4JJ5T24kPMW76vJotwBARDhEkjn/AwDsD4JTnEiqLpXqOtMy0zQAlUjBly2Cfr6CL53iKi2TzAxLQslR7fJcWy6Rqr0njqtyiVSW7rvte93Y/d5pgWjq44J5vmCXUXf+QB7f2DzJGXpmvsDa79j7HLsKA5sfQGxLdopk5RGeAGA/OCzLCu2ecTNXXFwsrVu3ll27dklWVla0myPl5SLLl4u0aKFnNQ7gAVVukeJCkaQMcTgqpNeawxqdXd/d0vLMgNtTUt5COlzpKXW958kW0sIZgpCByMjqI9L/LhFne8/FTrVbJKE+/UCCMp/1CpHsfJHk9Gi3BgBiNhvQ4xTDLEeGlGUcLhllyxqcRzsgCD8JoniVyIdnifSaJNJ7sv38SfzyDCQMbw8/AKDJCE6xzOGQDQc+Lw6rLGRPWepTROK7XkubNFQvubJIUio95xlKrtwhyVU7au5LqVgrudv/IhtTzpakZEs6lL8o21JPlIpkTw+Jnt8px/2emabDyfT25vSzxJ3SxdyfVvWzdHC9KD9nXi5lKT2k0tFWKpOzPc9dvUNSqrebacnpbaR7twpJTfUJFV9PFzlsiqdnxp/0XJH0dvv+NueS2lp7HvcOEfdOkT3rRH6YK3LQRJGW3T33lawV+f4JkZ6XirToIZLaxjMkTuf30tXpXcV1n8P7+G7ni6R5XpOktBJJzardptUP1X4dOuTu04s9t4fME8nsGtwbBgAAAFsEp1jncIjlCHwonh0raV9wspIyxUoKPjhVpnUzF3/SXV+b4FTScZwJER1+flF2tL9WXOmeIYfO8q8l5+f3zDSlt3fnjq91vz6mLHtUzbSa5e69uN0Oqah0yIFt3CLp1r7x/UqHqrTuG9gLSekm0qKBohq7VnpCT5eR+55Pp2nw6TwqsGXUfQ7v4w88p+HH6zwanHxfh++xTnoMQ0rotgcAAAB4EJwQFak+W572ClXvHTWWurfCRU1PUSP3e6fVZ5ngBAAAAIQKJ8AFAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXEIRFRlSnvZlnO1uVa+t5tyf8C0zLie38i33Pj+8Pd8wS6j7vyBPD7UrwMAAAABcViWFXy96QQ5O3AklJeLLF8u0qKFSFog5yOtcosUF4okZYgk+5SeC+F5nAb+ynNeoi8+XNek8zhFm9stUlKaJPn93JLuLUcez7Qc+YKBntunfEE5cgD1/9+oKvGcxiA5PdqtAYCYzQYM1QMAAAAAGwQnAAAAALDBMU6IW+XlTTsJrsNhBTZsEgAAAAmD4IQGlZU1LXg0JsNpiSP0T1ubQ8TlSpLCNU07BsyZbklebzfhCQAAADUITmjQkOHdQv6cgwa4ZN6TG8MantJSRXJyKkWaUBfC7XaIq9whlqUNTIDCEgAAAAgIwQn1eoQ03BR86QzL8+vzlrkcYa/Wp+GpaSypqAx3lxgAAABiDcEJtWhPkPYIabgJ9bC/cPRgAQAAAJFAcILf8BSL528CAAAAwoVy5AAAAABgg+AExJPyomi3AAAAIC4RnIB4Ur412i0AAACISwQnAAAAALBBcQgAABJBdXm0WwAAPhwiyWkSSwhOAADEMz27RJVLZFdhtFsCAPskO0Wy8mIqPBGcAACIZ0lpIuk5en5vAGgeqt2eH3Ri7IuJ4AT4UV6+7wTADoclabHyY4h+CVWWBjZvcobnpF0AEiM8AUBzUlUhsYbgBPhyiLhcSVK4JrVmkjPdkrze7uiHJ9cW/+XGzS82e316ceDPlz1IZMg8whMAAEAACE6Aj7RUkZycypqeY7fbIa5yh1iWI/rdyT/OF/l2duieb0eBSFWZSEpm6J4TAAAgThGcAD/haR9LKiqbSY9MtzEiHU/0f9/OlSJf3S5y2BSRrD7+50nPFUlv5wlL7wwJa1MBAADiDcEJiBXO9p5LY7LzRVr3jVSLAAAAEgYnwAUAAAAAGwQnRFxZmR4zFO1WAAAAAIEjOCHihgzvJmMv6UR4AgAAQMwgOCEiMpyWDBqwr2x2wZdOKXM1k6ILAAAAgA2CEyJCTxU078mN8vE7P0a7KQAAAEDQCE6IaHjKyGB8XlhomfFekzzXAAAACDnKkQPxQMuU954c7VYAAADELXqcAAAAAMAGwQkAAAAAbBCcAAAAAMAGxzgBASgv37/S6Q6HJWlpIWsOAAAAIozgBDTGIeJyJUnhmtT9ehpnuiV5vd2EJwAAgBhFcEKzsKUoWea/1ErGnLlb2rersp0eKWmpIjk5lSL7UUXd7XaIq9whlqW9VpRjBwAAiEUc44RmoWhrssz+a7a5DmR6pMOT9hQ1/UJYAgAAiHUEJwAAAACwQXACAAAAABsEJwAAAACwQXACEBzXFpHVszzX/u5bOdNz8Xd/OJbX0H1Neb54ESuvMdztjJX1EEx7Q/15AAAEjOCEqCkrc0jp3ovL5TlPkl6XBjDd7mJRjyF8yotEvp3tufZ339q/eS7+7g/H8hq6rynPFy9i5TWGu52xsh6CaW+oPw8AgIBRjhwRo6XFN/y0b5MbMrxbvXnGXtrZ72Mbmt6QQQNcMu/JjeLYv/PWAgAAAAY9TogYPR9TsAGoqQq+dErZ3t4qAAAAYH/R44SI0ZPYnnh8qRlGV16+L9Ts2Jkky1eky9yns2XiuB3SvVtFzX1rf0yVJ57OlkvH7ZCB/cslu011vefNza2SdrlVNcP//PVkAQAAAPuD4ISIad+uylz86dSxygSnkSeXSt88d830lYVpJjiNqjMdAAAAiCSCE5DIqsqa8BjXvuvKUv/3NXR/UwSyvGCW1ZTHxJpYeY3hbmesrIdg2tvUz0NyhnDQJwDsH4ITkMjeGdL0x348dv/uD+XymrKsULevOYqV1xjudsbKegimvcF+HrL6iPS/S8TZ3nMBAASN4AREiO9xXdHgcFiSlrb3l+fsQSI7CqLaHgARVLxK5MOzRHpNEuk9OdqtAYCYRHACws2h56FKksI1qVFthjPdkrzebklLc4gMmdf4MD09D0z5Vv/3bVsqsvpBkc5niLToVPu+0l9Efn7Fc7vnpSItetR/fGobkaxeIuntglveQRNFWnavfV/JWpHvnxDpfa1IzlH7prt3iLh37lteenbtHcivp4scNsXzK7w/6bm129ccNbbOmtNrDHc7Y2U9BNPe3td52uXPnnUiP8wN/POgw/Y+vdhzWz/3mV1D+WoAIKEQnIAwS0sVycmpFIniSXndboe4yvXEwNrrZXmOdUjJbPgBKd1EWjRQnTDZ6QkyPS8Uad239n27Vu4LTp1H1b9/f5bXZaT/5emOYvvjA1+WPp/Kzg/8Mc2R3TprLq8x3O2MlfUQTHvbH9dwe3Wb1+AU6OfB91inrLzGP/cAgEYRnNAsaDnxSZftqCkrbjc9FsNTdFlSUcmB4QAAAE1FcEKzoGXKJ//fzoCnAwAAAJGUFNGlAQAAAEAMIjghrm0pSpZZf2ljrn1vAwAAAMEgOCGuFW1Nltl/zTbXvrcBAACAYHCME4DgaOlmPReMvxLOOq3HhH23I7G8hu5ryvPFi1h5jeFuZ6ysh2DaG+rPAwAgYAQnAMFxtm/4BJp6X99bIru8YE/m2ZTHxJpYeY3hbmesrIdg2hvqzwMAIGAM1QMAAAAAGwQnAAAAALDBUD3ErbIyh7hcnpO+eq+9t0vLGj4ZbIbTEgfnigUAAIAPghNinpYX91bK27hpX8W8IcO71dwee2lnv7f9GTTAJfOe3Eh4AgAAQA2CE2Le/JdamTLjoVLwpVPKXA7JzLBC9pwAAACIbQQnxLwxZ+6WE48vNbe370iS7TuSpaLCc9+6H1Nl7tPZMnHcDtEY9MTT2XLpuB3So9veGUSkTetqyW5TLa5yh1x8VadovQwAAAA0YwQnxLz27arMxZ+VhWkmOI082ROsNDiNOrlU+ua5683b2HFPAAAASGwEJyCBlJd7wqHDYUlaWrRbAwAAEDsITkAicGg1wSQpXJNq/nSmW5LX2014AgAACBDBCUgAaakiOTmVogd6ud0OczyXZWnvEwUwAAAAAkFwAhIoPHlYUlHJ8VwAAADBSApqbgAAAABIQPQ4NRPu+kXe/NPicW6HSJJDZN+5Xhvn8O1tSCztcqtk0mU7zLXyvQ0AAAAEiuAUZQ6HiNOpB+5LzbmHGlWtdbOTRJKTRJIC6zDUogB6fEsihictUz75/3bW/O17GwAAAAgUwSnKtKpZXp6IFegx+lWWyI5SkWTtcUoLqPy0qaRGDQAAAACgyQhOzUBQJaF1lFm6JZK89wIAAAAg7AhOgB9/nt1GUpMdcvGFu8xwPwAAACQ2ghPgx7+ea2OuTxu5h+AEAAAAglPMqvYpw6en5EkKZrwfAAAAgGAQnGKOFoVwilS5RKr2luHT2+k5hCcAAAAgTAhOsUYr6WXlSU2ZvOpykV2FVM0DAAAAwojgFIsCKEMOAAAAIHQCO4MqkKBcLkfg59gCAABA3KLHCQlpS1GyrP42VXbsTK6Z5vapt+E19tLO0v1At1x56U5xaBGOvbLbVEnvXhVU3AMAAEgQBCckpPkvtZLZf80OaN5169Pkpint602fdNkOmfx/O8PQOgAAADQ3BCckpDFn7pb8fq5aPU5Kh+V9+12qzH26dqi66/YtkpZWv8cplpWX+3ShNWMOh1Vr3QMAAEQDwQkJSYfYNTTMbmVhWr3gNGpEqWRmxMnBTg49ditJCtekSixwpluS19tNeAIAAFFFcEoQbrf2LgSx4+8QSYuN/WoESd/XnJzKmChhr9utq1wLdAS5/QIAAIQYwSnO6TAn/cVedz4rKgMfmqU9ErpzTXiKT7HzvlpBbbcAAADhQnCKczq8SYc5eX6xD/zYFzOMix/4AQAAAIPglAA8x4aQggAAAICYPQHunDlzpHv37uJ0OuXoo4+Wzz77rNH5H3roIendu7dkZGRI165d5brrrhOXyxWx9gIAAABIPFENTvPnz5frr79epk6dKgUFBTJgwAAZMWKEbNmyxe/88+bNk1tuucXMX1hYKE8++aR5jj/84Q8RbzviV7vcKpnwu53yuzG7ot0UAAAANBNRHar3wAMPyMSJE2XChAnm78cff1zeeOMNeeqpp0xAquvjjz+WY489VsaOHWv+1p6q888/Xz799NOIt73ZqXaH7rmqHCLVSSJV1SJViTfEr33bCrnlapeUljnkX/NbeyZWVTT/daGHsSXFTNUHAACAmBK14OR2u2XZsmVy66231kxLSkqSYcOGyZIlS/w+ZsiQIfKvf/3LDOc76qij5IcffpA333xTLrzwwgaXU15ebi5excXFEl8cIslOkSqXZ+c+VMFJQ0J1tUh1Mw8L4aThsea2y7M+mrOqcpG0NoQnAACAeApOW7dulaqqKunQoUOt6fr3qlWr/D5Ge5r0cb/61a/EsiyprKyUyy+/vNGhejNmzJBp06ZJ3EpOE8nKC23xB82ZWQ6RTK0sIYnL99PRqrdnfTTnHsc931MDBAAAIExiqqre4sWL5e6775ZHH33UFJL47rvv5JprrpE777xTbr/9dr+P0R4tPY7Kt8dJi0rEXXgK6fPtPfotee8lUSXXWceJvC4AAAASXNSCU25uriQnJ8vmzZtrTde/O3bs6PcxGo50WN6ll15q/u7Xr5+UlJTIZZddJn/84x/NUL+60tPTzQUIJa1fMn++yJgxIu3bR7s1AAAAiNuqemlpaXL44YfLwoULa6ZVV1ebvwcPHuz3MaWlpfXCkYYvpUP3gEgpKhKZPdtzDQAAgPgX1aF6OoRu/PjxcsQRR5hiD3qOJu1B8lbZGzdunHTp0sUcp6ROO+00U4lv4MCBNUP1tBdKp3sDFAAAAADEVXAaM2aMFBUVyZQpU2TTpk2Sn58vCxYsqCkYsX79+lo9TLfddps4HA5z/fPPP0u7du1MaLrrrrui+CoAAAAAxLuoF4eYNGmSuTRUDMJXSkqKOfmtXgAAAAAg7o9xAgAAAIBYEfUeJyAWlJXV/tvl2nddWmr/+IwMEYcjPG0DAABA+BGcgAb4VswbMsT/PGPHBvZcgwaJzJtHeGqq8vL4WnEOhyVpiXxyaQAAYhDBCWjAq6+G7rkKCjy9VpmZoXvOhODQXr0kKVyTKvHEmW5JXm834QkAgBhCcAIacN55IieeqL0d9e9btUpk+nSRKVNE+vTx//jcXJGWLRvurYK9tFSRnJxKkTg6TZvb7RBXuUMsS3vR4uiFAQAQ5whOaJDbHZ7njZVf2du391z8cTo91/n5In37NvwcgRz/BPvwFF8sqaiMr6GHAAAkAqrqoR49DkeDQUWFSElJaC/btoUvkAEAmgnXFpHVszzXkVxWJJeL8An2fYzn9z0Ur605rh9Xkch3c0XKNkosoccJfnuE8vJErBCPItIhb4WFoX1OAEAzVF4k8u1skY4nijjbR25ZKlLLRfPZfiK5vUVaKF5bc1w/5UUiPzwp0vNSkZbdJVYQnBDTw+kAAACASGCoHgAAAADYIDgBAAAAgA2CE9AE7dqJTJrkuQYAAED84xgnoAm0TPnkydFuBQAEoaosgsty7buuLI3csnynhXu5aD7bTyS3t0gLxWtrjuunam+bQl2JLMwITgAAJIJ3onA27o/HRmdZkVwuwifY9zGe3/dQvLbmuH62fSqSku7/voxOnkszQnACACBeJWeIZA8S2VEQ7ZYAQH1fXCcNOmyqSP87pDkhOAEAEM9nNB8yLzzD9PQ8LOVb/d9XvErk6+kih00Ryerjf570XJH0dsEvy71DxL1z33171on8MFfkoIkiDkvk+yc854Zp0WPfPKltRNKzg18uwifY7afu++77noZ6e4u0UHyWzPNE6PMYite182uRb+4WOXyOSLtj/M/TzHqbFMEJAIB4D08pmaF/3pRuIi26+b8v2em5zs4Xad03vMvatdITnLqM9PytwanzqNAsF+ETyu0n1NtbrK6LSH0eQ/G6JNlz1XaQ5xIjqKoHAAAAADYITkAztmWLyKxZnmsAAABED8EJaMaKikRmz/ZcAwAAIHoITgAAAABgg+AEAAAAADaoqgcAAEJLyxr3mhSZ8s91lxWp5aL5bD+R3N4iLRSvrTmun/R2IgddIpLRUWIJwQkAAISWs71I78nRWVaklovms/1EcnuLtFC8tua4fpztRA6e2CzP1dQYghMQIWVNOP+ky7XvurS0kRmrdAFJIkkOkWSH31kynJY5nQsAAACCR3ACImTIkKY/duxYuznSRKRfo3MMGuCSeU9uJDwBAAA0AcEJEed2e67TdF8/zmVkiAwaJFJQEO2WiBR86ZQyl0MyM6xoNwUiUl5Ogg0Xh8NKiO8XAEBkEZwQMdrT4XR6hp3t3i2SkxP/4Ulf87x5jQ/T03M0bd3q/75Vq0SmTxeZMkWkTx//8+TmirRr6xbZvVokySmSnFrr/rIyhwwZ3m1/XgZCyaGfgSQpXFP7fULoONMtyevtjvvvFwBAZBGcEDG6E5OX5wlOhYWSMDQ8ZWY2fH+3bp6LPxo0VX6+SN++Nsc4VVaLJFkiyfQoNWdpqfqjQaUIb1NYuN0OcZU7xLK0R4+VDAAIHYITIh6eLPZlkOA0PCFcLKmoZBgkACD0OAEuAAAAANggOAEAAACADYITAAAAANggOAEAAACADYIT0Iy1aycyaZLnGgAAANFDVT2gGWvfXmTy5Gi3AgAAAPQ4AQAAAIANghMAAAAA2GCoHhBPqivqT6vyORloVYVI1d4zEOvkJM7ECgAAEAiCExAvktJEqt0i1ZW1p1f7dCxXu0Sqqz23q8pF0toQngAAAAJAcALiQXKaSMue9p/yVr1FMjVAuUX2fC+yt/MJAAAAjSM4AfEUnvxOrzOP798AAAAICMUhAAAAAMAGwQkAAAAAbBCcAAAAAMAGwQkAAAAAbBCcAAAAAMAGVfUQNW53eJ8/rYEic4msrMxznZHmOf8tAAAAAkNwQsQ5HCJOp4jLJVJREZ5l6HPn5BCe6hoyxHM9aFCKzJtDeAIAAAgUwQkRp2EmL0/ECtPJV8vLRQoLw/PcsSgjQ4OSSEHBvmkFBUlS5kqSzBbRbBkAAEDsIDghKugJimwP37x5nmF6evH2OgEAACBwBCcgQcJTZma0WwEAABC7qKoHAAAAADbocQIAxJ3y8sZLnzgcFkOGAQBBITgBAOKHQ6tqJknhmtRGZ3OmW5LX2014AgAEjOAEAIgbaal6KoJKkUaqdrrdDnGVO8SytFcqTOU9AQBxh+AEAIi78NQ4SyoqOYsZACA4FIcAAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACUM+WomSZ9Zc25hoAAAAEJwB+FG1Nltl/zTbXAAAAIDgBAAAAgC2CEwAAAADYSLGbAYhVbnfTH5uWFsqWAAAAIOF6nKZOnSo//vhjeFoDhIDDIeJ0ilRUiJSUBH/Ztm3/QhcAAADiT9A9Tq+++qrcddddMnToULnkkkvkrLPOkvT09PC0Dmhib1FenohlBf/Y8nKRwkJJCGVlSSJJDpFkR737XC5HzXVpWf37IynDaZkwDAAAEE0Oywp+9/KLL76Qv/3tb/Lss89KZWWlnHfeeXLxxRfLkUceKc1dcXGxtG7dWnbt2iVZWVnRbg6aGQ1Oy5eLtGgRn8P1SktFBg6UmDJogEvmPbmR8ISQ0R7lktIkye/nlvT0JvzCAgDYP1VukaoSkex8keT0mMkGTSoOMXDgQHnkkUfkl19+kSeffFJ++uknOfbYY6V///7y8MMPmwUDaH4yMkQGDZKYUvClU8r29oABAADEZHEI7ayqqKgQt9ttbmdnZ8vs2bPl9ttvl7lz58qYMWNC11IA+017bebNEynb45aidd/J1h0tRRz1vwZWrUmV6fe2kyk3FUmfQyr8PldubpW0y60KW1vLyhwyZHi3sD0/AABA2IPTsmXLaobq6fFN48aNkzlz5sjBBx9s7p81a5ZcffXVBCegmYanzEyRbgdUSLcDXSLJqfXmcTo9w5fy+7ulbx6VMgAAAIIeqtevXz855phjZO3atWaY3oYNG2TmzJk1oUmdf/75UlRUFOq2AgAAAEBs9Dide+65phBEly5dGpwnNzdXqqur97dtAAAAABCbwcl7LFNdZWVlct9998mUKVNC1TYA4Vbt//glqd7bGV1dKVLVwDzhVuVTEELbUEX1M4Rw29Jt3FR1SpDtSj9OSXFYKhQAmnNwmjZtmlx++eWSqQdJ+CgtLTX3EZyAGKE7UdVuTziqy9thXF0uUl0W6ZbtXbbPSOJqlwi92AiVaodIlQan0sQJTlUukfQcwhMARLrHyeHnhCpffvmltG3bdn/aAiBSktNEWvZs+P6Wez/jLXuIZFnR/3Zq1Vuk9m81QNO5925f2ZZIIpy/XX8A2VUokiAZEQCiHpx0eJ4GJr0ccsghtcJTVVWV7Nmzx/REAYih8NSAdh1EJk3S61SRZImO5DptjVY7EH90W0rae812BQAIdXB66KGHTG+TFobQIXl6hl2vtLQ06d69uwwePDjQpwPQjLVvLzJ5crRbAQAAEIPBafz48ea6R48eMmTIEElNrX/uFwAAAABI2OBUXFwsWVlZ5vbAgQNNBT29+OOdDwAAAAASKjjp8U0bN26U9u3bS5s2bfwWh/AWjdDjnQAAAAAg4YLTe++9V1Mxb9GiReFuEwAAYVdeHt3l62+QaVQHB4D4Ck5Dhw71exsAgFjkcokUFka3DU6nSF4e4QkA4io4rVixIuAn7N+///60BwCAsNKgkpMT3Ta43Z7wZnFuJQCIr+CUn59vjl/S45gawzFOAMKhqEjktddExozxlEr3tWWLyPz5/u8LFbtlBNqGSLQVgWkOvTwVFdFuAQAg5MFp7dq1QT0pAITS1q0is2eLnHhi/cChoaqh+0LFbhmBtiESbQUAAFEMTt26dQvT4gEAAAAgToLTa6+9JiNHjjQnvdXbjTn99NND1TYAAAAAiJ3gNHr0aNm0aZM5j5PebgjHOAEAAABI2OBUXV3t9zYQr7TiVXM7kBwAAADNPDgBiUJPSKnnVtEywd6KV3pbSxcTnqJH3wPvdWlp4PdFYvnBtME7HyWoAQCIPQ7Lrsa4HwsXLpQHH3xQCveePTAvL0+uvfZaGTZsmDR3xcXF0rp1a9m1a5dkZWVFuzlopr1N3k9FebnnJJktWhCcwk1LdWvVOd+QMXasxKUDDxR58EFPUK+rXTsq7iXK90xJiZ7uQyQ9PcwLqyoX2bFcJLmFSDJfZACagSq3SFWJSHa+SHK4vwRDlw2C7nF69NFH5ZprrpGzzz7bXKtPPvlERo0aZcLUVVdd1fSWA80AASk69PxGWqo7EaxfL3LWWf7vmzRJZPLkSLcIAACEvMfpgAMOkFtuuUUm6f/uPubMmSN33323/Pzzz9Kc0eOEYGiP0/Ll9DhFo8dJ6beTvgdLl3p6aCZOFOnevfY8epq5J54QufZakaOO8v/cubmenpzG6LL1fFH+rFolMn26/+X7tmHKFJEOHUR27qw/T5s2Ihs2iMyc6fl73jzPsNC66HFKDPQ4AUhoVQnS47Rz50455ZRT6k0/+eST5eabbw726QDA0LDQUGDQgKHBaeRIkb59a9+3cqUntBx/fP37gqGnq2volHXegONv+b5t0J3gxtqwbNm+23l5IpmZTW8vAACIrKRgH6DnaXr55ZfrTX/11Vfl1FNPDVW7AAAAAKDZCKjH6ZFHHqm5feihh8pdd90lixcvlsGDB9cc4/TRRx/J73//+/C1FAAAAACac3DSog++srOz5ZtvvjEXrzZt2shTTz0lt912W+hbCQAAAADNPTit1SOfAQAAACBBBX2MEwAAAAAkmqCr6qmffvpJXnvtNVm/fr24taaqjwceeCBUbQOAmhLdegYEfyXFG7svEssPpg1aFh0AACRIcFq4cKGprHfQQQfJqlWr5LDDDpN169aJng5q0KBB4WklgISmZcobOilsY/dFYvnBtCGc4Q4AADSzoXq33nqr3HDDDfLVV1+J0+mUF198UTZs2CBDhw6Vc845JzytBAAAAIBYCk6FhYUybtw4czslJUXKysqkZcuWMn36dLnnnnvC0UYAAAAAiK3g1KJFi5rjmjp16iTff/99zX1bt24NbesAAAAAIBaPcTrmmGPkww8/lLy8PBk1apQ56a0O23vppZfMfQAAAAAgiR6ctGrenj17zO1p06aZ2/Pnz5devXpRUQ8AAABAXAo6OGk1Pd9he48//nio2wQAAAAAsX8eJ/X555+bQhHq0EMPlcMPPzyU7QIAAACA2A1OevLb888/Xz766CNp06aNmbZz504ZMmSIPPfcc3LAAQeEo50AAAAAEDtV9S699FKpqKgwvU3bt283F71dXV1t7gMAAAAASfQep/fff18+/vhj6d27d800vT1r1iw57rjjQt0+AAAAAIi9HqeuXbuaHqe6qqqqpHPnzqFqFwAAAADEbnC67777ZPLkyaY4hJfevuaaa+T+++8PdfsAAAAAIDaG6mVnZ4vD4aj5u6SkRI4++mhJSfE8vLKy0ty++OKLZfTo0eFrLQAAAAA01+D00EMPhb8lQDPmdu//c6SlhaIlAAAAaLbBafz48eFvCdAMaUer0ynicon4ObQvYPr4nBzCEwAAQEKdAFcLQbzyyis1J8Dt27evnH766ZKcnBzq9gFRpUEnL0/Espr+HOXlIns/KgAAAEiU4PTdd9/JqFGj5Oeff64pST5jxgxTbe+NN96Qnj17hqOdQNTQSwSEzpYtIvPni4wZI9K+fbRbAwBAGKvqXX311SYcbdiwQQoKCsxl/fr10qNHD3MfAAANKSoSmT3bcw0AQNyfAPeTTz6Rtm3b1kzLycmRmTNnyrHHHht0A+bMmWNKnG/atEkGDBhgTqR71FFHNTj/zp075Y9//KO89NJLsn37dunWrZspXqG9YAAAxBIdyht2Vbogh0jy3gsANIHDYSX8KJygg1N6errs3r273vQ9e/ZIWpBrc/78+XL99dfL448/bsqbawAaMWKErF69Wtr7GcPhdrtl+PDh5r4XXnhBunTpIj/++KO0adMm2JcBAEBUadGYiBz/WO0QKc4USXaKJDXp0GYAEGe6JXm93QkdnoL+Bj311FPlsssukyeffLKmZ+jTTz+Vyy+/3BSICMYDDzwgEydOlAkTJpi/NUDpcVJPPfWU3HLLLfXm1+nay/Txxx9Lamqqmda9e/dgXwIAAFGlOx5aaTMitMepslokqVokuTpCCwUQT9xuh7jKHWJZ2mu9HxWzEi04PfLII6Y8+eDBg2vCi54AV0PTww8/HPDzaO/RsmXL5NZbb62ZlpSUJMOGDZMlS5b4fcxrr71mlnvVVVfJq6++Ku3atZOxY8fKzTff3GBFv/LycnPxKi4uDuLVAgAQHhH71VaDU5olkmSJUPwWQJNYUlHJUN+ggpNlWSZ4PPfcc6aqnrcceV5enhx88MFBLXjr1q2mrHmHDh1qTde/V61a5fcxP/zwg7z33ntywQUXyJtvvmkq/F155ZVSUVEhU6dO9fsYrfg3bdq0oNoGAOFWViYJOzzNe11aGp5lZGR4zsEGAEBUg5MGpJUrV0qvXr2CDkv7q7q62hzf9Ne//tX0MB1++OEmwGlxiYaCk/Zo6XFUXhr8tHQ6AETTkCGS0MaODd9zDxokMm8e4QkAEMVy5DqUTgPTtm3b9nvBubm5Jvxs3ry51nT9u2PHjn4f06lTJznkkENqDcvT3i6tyKdD/xoqZpGVlVXrAgDRoD0hulOP8CooSNwePQBAMzrGScuO33jjjfLYY4/JYYcd1uQFawU+7TFauHChjB49uqZHSf+eNGmS38doufN58+aZ+TTEqTVr1phAFWxFPwCINO0B0Z6QeN+p13M0bd3q/z4diT19usiUKSJ9+vifJzdXpF274Jer6zXRe/IAAM0oOI0bN05KS0vNOZc0rGToT6g+tOpdoHQInRaaOOKII0yFPi1HXlJSUlNlT5elJcf1OCV1xRVXyOzZs+Waa66RyZMny7fffit33303J94FEFPhKTNT4lq3bp6LP06n5zo/X6Rv34g2CwCAyAYnDTehMmbMGCkqKpIpU6aY4Xb5+fmyYMGCmoIR69evr+lZUnps0ltvvSXXXXed9O/f34QqDVFaVQ8AAAAAmk1w0h6iUNJheQ0NzVu8eHG9aVqO/JNPPglpGwAAAACgMU06hbiWEX/55ZdrypEfeuih8tvf/lZSUjgjOQAAAID4E3TS0VLkerJbHVrXu3dvM+2ee+4xJ6P9z3/+s18FIwAAAAAg5suRq0svvVT69u0rP/30kxQUFJjLhg0bzDFHl112WXhaCQAAAACx1OO0fPly+fzzzyU7O7tmmt6+66675Mgjjwx1+wAAcUTLjOthrU0pNw4AQEz1OOkJaOuetFZt2bJFDj744FC1CwAQh9q3F5k82XMNAEBcByc9p5KeN+mFF14ww/X0orevvfZac6xTcXFxzQUAAAAAEnKo3qmnnmquzz33XHHomRxFxLIsc33aaafV/K33afU9AAAAAEi44LRo0aLwtAQAAAAA4iU4DR06NDwtAQAAAIB4OcYJAAAAABJN0D1OAJrG7Y52C0TS0qLdAgAAgNhEcALCTGuoOJ0iLpdIRUX02qHLz8khPAEAADQFwQkIMw0qeXlabTJ6bSgvFyksjN7yAQAAYh3BCYgAenkAAAASIDgNHDiw5pxNdgoKCva3TQAAAAAQe8Fp9OjR4W8JAAAAAMRycJo6dWr4WwIAAAAAzRTncQIAAACAUBeHqKqqkgcffFCef/55Wb9+vbjrnJxm+/btwT4lAAAAAMRXj9O0adPkgQcekDFjxsiuXbvk+uuvlzPPPFOSkpLkjjvuCE8rAQAAACCWgtMzzzwjc+fOld///veSkpIi559/vjzxxBMyZcoU+eSTT8LTSgAAAACIpeC0adMm6devn7ndsmVL0+ukTj31VHnjjTdC30IAAAAAiLXgdMABB8jGjRvN7Z49e8rbb79tbi9dulTS09ND30IAAAAAiLXgdMYZZ8jChQvN7cmTJ8vtt98uvXr1knHjxsnFF18cjjYCAAAAQGxV1Zs5c2bNbS0Q0a1bN/n4449NeDrttNNC3T4AAJpkyxaR+fP1/yrP397b7dtHu2UAgIQITi6XS5xOZ83fxxxzjLkAANCcFBWJzJ4tcuKJnr+9twlOAICIDNVr3769jB8/Xt555x2prq5u0kIBAAAAIK6D0z/+8Q8pLS2V3/72t9KlSxe59tpr5fPPPw9P6wAAAAAgVotD/Pvf/5bNmzfL3XffLd98840ZqnfIIYfI9OnTw9NKAAAAAIil4OTVqlUrmTBhgilHvmLFCmnRooVMmzYttK0DAAAAgFgOTlok4vnnn5fRo0fLoEGDZPv27XLjjTeGtnUAADRBWZn+P+W5rde+t0tLRSwrqs0DACRCVb233npL5s2bJ6+88oqkpKTI2WefbXqdjj/++PC0EACARkqOa/U8tffc7MaQIftujx1b/3b37iJXXinStq3notq1o+IeACCEwUmPcTr11FPl6aefllGjRklqamqwTwEAQEjouZm0zHiw1q0Tuemm2tMmTdITu4esaQCARA9OWhRCj28CACDa9IS23vM0bd/uuVRU7AtHc+eKTJzoGZr3xBMi48drdVjP/XfdJdKhQ+0eJwAA9is4FRcXS1ZWlrltWZb5uyHe+QAACDcdWtfQ8LqVKz3BaeRIz98anEaM2BecRo0SycyMXFsBAAkQnLKzs2Xjxo3m5Ldt2rQRh8NRbx4NVDq9qqoqHO0EAAAAgOYdnN577z1pu3csg972F5wAAAAAIKGD09ChQ2tun3DCCeFsD4Awcrv33U5Li2ZLAAAA4vw8Tr169ZI77rhDvv322/C0CEDIaSex0+k5aL6kRGTbttohCgAAACEOTldeeaW88cYb0qdPHznyyCPl4Ycflk2bNgX7NAAiSHuX8vJE8vM91xqiAAAAEMbgdN1118nSpUulsLDQnMdpzpw50rVrVzn55JPNuZ0ANN/wlJ7uuQCJQMuL67mZ9Np7Ozc32q0CAMQqh6Xl8PbTJ598IldccYWsWLGi2VfV01LqrVu3ll27dlE6HQmpvFxk+XKRFi04zgmJp7RUZOBAz+0vvkiQcuRVbpHiQpGkDJFkTloPIHg6vL+kNEny+7klPd0KzfdSVYlIdr5IcnrMZIOgT4Dr67PPPpN58+bJ/PnzzULPOeec/Xk6AAAAAGiWgg5Oa9askWeeeUaeffZZWbt2rZx44olyzz33yJlnniktW7YMTysBAAAAIJaCk7coxFVXXSXnnXeedOjQITwtAwAAAIBYDE56/NJf/vIXOfvssyU7Ozt8rQIAAACAWK2ql5ycLJMnT5adO3eGr0UAAAAAEOvlyA877DD54YcfwtMaAAAAAIiH4PSnP/1JbrjhBnn99ddl48aNppqe7wUAAAAAJNGLQ+hJb9Xpp58uDoejZrqeDkr/bu7ncQIAAACAsAenRYsWBb0QAAAAAEio4DR06NDwtAQAAAAA4iU4ffDBB43ef/zxx+9PewAAAAAg9oPTCSecUG+a77FOHOMEAAAAQBK9qt6OHTtqXbZs2SILFiyQI488Ut5+++3wtBIAAAAAYqnHqXXr1vWmDR8+XNLS0uT666+XZcuWhaptAAAAABCbPU4N6dChg6xevTpUTwcAAAAAsdvjtGLFilp/6/mb9ES4M2fOlPz8/FC2DQAAAABiMzhpONJiEBqYfB1zzDHy1FNPhbJtAAAAABCbwWnt2rW1/k5KSpJ27dqJ0+kMZbsAAAAAIHaDU7du3cLTEgAAAACI9eIQS5Yskddff73WtKefflp69Ogh7du3l8suu0zKy8vD0UYAAAAAiI3gNH36dFm5cmXN31999ZVccsklMmzYMLnlllvkP//5j8yYMSNc7QQAAACA5j9Ub/ny5XLnnXfW/P3cc8/J0UcfLXPnzjV/d+3aVaZOnSp33HFHeFoKIKTcbokZaWnRbgEAAEh0AQenHTt2mHM1eb3//vsycuTImr+PPPJI2bBhQ+hbCCCkHA4RreXicolUVEizp+3MySE8AQCAGAlOGpq0op72LLndbikoKJBp06bV3L97925JTU0NVzsBhIgGkLw8PQebNHt62GRhYbRbAQAAEERwGjVqlDmW6Z577pFXXnlFMjMz5bjjjqt1YtyePXuGq50AQojeGwAAgDAFJz2+6cwzz5ShQ4dKy5Yt5R//+Iek+ex96clvTz755CAXDwAAAABxFJxyc3Plgw8+kF27dpnglJycXOv+f//732Y6AAAAAEiinwC3devWfqe3bds2FO0BAAAAgNg9jxMAAAAAJCqCEwAAIbBli8isWZ5rAED8ITgBABACRUUis2d7rgEA8YfgBAAAAAA2CE4AAAAAYIPgBAAAAAA2CE4AAAAAEOrzOAEAEA/KykL7fC7XvuvSUvv5MzJEHA6JnOqKCC4MQFypcohUJ4lUuUWqrP1/vmq3xCKCEwAgIQ0ZEp7nHTs2sPkGDRKZNy9C4SkpzbOjUl0ZgYUBiDvVDpEqDU6loQlOKtkpIpH89Wj/EZwAAAlDe3k0sBQURLslnjZor1dmZpgXlJwm0rJnmBcCIK6596aGbEskPVRP6vB8P8UQghMAIGFo74728jR1mJ6eo2nrVv/3rVolMn26yJQpIn36+J8nN1ekZcvw9XY1KMZ2TgA0M8l7KyMk770kKIITACDhwlNTe3m6dfNc/HHqqBMRyc8X6du34ecI5PgnAEDzQ1U9AAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAIATatROZNMlzDQCIP5QjBwAgBNq3F5k8OdqtAACEC8EJQLPn1jOWx5k0zkcKAEBMITgBaNYnKtWTirpcIhUVEjf09eTkEJ4AAIglBCcAzZYGi7w8EcuSuFFeLlJYGO1WAACAYBGcADRr9MoAAIDmgKp6AAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOAACgUVu2iMya5bmO52VGernReo1AsIqKRObOFdm4URJaswhOc+bMke7du4vT6ZSjjz5aPvvss4Ae99xzz4nD4ZDRo0eHvY0AACTyTtPs2Z7reF5mpJcbrdcIBEu30SefFNm0SRJa1IPT/Pnz5frrr5epU6dKQUGBDBgwQEaMGCFbbH5+Wbdundxwww1y3HHHRaytAAAAABJT1IPTAw88IBMnTpQJEybIoYceKo8//rhkZmbKU0891eBjqqqq5IILLpBp06bJQQcdFNH2AgAAAEg8UQ1Obrdbli1bJsOGDdvXoKQk8/eSJUsafNz06dOlffv2cskll9guo7y8XIqLi2tdAAAAACAYKRJFW7duNb1HHTp0qDVd/161apXfx3z44Yfy5JNPyvLlywNaxowZM0zPFAA0J253tFsQ39LSot0CAEC8iWpwCtbu3bvlwgsvlLlz50pubm5Aj7n11lvNMVRe2uPUtWvXMLYSABrmcIg4nSIul0hFRbRbE5903ebkxEZ4KiuTmFmn3uvS0vhdZqSXG63XCDR1W7UsSWhRDU4afpKTk2Xz5s21puvfHTt2rDf/999/b4pCnHbaaTXTqqurzXVKSoqsXr1aevbsWesx6enp5gIAzYHuzOfl8Z9PuJSXixQWSswYMkRiytixibHMSC83Wq8RCNann+q+tf/7OnXyXOJZVINTWlqaHH744bJw4cKakuIahPTvSZMm1Zu/T58+8tVXX9Wadtttt5meqIcffpieJAAxIRZ6QhA+GRkigwaJFBREuyUAEJzrrmv4vqlTRe64Q+Ja1Ifq6TC68ePHyxFHHCFHHXWUPPTQQ1JSUmKq7Klx48ZJly5dzLFKep6nww47rNbj27RpY67rTgcAoLkO15w3r/kN09PztGzd6v8+Pex4+nSRKVP0R0z/8+gI+nbtmv8yI73caL1GIFiNbatffy1y99167lWRY47xP0+89zY1i+A0ZswYKSoqkilTpsimTZskPz9fFixYUFMwYv369abSHgAA8RSeMjOlWenWzXPxR4/LU/n5In37xvYyI73caL1GIJTbanKy51p7y/WSqKIenJQOy/M3NE8tXry40cf+/e9/D1OrAAAAAMCDrhwAAAAAsEFwAgAAAAAbBCcAAAAAsEFwAgAAAAAbBCcAANAoLYetNZwiWRY7GsuM9HKj9RqBYOk2esklIh07SkJzWFZinb++uLhYWrduLbt27ZKsrKxoNwcAEELl5SLLl4u0aMGJhgEgVNxukZIST+n89HRJ2GxAjxMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2EixmwEAgFjjdtvPk5YWiZYAAOIFwQkAEDccDhGnU8TlEqmoaHg+vT8nh/AEAAgcwQkAEDc0COXliVhWw/OUl4sUFkayVQCAeEBwAgDEFXqRAADhQHEIAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGwQnAAAAALBBcAIAAAAAGyl2MwAAEI/c7mi3QCQtLdotAAAEiuAEAEgoDoeI0ynicolUVESvHbr8nBzCEwDECoITACChaFDJyxOxrOi1obxcpLAwessHAASP4AQASDj08gAAgkVxCAAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACwQXACAAAAABsEJwAAAACIheA0Z84c6d69uzidTjn66KPls88+a3DeuXPnynHHHSfZ2dnmMmzYsEbnBwAAAICYD07z58+X66+/XqZOnSoFBQUyYMAAGTFihGzZssXv/IsXL5bzzz9fFi1aJEuWLJGuXbvKySefLD///HPE2w4AAAAgMTgsy7Ki2QDtYTryyCNl9uzZ5u/q6moThiZPniy33HKL7eOrqqpMz5M+fty4cbbzFxcXS+vWrWXXrl2SlZUVktcAAEAwystFli8XadFCJC0t2q0BgMa53SIlJSL5+SLp6RJXgskGUe1xcrvdsmzZMjPcrqZBSUnmb+1NCkRpaalUVFRI27Zt/d5fXl5uVojvBQAAAACCEdXgtHXrVtNj1KFDh1rT9e9NmzYF9Bw333yzdO7cuVb48jVjxgyTIr0X7c0CAAAAgJg6xml/zJw5U5577jl5+eWXTWEJf2699VbT9ea9bNiwIeLtBAAAABDbUqK58NzcXElOTpbNmzfXmq5/d+zYsdHH3n///SY4vfvuu9K/f/8G50tPTzcXAAAAAIjJHqe0tDQ5/PDDZeHChTXTtDiE/j148OAGH3fvvffKnXfeKQsWLJAjjjgiQq0FAAAAkKii2uOktBT5+PHjTQA66qij5KGHHpKSkhKZMGGCuV8r5XXp0sUcq6TuuecemTJlisybN8+c+8l7LFTLli3NBQAAAADiLjiNGTNGioqKTBjSEJSfn296krwFI9avX28q7Xk99thjphrf2WefXet59DxQd9xxR8TbDwAAACD+Rf08TpHGeZwAANHGeZwAxBLO4xQHVfUAAAAAIBIITgAAAABgg+AEAAAAADYITgAAAABgg+AEAAAAAM29HDkAAIlcqQoAmju+qzwITgAARJjDIeJ0irhcIhUV0W4NANhzOj3fXYmM4AQAQITpuZvy8kQS60yKAGKZw8F55whOAABEQaLvgABArKE4BAAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgA2CEwAAAADYIDgBAAAAgI0USTCWZZnr4uLiaDcFAAAAQBR5M4E3IzQm4YLT7t27zXXXrl2j3RQAAAAAzSQjtG7dutF5HFYg8SqOVFdXyy+//CKtWrUSh8PRLFKuhrgNGzZIVlZWtJsTl1jH4cc6Dj/WcWSwnsOPdRx+rOPIYD3HxzrWKKShqXPnzpKU1PhRTAnX46Qr5IADDpDmRjcGPnThxToOP9Zx+LGOI4P1HH6s4/BjHUcG6zn217FdT5MXxSEAAAAAwAbBCQAAAABsEJyiLD09XaZOnWquER6s4/BjHYcf6zgyWM/hxzoOP9ZxZLCeE28dJ1xxCAAAAAAIFj1OAAAAAGCD4AQAAAAANghOAAAAAGCD4AQAAAAANghOUTRnzhzp3r27OJ1OOfroo+Wzzz6LdpNixh133CEOh6PWpU+fPjX3u1wuueqqqyQnJ0datmwpZ511lmzevLnWc6xfv15+85vfSGZmprRv315uvPFGqayslET1wQcfyGmnnWbOnK3r85VXXql1v9aRmTJlinTq1EkyMjJk2LBh8u2339aaZ/v27XLBBReYk9S1adNGLrnkEtmzZ0+teVasWCHHHXec2e71bOD33nuvJAq7dXzRRRfV265POeWUWvOwjhs3Y8YMOfLII6VVq1bmcz169GhZvXp1rXlC9f2wePFiGTRokKn2dPDBB8vf//53SRSBrOcTTjih3vZ8+eWX15qH9dywxx57TPr3719z4s/BgwfLf//735r72Y7Dv47ZhkNv5syZZj1ee+21sbkta1U9RN5zzz1npaWlWU899ZS1cuVKa+LEiVabNm2szZs3R7tpMWHq1KlW3759rY0bN9ZcioqKau6//PLLra5du1oLFy60Pv/8c+uYY46xhgwZUnN/ZWWlddhhh1nDhg2zvvjiC+vNN9+0cnNzrVtvvdVKVLoO/vjHP1ovvfSSVtq0Xn755Vr3z5w502rdurX1yiuvWF9++aV1+umnWz169LDKyspq5jnllFOsAQMGWJ988on1v//9zzr44IOt888/v+b+Xbt2WR06dLAuuOAC6+uvv7aeffZZKyMjw/rLX/5iJQK7dTx+/HizDn236+3bt9eah3XcuBEjRlh/+9vfzGtfvny5NWrUKOvAAw+09uzZE9Lvhx9++MHKzMy0rr/+euubb76xZs2aZSUnJ1sLFiywEkEg63no0KHm/zbf7Vm3Ty/Wc+Nee+0164033rDWrFljrV692vrDH/5gpaammnWu2I7Dv47ZhkPrs88+s7p3727179/fuuaaa2qmx9K2THCKkqOOOsq66qqrav6uqqqyOnfubM2YMSOq7Yql4KQ7j/7s3LnTfPH9+9//rplWWFhodlSXLFli/tYPXVJSkrVp06aaeR577DErKyvLKi8vtxJd3Z366upqq2PHjtZ9991Xaz2np6ebHXOlX1T6uKVLl9bM89///tdyOBzWzz//bP5+9NFHrezs7Frr+Oabb7Z69+5tJZqGgtNvf/vbBh/DOg7eli1bzDp7//33Q/r9cNNNN5kfb3yNGTPGBIpEVHc9e3c6fXeO6mI9B08/20888QTbcQTWsWIbDp3du3dbvXr1st55551a6zXWtmWG6kWB2+2WZcuWmaFOXklJSebvJUuWRLVtsUSHiemQp4MOOsgMXdJuXKXrtqKiotb61WF8Bx54YM361et+/fpJhw4dauYZMWKEFBcXy8qVK6Pwapq3tWvXyqZNm2qt09atW5shpr7rVIeOHXHEETXz6Py6bX/66ac18xx//PGSlpZWa73rEJ8dO3ZE9DU1VzrUQIch9O7dW6644grZtm1bzX2s4+Dt2rXLXLdt2zak3w86j+9zeOdJ1O/wuuvZ65lnnpHc3Fw57LDD5NZbb5XS0tKa+1jPgauqqpLnnntOSkpKzHAytuPwr2MvtuHQuOqqq8xQu7rrIta25ZSQPhsCsnXrVvMB9d0AlP69atWqqLUrlugOu45d1Z3LjRs3yrRp08wxHV9//bXZwdedRt3BrLt+9T6l1/7Wv/c+1OZdJ/7Wme861R1+XykpKWZHyneeHj161HsO733Z2dmSyPR4pjPPPNOso++//17+8Ic/yMiRI80Xf3JyMus4SNXV1WYc/bHHHmt2elSovh8amkf/Iy8rKzPHASbyelZjx46Vbt26mR+49Li7m2++2QT4l156ydzPerb31VdfmZ14PQZEj/14+eWX5dBDD5Xly5ezHYd5HSu24dB47rnnpKCgQJYuXVrvvlj7TiY4ISbpzqSXHtipQUq/3J5//vmE+BJCfDrvvPNqbuuva7pt9+zZ0/RCnXTSSVFtW6z+wqk/pnz44YfRbkpCrufLLrus1vashWV0O9YfBXS7hj39cVBDkvbovfDCCzJ+/Hh5//33o92shFjHGp7Yhvffhg0b5JprrpF33nnHFCyKdQzViwLt8tVfj+tWDNG/O3bsGLV2xTL9peKQQw6R7777zqxDHQ65c+fOBtevXvtb/977UJt3nTS2zer1li1bat2vFW+0ChzrvWl0GKp+X+h2rVjHgZs0aZK8/vrrsmjRIjnggANqpofq+6GhebQyVyL9eNPQevZHf+BSvtsz67lx+ku8Vgc7/PDDTSXDAQMGyMMPP8x2HIF17A/bcPB0KJ7+v6XV7nSEhF40mD7yyCPmtvYKxdK2THCK0odUP6ALFy6sNdRB//YdV4vAaTlm/QVIfw3SdZuamlpr/WrXuh4D5V2/eq3d8747ofpriH7AvF302EeHfumXku861e5vPa7Gd53qF59+SXq99957Ztv2/mej82hJbh3P7Lve9Re/RBpCFqiffvrJHOOk27ViHdvTuhu6M6/DbXTd1B22GKrvB53H9zm88yTKd7jdevZHf9VXvtsz6zk4+lkvLy9nO47AOvaHbTh42kOn60jXnfeix+nqsene2zG1LYe01ASCKkeuFcn+/ve/m0pZl112mSlH7lsxBA37/e9/by1evNhau3at9dFHH5kSlVqaUis7eUtbamnc9957z5S2HDx4sLnULW158sknm1K6Wq6yXbt2CV2OXCveaJlPvehXwwMPPGBu//jjjzXlyHUbffXVV60VK1aY6m/+ypEPHDjQ+vTTT60PP/zQVNDxLZWt1XO0VPaFF15oyr3q50DLhyZKqezG1rHed8MNN5gqQrpdv/vuu9agQYPMOnS5XDXPwTpu3BVXXGHK5uv3g28J4dLS0pp5QvH94C19e+ONN5oKUHPmzEmoEsN26/m7776zpk+fbtavbs/6vXHQQQdZxx9/fM1zsJ4bd8stt5gqhbr+9DtX/9YKmm+//ba5n+04vOuYbTh8htapVhhL2zLBKYq0xrxuKHo+Jy1PrudlQWC0xGSnTp3MuuvSpYv5W7/kvHRn/sorrzRlRfWDdMYZZ5j/1H2tW7fOGjlypDnHjYYuDWMVFRVWolq0aJHZma970RLZ3pLkt99+u9kp19B/0kknmfNe+Nq2bZvZiW/ZsqUpEzphwgQTCHzpOaB+9atfmefQ904DWaJobB3rDqf+p6D/GWhp1m7dupnzh9T9MYV13Dh/61cves6hUH8/6PuZn59vvod0h8p3GYm+ntevX292MNu2bWu2Qz3fmO7Q+J4DR7GeG3bxxReb7wF93fq9oN+53tCk2I7Du47ZhiMXnMpiaFt26D+h7cMCAAAAgPjCMU4AAAAAYIPgBAAAAAA2CE4AAAAAYIPgBAAAAAA2CE4AAAAAYIPgBAAAAAA2CE4AAAAAYIPgBAAAAAA2CE4AgLh10UUXyejRo6PdDABAHEiJdgMAAGgKh8PR6P1Tp06Vhx9+WCzLilibAADxi+AEAIhJGzdurLk9f/58mTJliqxevbpmWsuWLc0FAIBQYKgeACAmdezYsebSunVr0wPlO01DU92heieccIJMnjxZrr32WsnOzpYOHTrI3LlzpaSkRCZMmCCtWrWSgw8+WP773//WWtbXX38tI0eONM+pj7nwwgtl69atUXjVAIBoITgBABLKP/7xD8nNzZXPPvvMhKgrrrhCzjnnHBkyZIgUFBTIySefbIJRaWmpmX/nzp1y4oknysCBA+Xzzz+XBQsWyObNm+Xcc8+N9ksBAEQQwQkAkFAGDBggt912m/Tq1UtuvfVWcTqdJkhNnDjRTNMhf9u2bZMVK1aY+WfPnm1C09133y19+vQxt5966ilZtGiRrFmzJtovBwAQIRzjBABIKP3796+5nZycLDk5OdKvX7+aaToUT23ZssVcf/nllyYk+Tte6vvvv5dDDjkkIu0GAEQXwQkAkFBSU1Nr/a3HRvlO81brq66uNtd79uyR0047Te655556z9WpU6ewtxcA0DwQnAAAaMSgQYPkxRdflO7du0tKCv9tAkCi4hgnAAAacdVVV8n27dvl/PPPl6VLl5rheW+99ZapwldVVRXt5gEAIoTgBABAIzp37iwfffSRCUlacU+Ph9Jy5m3atJGkJP4bBYBE4bA4pToAAAAANIqfygAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAAAABAGvf/vQzHRNlEvsEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_km_curves_fusion(risks, times, events, title_name, save_figure=False):\n",
    "    risks = np.array(risks)\n",
    "    times = np.array(times)\n",
    "    events = np.array(events)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8)) \n",
    "    high_risk_idx = risks > np.median(risks)\n",
    "    low_risk_idx = risks <= np.median(risks)\n",
    "    kmf_high = KaplanMeierFitter()\n",
    "    kmf_low = KaplanMeierFitter()\n",
    "    # fit low risk\n",
    "    kmf_low.fit(times[low_risk_idx], event_observed=events[low_risk_idx], label='Low risk')\n",
    "    kmf_low.plot_survival_function(ax=ax, ci_show=True, ci_alpha=0.15, show_censors=True, color='orange')\n",
    "    # fit high risk\n",
    "    kmf_high.fit(times[high_risk_idx], event_observed=events[high_risk_idx], label='High risk')\n",
    "    kmf_high.plot_survival_function(ax=ax, ci_alpha=0.15, ci_show=True,show_censors=True, color='blue')\n",
    "    ax.set_title(f\"Kaplan-Meier curve for final model on {title_name}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Survival probability\")\n",
    "    plt.legend()\n",
    "    if save_figure:\n",
    "        current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "        plt.savefig(f\"../evaluation-results/fusion_{title_name}_{current_time}.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "val_c_index = concordance_index(val_times, -np.array(val_risks), val_events)\n",
    "print(f\"validation c-index: {val_c_index}\")\n",
    "\n",
    "val_c_index_custom = concordance_index_custom(val_risks, val_times, val_events)\n",
    "print(f\"validation c-index custom: {val_c_index_custom}\")\n",
    "\n",
    "display_km_curves_fusion(val_risks, val_times, val_events, \"validation set\", save_figure=False)\n",
    "\n",
    "\n",
    "saved_model = False\n",
    "\n",
    "if saved_model:\n",
    "    current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "    checkpoint_path = f\"../checkpoints/trained-model_{date.today()}_{val_c_index:4f}.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(), # all weights all models\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'batch_size': batch_size,\n",
    "        'dropout_ratio': dropout_ratio,\n",
    "        'learning_rate': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'n_epochs': n_epochs,\n",
    "        'random_seed': 0,\n",
    "        'val_c_index': val_c_index\n",
    "    }, checkpoint_path)\n",
    "    print(f\"saved model: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_42568\\1125866967.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_state_dict OrderedDict([('clinical_rna_feedforward.feedforward.0.weight', tensor([[-5.4621e-03, -6.1640e-03, -7.0199e-03,  ..., -1.5176e-03,\n",
      "          3.2561e-03, -7.5520e-03],\n",
      "        [-5.4274e-03, -3.6572e-03, -1.4722e-03,  ..., -1.3165e-05,\n",
      "          3.1794e-03,  4.6950e-03],\n",
      "        [ 1.3362e-05,  1.9953e-05,  8.7845e-07,  ..., -3.0789e-04,\n",
      "         -5.6543e-07, -1.5444e-04],\n",
      "        ...,\n",
      "        [-3.4046e-03,  1.8917e-03, -3.0014e-05,  ..., -5.7996e-05,\n",
      "          1.7551e-04,  1.9835e-03],\n",
      "        [ 2.8261e-03, -1.3597e-03,  2.4243e-03,  ..., -7.8967e-05,\n",
      "          1.2085e-03, -2.9615e-03],\n",
      "        [ 7.9624e-03, -4.7747e-03,  5.8598e-03,  ..., -2.0958e-04,\n",
      "         -5.5119e-03, -3.9736e-03]], device='cuda:0')), ('clinical_rna_feedforward.feedforward.0.bias', tensor([ 6.0108e-04, -6.3396e-03, -1.0113e-04, -3.1948e-03, -5.7784e-03,\n",
      "        -4.1889e-03,  5.4221e-03,  9.2889e-04, -8.7431e-04, -1.4771e-03,\n",
      "        -3.7117e-03, -7.9062e-04, -1.9237e-03,  7.7051e-05, -2.0966e-03,\n",
      "         7.2735e-03,  1.8441e-03, -1.3764e-06, -6.6735e-03,  2.3616e-03,\n",
      "         2.9491e-03,  4.7947e-03,  1.9209e-03, -1.1375e-04, -8.9705e-04,\n",
      "         3.9715e-04, -5.4176e-04,  6.1068e-03, -9.8825e-04, -1.2034e-03,\n",
      "         2.2182e-03, -1.2689e-04,  6.9694e-04,  4.0871e-03,  6.0484e-03,\n",
      "         2.7552e-04,  4.4751e-04,  5.3924e-03,  6.0238e-03, -5.5764e-03,\n",
      "        -2.0614e-03,  2.8101e-04, -1.2992e-03, -1.0609e-04, -6.6313e-05,\n",
      "        -2.3693e-03, -2.2058e-04, -9.9325e-05,  8.8606e-04,  5.2735e-03,\n",
      "        -1.4684e-03, -4.1147e-03, -1.0984e-04, -5.6881e-03, -4.4496e-03,\n",
      "         3.4841e-05,  1.8105e-05,  4.2792e-03,  1.6134e-05, -1.3046e-03,\n",
      "        -2.7897e-05,  5.9685e-03, -1.3551e-04, -1.4473e-03,  4.0617e-03,\n",
      "        -5.3270e-03, -4.2677e-03,  6.2813e-03,  5.1167e-03, -2.3674e-03,\n",
      "        -4.8703e-04, -1.7147e-03,  5.2298e-03, -2.8738e-03, -1.2113e-04,\n",
      "        -1.3291e-04, -1.8143e-03, -6.2790e-03, -2.7891e-03,  4.6185e-03,\n",
      "         9.4162e-04,  3.2265e-03, -1.1251e-05, -7.3151e-03, -2.0180e-03,\n",
      "        -9.9787e-05,  1.9040e-03,  1.1001e-05,  5.4200e-03,  1.3767e-03,\n",
      "        -6.0653e-04, -2.2736e-03,  2.5208e-03, -1.5649e-03, -5.7314e-03,\n",
      "         5.8495e-03, -4.2601e-03,  5.4532e-03,  5.2983e-04, -3.3936e-03,\n",
      "         5.9796e-04, -6.2707e-03, -1.4265e-03,  2.6434e-03,  1.6766e-03,\n",
      "         3.4685e-03, -1.9780e-03,  5.4033e-03, -4.4386e-04, -1.2175e-03,\n",
      "        -1.5817e-03, -2.1942e-03, -6.1480e-03,  5.1374e-03,  2.6031e-03,\n",
      "        -1.3850e-04, -1.6431e-03, -6.3376e-03,  2.6239e-04,  1.2891e-03,\n",
      "         5.2172e-03,  3.6044e-03, -1.2503e-03,  7.5750e-04,  6.7110e-03,\n",
      "         1.3609e-03, -1.7700e-04,  4.0900e-05,  1.4554e-03, -1.1585e-03,\n",
      "        -1.8499e-04, -2.9572e-03,  6.5019e-04,  3.6022e-03, -2.9945e-03,\n",
      "        -2.4333e-03,  4.7490e-03,  8.9177e-05, -1.3045e-03,  6.5290e-04,\n",
      "         3.0768e-03,  6.2436e-03, -3.0906e-03,  1.1833e-03, -2.4953e-04,\n",
      "        -5.2753e-03,  1.3219e-04,  5.3114e-04, -1.2312e-03,  1.2656e-03,\n",
      "        -4.7708e-03,  4.8748e-03, -4.1749e-03,  6.5624e-04,  6.6780e-04,\n",
      "         2.8766e-03, -2.9461e-03,  1.3933e-04,  1.5230e-03, -6.1672e-04,\n",
      "         1.2730e-04, -2.1393e-03, -1.3114e-03, -1.6394e-03,  4.1350e-05,\n",
      "        -2.6041e-03,  5.9851e-03, -8.1567e-04, -6.4837e-04, -2.7334e-03,\n",
      "        -9.7495e-04, -1.2492e-03,  3.9280e-03, -1.2239e-03,  7.1292e-03,\n",
      "        -5.2147e-04, -1.0060e-03, -8.1188e-06, -1.2855e-03, -3.0385e-03,\n",
      "        -2.8527e-03,  2.7620e-03,  2.2465e-03,  9.7763e-04,  9.3504e-04,\n",
      "         5.6934e-03,  4.0864e-04, -4.6313e-04, -1.6703e-03, -5.9577e-06,\n",
      "        -9.8054e-04,  8.3626e-04, -1.3770e-03,  5.8663e-03, -2.0230e-03,\n",
      "        -7.8204e-04, -1.3998e-03, -5.5738e-04,  1.7134e-03, -7.4115e-04,\n",
      "         2.2257e-05,  2.0885e-03,  3.8572e-03, -5.2031e-03, -1.0522e-05,\n",
      "        -5.0538e-05,  4.9752e-03,  1.6516e-05, -2.6540e-04, -7.0847e-03,\n",
      "        -2.4535e-03,  3.7474e-03,  2.4711e-03,  7.8393e-03, -3.1015e-03,\n",
      "        -4.7808e-03, -5.6938e-04,  1.0907e-03,  1.7008e-03, -5.9678e-03,\n",
      "        -1.2187e-04, -1.0858e-04,  4.6548e-03, -2.5736e-03, -9.6703e-04,\n",
      "         4.8707e-03, -4.0891e-03, -1.3541e-04,  2.3172e-03, -1.3415e-03,\n",
      "         1.2096e-04,  8.2663e-04,  2.5792e-03,  5.4492e-03, -1.2408e-04,\n",
      "        -1.3061e-03,  5.1100e-03,  3.3999e-04,  6.2111e-03, -5.7549e-03,\n",
      "         3.4213e-03,  2.6200e-03,  2.4043e-04,  5.5053e-03, -2.8105e-03,\n",
      "        -6.1342e-03,  3.6245e-04,  6.0339e-03,  4.9541e-03,  3.2037e-04,\n",
      "         4.1347e-05,  5.5361e-03, -3.5462e-03, -6.5189e-03,  3.8559e-03,\n",
      "         1.2621e-03,  5.7929e-03, -3.0167e-04, -4.9004e-04,  3.8006e-03,\n",
      "         8.1380e-04, -7.1575e-05, -1.3435e-04, -1.3762e-03, -2.5799e-04,\n",
      "        -3.1211e-03, -6.4645e-03, -5.3180e-03,  4.4114e-03,  1.1543e-03,\n",
      "        -6.3615e-03, -3.5543e-03,  9.2285e-04,  4.3357e-03,  4.9724e-03,\n",
      "         5.5191e-03, -1.4257e-04, -7.7728e-04, -1.5026e-03, -2.3181e-05,\n",
      "        -2.0769e-05,  1.8133e-04,  3.8803e-03, -1.0198e-03, -4.1573e-03,\n",
      "        -4.5999e-03, -2.5969e-05,  3.2206e-03,  3.4478e-03,  2.8376e-03,\n",
      "        -5.1127e-04,  5.8708e-05,  1.2438e-03, -1.5353e-03, -1.2000e-04,\n",
      "         1.2276e-03,  6.8540e-03, -1.7621e-03, -5.3607e-04,  4.3026e-04,\n",
      "         7.7216e-03,  4.8710e-04, -1.4670e-03,  1.0908e-03, -8.1293e-04,\n",
      "         5.6171e-03, -6.5327e-05, -1.8050e-03, -5.6114e-03,  4.3285e-04,\n",
      "         7.2704e-03, -5.5591e-03,  1.5413e-03, -1.5637e-03, -1.4580e-04,\n",
      "        -2.1650e-03,  3.1817e-05, -5.7495e-04, -1.9602e-03,  2.5703e-03,\n",
      "         6.2782e-03,  2.3839e-03,  1.1680e-03, -4.4983e-03, -1.0654e-06,\n",
      "         1.4865e-05, -4.4381e-04, -9.8401e-04,  2.1828e-03, -5.5826e-03,\n",
      "        -7.7023e-04,  5.2280e-03, -5.6071e-03,  6.2867e-03,  3.1690e-04,\n",
      "         6.8914e-04,  4.6263e-05, -3.6223e-03, -7.3568e-03, -5.3307e-04,\n",
      "        -1.4940e-03,  1.8786e-03,  6.6718e-03,  5.8581e-04,  3.9870e-03,\n",
      "         4.7532e-04,  1.9513e-03,  3.9323e-03, -9.1840e-05,  1.7754e-03,\n",
      "         5.6923e-03, -4.6527e-03, -1.2165e-04,  4.2611e-03,  3.8650e-03,\n",
      "         3.6334e-04, -4.6930e-03, -5.5449e-05, -1.9540e-03, -2.9536e-04,\n",
      "        -1.1002e-04, -4.1252e-04,  9.6526e-04, -1.7652e-03, -1.4288e-04,\n",
      "        -9.8161e-04, -7.0629e-03,  2.3556e-03,  1.3915e-03,  7.2769e-03,\n",
      "         1.5059e-06,  1.7972e-03, -7.3169e-03, -1.4757e-03, -1.5340e-04,\n",
      "         6.3189e-06, -3.7969e-03,  4.8158e-04,  3.6518e-04, -3.3193e-04,\n",
      "        -4.8919e-03,  5.1755e-03,  1.0995e-05, -1.1915e-03, -5.8674e-03,\n",
      "         3.3648e-04, -3.0038e-03,  6.2126e-03,  4.8569e-03,  1.1234e-03,\n",
      "        -2.7505e-04,  3.3553e-06, -9.7986e-04,  2.2738e-04, -5.6992e-03,\n",
      "         1.3090e-04, -5.3205e-04,  1.7823e-03, -3.4676e-03, -3.7529e-03,\n",
      "        -3.9524e-03,  7.2199e-07, -2.7810e-03,  4.8021e-03,  2.4981e-05,\n",
      "        -4.8898e-03,  3.7816e-03, -5.2787e-03,  6.2450e-03, -4.9677e-04,\n",
      "        -6.5855e-03, -3.3994e-03, -1.3821e-04, -6.7184e-04,  3.3874e-04,\n",
      "        -5.9883e-03,  3.3610e-03, -6.4646e-03,  3.8980e-03,  2.2441e-03,\n",
      "         1.2237e-03,  1.2016e-05,  2.1719e-03, -8.7040e-04,  4.3637e-03,\n",
      "         8.0485e-05, -4.4226e-03,  1.8775e-03, -4.8098e-03, -2.2891e-03,\n",
      "        -6.6371e-05, -1.9011e-03, -1.1880e-04,  1.3475e-03,  4.7853e-03,\n",
      "        -4.0571e-04, -1.1520e-04,  5.3308e-03,  2.8327e-03, -7.0019e-03,\n",
      "        -3.2035e-03, -5.1375e-03,  3.4474e-03, -1.4450e-04, -5.1813e-04,\n",
      "         6.2251e-03, -4.5632e-03, -4.3894e-03,  3.0361e-03, -6.2446e-03,\n",
      "         1.6508e-03, -3.9369e-05, -2.5405e-03, -2.8106e-03, -7.4574e-03,\n",
      "        -4.2717e-03, -1.2495e-04,  1.8816e-03,  4.3818e-04,  4.4695e-03,\n",
      "        -1.3329e-04,  4.8548e-03, -7.3564e-05, -6.0121e-03, -4.4400e-03,\n",
      "         5.5448e-04,  3.9887e-04,  1.4056e-03, -2.2393e-03, -4.6501e-04,\n",
      "         3.3737e-03, -1.8701e-03,  6.2652e-03,  1.4940e-04, -1.0332e-03,\n",
      "        -7.6626e-04, -3.7345e-04, -1.3459e-05, -3.7906e-03,  2.1761e-04,\n",
      "        -2.6256e-03,  2.1368e-03,  2.9636e-03,  5.8417e-03, -1.1307e-03,\n",
      "        -2.7677e-03,  1.1800e-03,  5.3391e-04,  6.4391e-03, -8.4998e-05,\n",
      "        -6.5232e-03, -4.1963e-03, -4.2929e-03, -1.3876e-03,  4.7079e-03,\n",
      "        -6.4031e-03, -2.6400e-03,  1.0780e-04, -9.4046e-04, -4.3278e-03,\n",
      "         3.5114e-03,  1.1566e-03,  6.9704e-04,  4.2982e-03,  1.6789e-04,\n",
      "        -6.2528e-03,  5.3771e-05, -5.0221e-04, -8.3199e-04,  1.0977e-03,\n",
      "         3.3812e-04, -5.3957e-03], device='cuda:0')), ('clinical_rna_feedforward.feedforward.3.weight', tensor([[-1.0912e-02,  2.1468e-02,  2.3076e-02,  ...,  5.6996e-03,\n",
      "         -4.1000e-02,  1.8762e-02],\n",
      "        [ 2.7145e-02,  4.3858e-02, -1.9045e-02,  ...,  1.0877e-02,\n",
      "         -2.3111e-02, -1.2344e-02],\n",
      "        [ 3.3079e-02,  4.5824e-03,  2.4570e-02,  ..., -8.3504e-03,\n",
      "         -1.6026e-02,  3.1652e-02],\n",
      "        ...,\n",
      "        [ 3.4629e-02,  7.2020e-03,  1.7670e-02,  ..., -2.4225e-02,\n",
      "         -1.7394e-02, -2.2528e-02],\n",
      "        [-4.0443e-02,  4.1396e-02,  5.2389e-05,  ...,  2.4602e-02,\n",
      "          9.9485e-03, -9.8377e-03],\n",
      "        [ 1.8584e-02,  9.4520e-03,  2.3789e-02,  ..., -3.4956e-02,\n",
      "         -8.7369e-03,  1.5941e-02]], device='cuda:0')), ('clinical_rna_feedforward.feedforward.3.bias', tensor([ 0.0175,  0.0217, -0.0261,  0.0102,  0.0287,  0.0069, -0.0252,  0.0093,\n",
      "        -0.0221,  0.0265, -0.0020,  0.0399,  0.0302, -0.0392, -0.0205, -0.0120,\n",
      "        -0.0325, -0.0170,  0.0296, -0.0395,  0.0389,  0.0280, -0.0099,  0.0348,\n",
      "         0.0245, -0.0150,  0.0432,  0.0441,  0.0016,  0.0310, -0.0332, -0.0331,\n",
      "         0.0433,  0.0325, -0.0008,  0.0292, -0.0221,  0.0044,  0.0205, -0.0411,\n",
      "        -0.0304, -0.0048, -0.0295,  0.0151,  0.0204,  0.0404, -0.0149, -0.0028,\n",
      "        -0.0253, -0.0217,  0.0083,  0.0277,  0.0134,  0.0090,  0.0032,  0.0098,\n",
      "        -0.0253,  0.0293,  0.0214,  0.0005,  0.0351,  0.0067,  0.0043, -0.0329,\n",
      "         0.0102, -0.0294, -0.0225, -0.0184,  0.0204, -0.0101,  0.0018,  0.0410,\n",
      "        -0.0422, -0.0111, -0.0182,  0.0243, -0.0348,  0.0294,  0.0299,  0.0323,\n",
      "        -0.0336,  0.0352,  0.0159, -0.0062,  0.0343,  0.0045, -0.0103,  0.0018,\n",
      "         0.0211,  0.0022,  0.0160,  0.0309,  0.0133,  0.0382, -0.0090, -0.0317,\n",
      "        -0.0155, -0.0118, -0.0397, -0.0110,  0.0226,  0.0198,  0.0032, -0.0011,\n",
      "        -0.0358, -0.0077, -0.0247, -0.0066,  0.0302,  0.0159, -0.0238, -0.0169,\n",
      "         0.0393, -0.0060,  0.0004, -0.0299, -0.0123, -0.0405, -0.0157,  0.0410,\n",
      "        -0.0315, -0.0048,  0.0441, -0.0383, -0.0028, -0.0068, -0.0041,  0.0197,\n",
      "        -0.0154, -0.0022, -0.0420, -0.0050,  0.0171,  0.0141,  0.0368,  0.0127,\n",
      "         0.0291,  0.0389,  0.0192, -0.0024, -0.0146, -0.0073, -0.0068,  0.0223,\n",
      "         0.0407,  0.0335,  0.0018,  0.0174,  0.0071,  0.0318,  0.0059,  0.0287,\n",
      "         0.0237, -0.0406, -0.0316, -0.0402, -0.0382,  0.0115, -0.0059, -0.0038,\n",
      "         0.0081, -0.0203, -0.0033,  0.0026, -0.0094, -0.0383,  0.0275, -0.0321,\n",
      "        -0.0280,  0.0327,  0.0231,  0.0382,  0.0255,  0.0324,  0.0303,  0.0436,\n",
      "         0.0009, -0.0439,  0.0280, -0.0168,  0.0402, -0.0005,  0.0253, -0.0230,\n",
      "        -0.0336,  0.0109,  0.0236, -0.0143, -0.0029,  0.0336,  0.0140, -0.0116,\n",
      "        -0.0183, -0.0062, -0.0374, -0.0041,  0.0147, -0.0296,  0.0041, -0.0044,\n",
      "         0.0036,  0.0078, -0.0278,  0.0003, -0.0067, -0.0388, -0.0278, -0.0313,\n",
      "         0.0189, -0.0300, -0.0133,  0.0253, -0.0127,  0.0058,  0.0171, -0.0110,\n",
      "         0.0013,  0.0127, -0.0102,  0.0173, -0.0275,  0.0313,  0.0196, -0.0340,\n",
      "        -0.0226,  0.0122, -0.0391,  0.0340, -0.0343,  0.0278,  0.0119, -0.0398,\n",
      "         0.0023, -0.0340,  0.0118, -0.0054,  0.0100, -0.0378,  0.0042,  0.0316,\n",
      "         0.0387,  0.0292,  0.0319,  0.0279,  0.0154,  0.0151,  0.0304,  0.0314,\n",
      "         0.0242,  0.0282, -0.0229,  0.0251,  0.0241, -0.0145, -0.0259, -0.0416],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.6.weight', tensor([[ 0.0077, -0.0612,  0.0231,  ...,  0.0249, -0.0596, -0.0119],\n",
      "        [-0.0046, -0.0135, -0.0039,  ...,  0.0317,  0.0151,  0.0514],\n",
      "        [-0.0517,  0.0214, -0.0561,  ..., -0.0100,  0.0359,  0.0225],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0114,  0.0186,  ..., -0.0452,  0.0461, -0.0053],\n",
      "        [ 0.0002, -0.0458,  0.0168,  ...,  0.0475,  0.0074,  0.0215],\n",
      "        [ 0.0430, -0.0413, -0.0537,  ...,  0.0095, -0.0299,  0.0339]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.6.bias', tensor([-0.0159,  0.0622, -0.0594, -0.0058, -0.0038,  0.0146,  0.0113,  0.0170,\n",
      "        -0.0176, -0.0066,  0.0204,  0.0179, -0.0357, -0.0416, -0.0475,  0.0177,\n",
      "         0.0424, -0.0398,  0.0224,  0.0280,  0.0308,  0.0286, -0.0018,  0.0108,\n",
      "        -0.0310,  0.0060, -0.0235, -0.0225,  0.0126, -0.0016,  0.0304,  0.0236,\n",
      "        -0.0526, -0.0091, -0.0081, -0.0602,  0.0471, -0.0564,  0.0367,  0.0556,\n",
      "         0.0354, -0.0344, -0.0595,  0.0421,  0.0616,  0.0341,  0.0263, -0.0140,\n",
      "        -0.0330,  0.0331, -0.0515,  0.0403, -0.0451, -0.0532, -0.0535,  0.0566,\n",
      "         0.0008,  0.0202, -0.0183,  0.0289, -0.0069, -0.0400,  0.0466,  0.0340,\n",
      "        -0.0139, -0.0294,  0.0106,  0.0487, -0.0359,  0.0026,  0.0327,  0.0068,\n",
      "         0.0467,  0.0246,  0.0538,  0.0212,  0.0099, -0.0256,  0.0390,  0.0585,\n",
      "        -0.0140, -0.0022,  0.0234,  0.0075,  0.0412,  0.0271,  0.0017, -0.0004,\n",
      "         0.0287, -0.0062, -0.0586, -0.0125,  0.0108, -0.0080,  0.0537, -0.0423,\n",
      "         0.0536, -0.0013,  0.0464, -0.0602,  0.0060, -0.0151, -0.0435,  0.0564,\n",
      "        -0.0005,  0.0366,  0.0116,  0.0592, -0.0054,  0.0087, -0.0623, -0.0553,\n",
      "        -0.0530,  0.0518,  0.0463,  0.0437,  0.0593,  0.0515,  0.0005,  0.0142,\n",
      "         0.0275, -0.0357, -0.0402, -0.0247, -0.0214, -0.0134,  0.0252, -0.0563,\n",
      "        -0.0087, -0.0344, -0.0232, -0.0580,  0.0386,  0.0474, -0.0055,  0.0241,\n",
      "         0.0558, -0.0272, -0.0300, -0.0601, -0.0212, -0.0152,  0.0498, -0.0271,\n",
      "        -0.0617, -0.0117,  0.0434, -0.0014,  0.0425, -0.0292,  0.0202, -0.0509,\n",
      "         0.0082,  0.0195,  0.0124,  0.0531, -0.0262, -0.0292, -0.0497,  0.0570,\n",
      "        -0.0411,  0.0581, -0.0055, -0.0387, -0.0160, -0.0369,  0.0208,  0.0225,\n",
      "         0.0308, -0.0007,  0.0355, -0.0183, -0.0289, -0.0417,  0.0350, -0.0169,\n",
      "         0.0295, -0.0548, -0.0116,  0.0171, -0.0118, -0.0574, -0.0315,  0.0256,\n",
      "        -0.0006,  0.0159, -0.0411,  0.0054, -0.0186, -0.0502,  0.0348, -0.0619,\n",
      "        -0.0012,  0.0397,  0.0059, -0.0222, -0.0316, -0.0031, -0.0406,  0.0025,\n",
      "        -0.0166, -0.0217,  0.0576, -0.0019,  0.0453, -0.0229,  0.0563, -0.0549,\n",
      "        -0.0376,  0.0510,  0.0314, -0.0541, -0.0195,  0.0558, -0.0518, -0.0071,\n",
      "         0.0262, -0.0331,  0.0215, -0.0282,  0.0174,  0.0510,  0.0577,  0.0030,\n",
      "         0.0237,  0.0092,  0.0439,  0.0261, -0.0144,  0.0107, -0.0200,  0.0489,\n",
      "        -0.0172, -0.0195,  0.0500, -0.0509,  0.0474, -0.0278,  0.0059,  0.0546,\n",
      "        -0.0246, -0.0406,  0.0209, -0.0533, -0.0545, -0.0447,  0.0582,  0.0437,\n",
      "         0.0593,  0.0439, -0.0288,  0.0123, -0.0024, -0.0306,  0.0574, -0.0276],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.9.weight', tensor([[ 0.0617, -0.0589,  0.0492,  ...,  0.0404,  0.0064,  0.0523],\n",
      "        [ 0.0044,  0.0177,  0.0190,  ...,  0.0647, -0.0473,  0.0605],\n",
      "        [ 0.0187, -0.0475,  0.0189,  ..., -0.0311,  0.0148,  0.0041],\n",
      "        ...,\n",
      "        [-0.0026,  0.0351, -0.0011,  ..., -0.0602,  0.0404,  0.0352],\n",
      "        [ 0.0570,  0.0149,  0.0595,  ..., -0.0601,  0.0592,  0.0614],\n",
      "        [ 0.0159, -0.0174,  0.0437,  ...,  0.0502,  0.0078,  0.0200]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.9.bias', tensor([-0.0125,  0.0591, -0.0546,  0.0501,  0.0150,  0.0461, -0.0124, -0.0346,\n",
      "         0.0416, -0.0279, -0.0160,  0.0281, -0.0517,  0.0106, -0.0005,  0.0348,\n",
      "         0.0159, -0.0424, -0.0394, -0.0325, -0.0086,  0.0253, -0.0580, -0.0092,\n",
      "         0.0543, -0.0108,  0.0496, -0.0260, -0.0031,  0.0320,  0.0037, -0.0094,\n",
      "         0.0570, -0.0536,  0.0437,  0.0079,  0.0082, -0.0542, -0.0333, -0.0023,\n",
      "        -0.0105,  0.0458,  0.0598,  0.0282, -0.0247,  0.0116, -0.0347, -0.0523,\n",
      "         0.0322,  0.0117,  0.0259, -0.0129, -0.0374, -0.0116, -0.0080, -0.0199,\n",
      "        -0.0462,  0.0487,  0.0449, -0.0492, -0.0499, -0.0519, -0.0123, -0.0266],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.12.weight', tensor([[ 0.0710,  0.0705, -0.0186,  ...,  0.0503, -0.1062, -0.0727],\n",
      "        [-0.1046,  0.0653, -0.0205,  ...,  0.0919,  0.0807, -0.0393],\n",
      "        [ 0.0919, -0.0938, -0.0982,  ..., -0.0196, -0.0343, -0.0949],\n",
      "        ...,\n",
      "        [-0.1161, -0.0159,  0.0172,  ...,  0.0548, -0.0739,  0.0389],\n",
      "        [ 0.1138,  0.0238, -0.0672,  ...,  0.0368, -0.0970,  0.0715],\n",
      "        [-0.1173,  0.0355,  0.0004,  ..., -0.1039,  0.1027, -0.0377]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.12.bias', tensor([ 0.1165,  0.0446, -0.0211, -0.1029, -0.0559,  0.1145, -0.0998,  0.0377,\n",
      "        -0.0834, -0.0261, -0.0205, -0.0888,  0.0730, -0.0316, -0.0898,  0.0912,\n",
      "        -0.0024, -0.0507,  0.0871,  0.0138, -0.0072,  0.0114,  0.1090,  0.0765,\n",
      "         0.0047,  0.0678,  0.0880, -0.1019, -0.0868,  0.0788, -0.0105, -0.0058,\n",
      "        -0.0168, -0.0419, -0.0784, -0.1045, -0.0767,  0.0436, -0.1060,  0.1152,\n",
      "        -0.0469, -0.0672, -0.0853, -0.0675,  0.0137, -0.0403, -0.0991, -0.0653,\n",
      "         0.0552, -0.0735, -0.1173, -0.0245, -0.0258, -0.1075, -0.0169,  0.0042,\n",
      "        -0.0681,  0.0681,  0.0937, -0.0158, -0.0584, -0.0156,  0.0065, -0.0806],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.15.weight', tensor([[ 0.0656, -0.0073, -0.1240,  ...,  0.1129, -0.0624, -0.1159],\n",
      "        [-0.1239, -0.0242, -0.0311,  ...,  0.1022,  0.0308,  0.0413],\n",
      "        [ 0.0395, -0.0098, -0.0144,  ..., -0.0831, -0.1071,  0.0836],\n",
      "        ...,\n",
      "        [-0.0576,  0.0815, -0.0950,  ...,  0.0763,  0.0386,  0.0661],\n",
      "        [-0.0142, -0.0108, -0.0050,  ...,  0.0172,  0.0106,  0.0714],\n",
      "        [ 0.0129,  0.0081, -0.0990,  ..., -0.0371,  0.0879,  0.0165]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.15.bias', tensor([ 0.0900,  0.0942,  0.0736,  0.0035,  0.1214,  0.0707,  0.0292, -0.0856,\n",
      "         0.1199, -0.0043, -0.0964, -0.0269,  0.0081,  0.0117,  0.0449, -0.1227,\n",
      "        -0.0449,  0.0451, -0.0585, -0.0545, -0.1128, -0.0064,  0.0278, -0.1147,\n",
      "        -0.0163,  0.1238,  0.0528, -0.1036,  0.0603, -0.0421,  0.1182,  0.0289],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.18.weight', tensor([[ 0.0095, -0.1136, -0.0818,  ...,  0.0884, -0.1389, -0.1433],\n",
      "        [-0.0793, -0.0712,  0.0106,  ...,  0.1089, -0.1486,  0.0600],\n",
      "        [-0.0466,  0.0950,  0.0857,  ..., -0.0173, -0.1564,  0.0665],\n",
      "        ...,\n",
      "        [ 0.1284, -0.0889, -0.1218,  ...,  0.1318,  0.0824,  0.0849],\n",
      "        [-0.0647, -0.1748, -0.0287,  ...,  0.0169,  0.1586, -0.0882],\n",
      "        [ 0.0003,  0.0995,  0.0327,  ...,  0.1458, -0.1549, -0.0213]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.18.bias', tensor([-0.0142, -0.1429, -0.0669,  0.1627,  0.1141, -0.0686, -0.1454,  0.0474,\n",
      "        -0.1276,  0.1043,  0.0046, -0.1604, -0.0515, -0.1385,  0.0963,  0.0506,\n",
      "         0.0708, -0.0693,  0.1077, -0.1676, -0.0900,  0.1207, -0.1287, -0.0980,\n",
      "        -0.1638,  0.1487, -0.0353, -0.1005, -0.0485, -0.0877, -0.1477,  0.0006],\n",
      "       device='cuda:0')), ('wsi_fcn.conv.weight', tensor([[[-0.0054],\n",
      "         [-0.0240],\n",
      "         [ 0.0155],\n",
      "         ...,\n",
      "         [ 0.0111],\n",
      "         [ 0.0098],\n",
      "         [ 0.0384]],\n",
      "\n",
      "        [[-0.0165],\n",
      "         [-0.0369],\n",
      "         [ 0.0232],\n",
      "         ...,\n",
      "         [ 0.0052],\n",
      "         [ 0.0201],\n",
      "         [-0.0444]],\n",
      "\n",
      "        [[ 0.0242],\n",
      "         [ 0.0066],\n",
      "         [-0.0029],\n",
      "         ...,\n",
      "         [ 0.0075],\n",
      "         [ 0.0342],\n",
      "         [ 0.0016]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0293],\n",
      "         [ 0.0310],\n",
      "         [ 0.0267],\n",
      "         ...,\n",
      "         [ 0.0101],\n",
      "         [-0.0388],\n",
      "         [-0.0343]],\n",
      "\n",
      "        [[ 0.0101],\n",
      "         [-0.0305],\n",
      "         [-0.0168],\n",
      "         ...,\n",
      "         [-0.0018],\n",
      "         [-0.0252],\n",
      "         [ 0.0312]],\n",
      "\n",
      "        [[ 0.0029],\n",
      "         [ 0.0285],\n",
      "         [-0.0133],\n",
      "         ...,\n",
      "         [ 0.0253],\n",
      "         [-0.0010],\n",
      "         [ 0.0244]]], device='cuda:0')), ('wsi_fcn.conv.bias', tensor([ 0.0183,  0.0274, -0.0253,  0.0295,  0.0008,  0.0330,  0.0008,  0.0159,\n",
      "         0.0134,  0.0367,  0.0368, -0.0215,  0.0190,  0.0420, -0.0378, -0.0109,\n",
      "        -0.0318,  0.0178, -0.0254,  0.0224,  0.0171,  0.0305, -0.0246,  0.0263,\n",
      "        -0.0302,  0.0396, -0.0324, -0.0317, -0.0299, -0.0396,  0.0008, -0.0124,\n",
      "         0.0277, -0.0394, -0.0326, -0.0121,  0.0353,  0.0375, -0.0069,  0.0254,\n",
      "         0.0175,  0.0419, -0.0030,  0.0411, -0.0217,  0.0049,  0.0300,  0.0270,\n",
      "        -0.0382,  0.0086,  0.0141, -0.0448,  0.0079, -0.0185,  0.0138,  0.0365,\n",
      "        -0.0016,  0.0190, -0.0011,  0.0265,  0.0252,  0.0290, -0.0146, -0.0104],\n",
      "       device='cuda:0')), ('attention.attention.0.weight', tensor([[-0.0855,  0.1237, -0.0585,  ...,  0.0891,  0.0429,  0.1126],\n",
      "        [ 0.0828, -0.0636,  0.0985,  ...,  0.0314, -0.0128, -0.0431],\n",
      "        [-0.1128,  0.1230,  0.0018,  ..., -0.0641, -0.0452, -0.0883],\n",
      "        ...,\n",
      "        [-0.0687,  0.0310,  0.0345,  ...,  0.0940, -0.1008,  0.1094],\n",
      "        [ 0.0524,  0.0529,  0.1003,  ..., -0.0443, -0.0813,  0.0506],\n",
      "        [-0.0373,  0.1027, -0.0240,  ...,  0.0987,  0.0344,  0.0568]],\n",
      "       device='cuda:0')), ('attention.attention.0.bias', tensor([-0.0573,  0.1089, -0.0950,  0.0464, -0.0514,  0.0877, -0.0769,  0.0445,\n",
      "        -0.0570,  0.1034,  0.0168, -0.0378,  0.0466, -0.0750,  0.0865,  0.0613,\n",
      "        -0.0721, -0.0222, -0.0496,  0.0190, -0.0802, -0.0958,  0.0606, -0.0162,\n",
      "        -0.0058, -0.0253,  0.1170, -0.1189, -0.0683, -0.0909,  0.1198, -0.0744,\n",
      "        -0.1064, -0.0753,  0.0116, -0.0958, -0.0385, -0.0063, -0.0066, -0.1209,\n",
      "        -0.0220, -0.0380,  0.0006, -0.0988, -0.0898, -0.0303,  0.1149, -0.1028,\n",
      "         0.0802,  0.0992, -0.0074,  0.0636, -0.0143,  0.1147,  0.0087, -0.0474,\n",
      "         0.1030,  0.1150, -0.0784, -0.0580,  0.0500, -0.0117, -0.0895, -0.1140],\n",
      "       device='cuda:0')), ('attention.attention.2.weight', tensor([[ 0.0875,  0.1021, -0.0982, -0.0302, -0.0282, -0.0706,  0.0253, -0.0148,\n",
      "          0.0045,  0.0809,  0.0004,  0.0685, -0.0145, -0.1220, -0.0034, -0.0476,\n",
      "         -0.0297,  0.0095,  0.1218,  0.0446, -0.0705,  0.0155, -0.0415, -0.0552,\n",
      "         -0.0836, -0.0726,  0.1182,  0.0850, -0.0405,  0.0382, -0.0690,  0.0170,\n",
      "         -0.0034,  0.0622, -0.0633,  0.1026, -0.0227, -0.0349,  0.0531,  0.0281,\n",
      "         -0.1217, -0.0287,  0.0761, -0.0080, -0.0276, -0.1219,  0.0294,  0.0893,\n",
      "          0.0002, -0.1072,  0.0720,  0.0763,  0.0971,  0.0366,  0.0794,  0.1059,\n",
      "          0.0614,  0.0201, -0.0678,  0.0175,  0.0745, -0.1029, -0.0624,  0.0508]],\n",
      "       device='cuda:0')), ('attention.attention.2.bias', tensor([-0.0552], device='cuda:0')), ('baby_feed_forward.0.weight', tensor([[-0.0528, -0.0581,  0.0932,  ..., -0.0108,  0.0603, -0.0985],\n",
      "        [ 0.0523, -0.0661,  0.0291,  ..., -0.0620,  0.0576,  0.0251],\n",
      "        [-0.0569, -0.0615,  0.0534,  ..., -0.0137,  0.0365,  0.1024],\n",
      "        ...,\n",
      "        [-0.0819,  0.0433, -0.0643,  ...,  0.0316, -0.0342, -0.0824],\n",
      "        [-0.0169, -0.0778, -0.0075,  ...,  0.0139,  0.0450,  0.0862],\n",
      "        [-0.0083, -0.0540, -0.0701,  ...,  0.0935,  0.1009,  0.0776]],\n",
      "       device='cuda:0')), ('baby_feed_forward.0.bias', tensor([-0.0895,  0.0457,  0.0080, -0.0382,  0.0431,  0.0843,  0.0334,  0.0213,\n",
      "         0.0700, -0.0485, -0.0140, -0.1028, -0.0940, -0.0851,  0.0063,  0.0473,\n",
      "         0.0055,  0.0359, -0.0459, -0.0076, -0.0361, -0.0257, -0.0468, -0.0422,\n",
      "        -0.0486, -0.0074, -0.0461, -0.0969, -0.0772,  0.0569,  0.0046,  0.0857,\n",
      "         0.0679, -0.0287,  0.0415, -0.0898, -0.0446,  0.0335,  0.0633,  0.0218,\n",
      "        -0.0120, -0.0642, -0.0871, -0.0124, -0.0094, -0.0896, -0.0573, -0.0088,\n",
      "        -0.0087, -0.0730,  0.0579,  0.0350, -0.0945,  0.0687,  0.0685,  0.0357,\n",
      "        -0.0290,  0.0820,  0.0816,  0.0800,  0.0075,  0.0233, -0.0324,  0.0605],\n",
      "       device='cuda:0')), ('baby_feed_forward.2.weight', tensor([[-0.0121, -0.0708, -0.1021,  ...,  0.0037,  0.0182,  0.1237],\n",
      "        [ 0.0522, -0.0743,  0.0725,  ..., -0.0396, -0.0635, -0.0566],\n",
      "        [-0.1010, -0.0618,  0.1251,  ...,  0.0309, -0.0686,  0.0998],\n",
      "        ...,\n",
      "        [ 0.0385, -0.1060,  0.0193,  ..., -0.0056, -0.0432, -0.0232],\n",
      "        [-0.0420, -0.0254, -0.0270,  ..., -0.0230,  0.0716, -0.0692],\n",
      "        [ 0.0713,  0.0653,  0.0032,  ...,  0.0915,  0.0951, -0.1030]],\n",
      "       device='cuda:0')), ('baby_feed_forward.2.bias', tensor([-0.0019, -0.1193, -0.1087, -0.0823, -0.0181,  0.0671, -0.0442,  0.1056,\n",
      "         0.1123, -0.0682, -0.1010,  0.0129, -0.0408, -0.0628,  0.0825,  0.0754,\n",
      "         0.0370, -0.0949,  0.1053,  0.1247,  0.0259, -0.0853, -0.1152,  0.0573,\n",
      "        -0.1207,  0.0108,  0.0734,  0.0297,  0.0152,  0.0081, -0.0496, -0.0401],\n",
      "       device='cuda:0')), ('baby_feed_forward.4.weight', tensor([[-1.4726e-01,  2.4631e-03, -5.6435e-02,  1.1140e-01,  1.6672e-01,\n",
      "         -1.4602e-01,  1.4268e-02, -6.1462e-02,  7.3468e-02, -4.9076e-02,\n",
      "          9.4807e-03, -3.7458e-02,  5.9095e-02, -1.1341e-01,  9.8433e-02,\n",
      "          9.9985e-02,  8.6931e-02, -4.0701e-02, -1.3057e-01,  1.6696e-01,\n",
      "          2.3232e-02, -1.4521e-01,  7.6511e-02, -1.2076e-01,  1.4376e-01,\n",
      "          9.1447e-02,  1.7309e-01, -3.9360e-02,  1.7669e-01,  6.7435e-02,\n",
      "          7.1931e-02,  1.2879e-01],\n",
      "        [-1.1408e-01,  6.1430e-03, -1.3250e-01, -1.0851e-01, -1.3777e-02,\n",
      "         -3.4715e-02, -6.4154e-02,  1.6018e-01, -1.3978e-01,  8.1990e-02,\n",
      "          1.6886e-01,  2.5112e-02, -3.9208e-02, -2.6525e-02, -5.3967e-02,\n",
      "          6.0326e-02, -9.7748e-02,  7.7467e-02,  1.1030e-01,  9.3941e-02,\n",
      "         -6.6450e-02, -1.5945e-02, -1.2780e-01,  7.1679e-02,  3.2485e-02,\n",
      "          2.9503e-02,  3.6164e-02, -1.4173e-01,  9.0665e-02, -1.2597e-01,\n",
      "         -1.5801e-01,  1.5066e-01],\n",
      "        [ 4.6358e-02,  9.7121e-02, -1.4429e-01,  9.0761e-02,  2.7723e-02,\n",
      "          3.2493e-02,  1.3488e-01, -9.4426e-02, -1.6143e-01, -8.1084e-02,\n",
      "          5.4901e-02, -8.8779e-02, -8.6739e-02,  1.0654e-01,  1.0858e-04,\n",
      "         -1.3732e-01, -1.1598e-01,  1.5745e-02, -8.7524e-02, -4.2491e-02,\n",
      "         -1.3874e-01,  1.0308e-01,  1.6406e-01, -1.5042e-01,  7.7436e-02,\n",
      "         -1.3954e-01, -1.4630e-01,  1.2813e-02, -1.6350e-01,  8.0402e-02,\n",
      "         -2.9271e-02, -1.1832e-01],\n",
      "        [ 1.1962e-01, -1.4305e-02,  4.9445e-02, -5.7673e-02,  2.8304e-02,\n",
      "         -1.8347e-02, -4.0273e-03,  8.1650e-02, -3.2421e-02, -1.6534e-01,\n",
      "         -7.6115e-02,  1.0543e-01, -1.4489e-02,  1.4820e-01, -1.3486e-02,\n",
      "         -6.0866e-02,  8.1966e-02,  9.8169e-02,  1.2285e-01,  6.6539e-02,\n",
      "         -1.0114e-01, -1.4002e-01,  6.1068e-02,  8.7477e-02,  1.3400e-01,\n",
      "          1.3965e-03,  1.5560e-01,  3.1433e-02,  1.5070e-01,  1.7215e-01,\n",
      "         -1.5314e-01, -7.6219e-02],\n",
      "        [-8.7444e-02,  1.1105e-01, -8.6566e-02, -1.2404e-01,  1.6844e-01,\n",
      "          4.9820e-03, -5.1593e-02,  1.4978e-01,  9.2228e-02,  4.9671e-02,\n",
      "         -1.6773e-01, -2.9694e-02, -1.5790e-02,  1.2630e-01,  2.6702e-02,\n",
      "          9.4173e-02, -1.4980e-02, -7.7723e-02,  1.7444e-01,  1.6758e-01,\n",
      "         -4.6357e-02,  1.7067e-01,  2.5737e-02,  6.2494e-02,  7.7217e-02,\n",
      "         -1.1095e-01,  3.2627e-02, -4.8085e-02, -1.3179e-01,  1.0225e-01,\n",
      "         -6.7031e-02, -6.5297e-02],\n",
      "        [ 1.4040e-01,  9.2580e-02,  1.5799e-01,  8.9725e-02, -1.4508e-01,\n",
      "         -1.2235e-01, -6.3061e-02,  3.3343e-03, -4.0623e-02, -1.2153e-01,\n",
      "          2.4127e-02, -1.2955e-01, -2.3647e-02,  8.1786e-02,  3.9430e-02,\n",
      "          1.2739e-01, -1.7262e-01, -7.4705e-02,  1.3784e-01,  8.3340e-02,\n",
      "          1.4844e-01,  3.9631e-02,  2.1334e-02, -5.0152e-02, -2.1583e-04,\n",
      "          5.8351e-02,  2.3577e-02, -9.5631e-02, -8.1182e-02,  1.0068e-01,\n",
      "          1.0043e-01,  1.6233e-01],\n",
      "        [ 7.0122e-02,  1.6259e-01, -2.7766e-03,  1.1662e-01, -1.4958e-01,\n",
      "         -4.7031e-02,  4.8181e-02, -1.2014e-01, -1.3913e-01,  7.3997e-02,\n",
      "         -1.4532e-01, -2.4479e-02, -4.2361e-03,  1.3760e-01, -7.7745e-02,\n",
      "          1.0696e-01, -1.3640e-01,  7.8522e-02, -1.0076e-01,  1.2965e-01,\n",
      "          1.4892e-01, -1.2764e-01,  1.5702e-01, -7.2277e-02, -1.6141e-02,\n",
      "         -1.5983e-01,  1.8711e-02,  1.2216e-01, -1.3914e-01, -1.1181e-01,\n",
      "         -3.5989e-02, -3.9656e-02],\n",
      "        [ 5.5279e-02, -2.3566e-02, -1.2775e-02,  5.5331e-02,  1.1919e-01,\n",
      "         -2.2528e-02,  4.4318e-02, -3.5859e-02, -1.2980e-01,  8.9109e-02,\n",
      "          6.7667e-03, -7.4797e-03,  6.4824e-02,  2.5794e-02,  1.4071e-01,\n",
      "         -5.2404e-02,  1.5282e-01, -1.1504e-01, -3.3954e-02,  1.6028e-01,\n",
      "         -5.3838e-02, -4.8695e-03,  1.4711e-01, -2.3736e-03,  1.4920e-01,\n",
      "         -1.6785e-01,  1.0632e-01,  7.8669e-02,  1.5375e-02, -8.2057e-02,\n",
      "         -1.2943e-01,  1.5582e-01],\n",
      "        [ 1.1683e-01,  3.0049e-02, -4.4926e-02,  1.4916e-01,  1.7905e-02,\n",
      "          1.0383e-01, -4.0286e-03,  2.3246e-02,  6.5012e-02,  1.2808e-01,\n",
      "          7.5060e-02,  1.1902e-01,  1.7630e-01, -2.7735e-02, -7.5357e-03,\n",
      "         -6.4025e-02,  1.2126e-01, -3.5676e-02, -8.5831e-02,  9.5062e-02,\n",
      "         -1.9849e-02,  1.6833e-01, -1.0239e-01,  1.6816e-01,  2.5433e-02,\n",
      "          1.1230e-02,  1.3153e-01,  3.5736e-02, -5.3903e-02,  5.4891e-02,\n",
      "         -9.8879e-02,  1.2163e-01],\n",
      "        [-1.1947e-01,  1.7050e-01,  4.0653e-02, -3.8649e-02, -8.1748e-02,\n",
      "          8.2915e-02,  1.4553e-01,  1.7471e-01,  1.0430e-01,  1.1030e-01,\n",
      "          2.5381e-03, -1.3669e-01, -8.1286e-02,  2.0729e-02, -1.3355e-01,\n",
      "          4.7335e-02, -9.8453e-02,  8.1112e-02, -6.4213e-02, -1.9903e-02,\n",
      "          1.1709e-01, -4.8573e-02, -2.5045e-03,  1.2019e-01,  7.7514e-02,\n",
      "          5.8542e-02,  1.2186e-01,  8.6602e-02,  1.8386e-02, -2.4500e-02,\n",
      "         -1.3617e-01,  6.8511e-02],\n",
      "        [-1.3685e-01,  1.4180e-01,  1.0305e-01,  8.2326e-02, -1.6261e-01,\n",
      "          4.7321e-02,  8.1185e-02, -1.2944e-01, -1.9839e-02, -1.6327e-01,\n",
      "          2.1071e-06, -1.0884e-01, -9.5058e-02,  2.3615e-02, -9.7578e-02,\n",
      "         -4.1634e-03,  8.6237e-02, -1.2229e-01,  4.1858e-02, -1.6052e-01,\n",
      "          1.5562e-01, -4.6699e-02,  5.9593e-02,  9.3618e-02, -1.0973e-02,\n",
      "         -3.0270e-02, -1.2509e-01,  7.4041e-02,  3.6071e-02, -1.0369e-01,\n",
      "          5.3073e-02, -1.1190e-01],\n",
      "        [ 1.5940e-01,  8.0736e-02, -8.1448e-02, -6.7214e-02,  1.8909e-02,\n",
      "          8.1920e-02,  3.8910e-02,  7.6218e-02, -5.7239e-02,  1.0309e-01,\n",
      "         -1.0398e-01,  1.4635e-01, -9.6511e-02, -8.1768e-03, -1.7449e-01,\n",
      "         -1.0851e-01, -8.3186e-02,  7.6147e-02, -1.6858e-01,  2.3963e-02,\n",
      "         -1.2819e-01, -1.8593e-02, -5.3888e-02,  8.9362e-02, -6.8946e-02,\n",
      "         -1.5971e-01, -4.5527e-02,  1.3262e-01, -3.6437e-02,  6.1541e-02,\n",
      "         -1.2217e-02,  2.7456e-02],\n",
      "        [-1.2541e-01,  4.7545e-02,  6.5874e-02, -3.2042e-02,  1.5304e-01,\n",
      "         -1.4657e-01,  1.5949e-01,  1.3852e-01, -1.4162e-01,  1.2144e-01,\n",
      "         -6.3083e-02, -4.3462e-02, -2.5907e-02,  4.4016e-03, -1.1773e-01,\n",
      "         -9.6978e-02, -1.5829e-01, -9.6966e-02,  8.3473e-02, -1.3423e-03,\n",
      "          5.1549e-02, -1.9087e-03,  9.8652e-02,  4.4344e-02, -1.0702e-01,\n",
      "         -8.1115e-02,  1.0241e-01, -9.5992e-02, -1.6287e-01, -1.0206e-01,\n",
      "         -1.0210e-01,  5.4324e-02],\n",
      "        [-1.1347e-01,  3.6185e-03, -8.6730e-02,  1.1605e-01, -9.0481e-02,\n",
      "          9.1385e-02,  6.5726e-02, -5.3509e-02,  1.2397e-01, -7.8324e-02,\n",
      "          1.3597e-01, -1.1672e-01, -1.6875e-01, -6.7043e-02, -1.9277e-06,\n",
      "          7.5192e-02,  3.3146e-02, -6.5687e-02, -5.2916e-03, -1.2037e-01,\n",
      "          1.5401e-01,  3.8306e-02, -1.6657e-01, -6.3826e-02, -5.7784e-02,\n",
      "         -1.1321e-02,  8.1548e-02, -1.3327e-01,  1.5715e-01,  1.2584e-02,\n",
      "         -1.1593e-01, -8.8935e-02],\n",
      "        [ 1.2104e-01,  3.0594e-02, -4.9297e-02,  1.4943e-01, -1.5159e-01,\n",
      "          4.4454e-02, -1.2909e-01,  4.7386e-02,  1.2367e-01, -1.4753e-01,\n",
      "          1.1865e-01,  1.9453e-02,  1.2475e-02,  1.6150e-01, -3.3531e-02,\n",
      "         -1.5833e-01, -8.2155e-02, -1.6837e-01, -1.1031e-01, -1.3913e-01,\n",
      "          8.8185e-02, -1.6677e-01,  2.9819e-02, -1.9644e-02, -2.8448e-02,\n",
      "         -8.6814e-02, -4.2390e-02, -2.8211e-02,  1.5279e-01,  1.4222e-02,\n",
      "         -1.4040e-01, -1.0513e-01],\n",
      "        [ 8.9219e-02, -3.9237e-02,  1.3179e-01,  1.5040e-02, -1.0191e-01,\n",
      "         -1.5867e-01, -3.4164e-02,  2.7070e-02, -1.6472e-01, -1.1291e-01,\n",
      "          6.5347e-02, -1.5014e-01, -8.2958e-02,  1.2578e-01,  4.6898e-02,\n",
      "          1.6892e-02,  6.1206e-02,  1.0557e-01, -6.9512e-02,  1.1970e-01,\n",
      "         -1.3762e-02, -3.4224e-02, -9.8610e-02, -7.1942e-02,  3.5890e-02,\n",
      "         -2.3796e-02,  5.6119e-02,  3.7835e-04,  7.9707e-02,  6.5602e-02,\n",
      "          4.1274e-03, -1.0750e-01]], device='cuda:0')), ('baby_feed_forward.4.bias', tensor([-0.1553,  0.0469, -0.0848,  0.1184, -0.1283, -0.1425, -0.1730,  0.0051,\n",
      "        -0.1296,  0.1234,  0.0015,  0.0632, -0.1164, -0.0482,  0.1677, -0.1324],\n",
      "       device='cuda:0')), ('baby_feed_forward.6.weight', tensor([[-0.0215, -0.0084,  0.0408,  0.0503, -0.1808, -0.0642,  0.0573,  0.1500,\n",
      "         -0.0991, -0.1813, -0.2057, -0.0140, -0.0808,  0.1349, -0.2009, -0.2408],\n",
      "        [ 0.1383,  0.2057,  0.0932,  0.1747, -0.2331, -0.0336, -0.1024, -0.0805,\n",
      "         -0.0426, -0.0039, -0.0158,  0.1819, -0.2204,  0.1453, -0.1150, -0.1278],\n",
      "        [-0.0203, -0.0892, -0.1209,  0.1953,  0.0503, -0.0395,  0.0834,  0.0166,\n",
      "         -0.0473,  0.0610,  0.2216,  0.0916, -0.1833, -0.2302, -0.0565,  0.0742],\n",
      "        [ 0.0638,  0.1354, -0.1510, -0.1966,  0.1476, -0.0810,  0.2181, -0.1204,\n",
      "         -0.2166,  0.0016, -0.0846,  0.1292,  0.0320, -0.0256, -0.1323,  0.1515],\n",
      "        [ 0.0159, -0.1987,  0.1026, -0.2214,  0.2191, -0.0757,  0.0722, -0.1480,\n",
      "         -0.1867,  0.1688, -0.1189, -0.0624, -0.1850, -0.2141, -0.2071,  0.0610],\n",
      "        [-0.0231, -0.1445,  0.0132,  0.0740,  0.0344, -0.1400,  0.1259,  0.1398,\n",
      "          0.1562,  0.0564,  0.0867,  0.0423, -0.1981, -0.0787,  0.2158,  0.0773],\n",
      "        [ 0.1707,  0.0344,  0.0549, -0.2162,  0.2108,  0.0477,  0.0713, -0.0027,\n",
      "         -0.1049,  0.2442, -0.0185, -0.0069, -0.0455, -0.1482,  0.2245, -0.0191],\n",
      "        [ 0.0652,  0.0942, -0.0153,  0.1612,  0.2232, -0.0640, -0.0326, -0.0076,\n",
      "          0.2159, -0.2392, -0.0362, -0.0832,  0.0346,  0.1098,  0.0221, -0.1802]],\n",
      "       device='cuda:0')), ('baby_feed_forward.6.bias', tensor([-0.1524,  0.0199, -0.1761, -0.0842, -0.1548,  0.1545,  0.0651,  0.2081],\n",
      "       device='cuda:0')), ('baby_feed_forward.8.weight', tensor([[ 0.0817, -0.2300, -0.0720, -0.2994,  0.3366, -0.2366,  0.2120, -0.2038]],\n",
      "       device='cuda:0')), ('baby_feed_forward.8.bias', tensor([0.0921], device='cuda:0'))])\n",
      "optimizer_state_dict {'state': {0: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 1.6196e-05, -2.8164e-05,  1.8168e-05,  ..., -2.1256e-07,\n",
      "          8.6835e-06,  1.4154e-07],\n",
      "        [-2.8931e-07, -4.0866e-07, -2.0759e-07,  ..., -3.0703e-09,\n",
      "          3.4542e-07,  5.4775e-07],\n",
      "        [ 2.3514e-10,  8.9206e-09,  1.8674e-10,  ..., -7.2316e-08,\n",
      "          7.0296e-12, -5.1888e-08],\n",
      "        ...,\n",
      "        [-4.0977e-07, -1.4084e-07, -3.1920e-09,  ...,  6.3094e-10,\n",
      "          7.3070e-08,  6.4562e-07],\n",
      "        [ 2.3177e-07, -1.9769e-07,  2.5844e-07,  ..., -1.1063e-08,\n",
      "          1.3232e-07, -2.2894e-07],\n",
      "        [ 5.7189e-05,  6.8797e-05,  4.8160e-06,  ..., -5.9400e-08,\n",
      "          7.5903e-06, -6.4061e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[1.2361e-08, 1.0251e-08, 2.4690e-10,  ..., 9.2378e-15, 4.1755e-10,\n",
      "         2.2097e-09],\n",
      "        [1.5020e-09, 4.1322e-11, 8.9955e-15,  ..., 7.2126e-17, 1.5645e-11,\n",
      "         1.4279e-10],\n",
      "        [6.8557e-18, 3.5754e-16, 9.0940e-19,  ..., 3.4576e-15, 1.7600e-20,\n",
      "         2.7851e-15],\n",
      "        ...,\n",
      "        [1.1148e-10, 2.5609e-09, 8.6827e-17,  ..., 1.7227e-16, 7.1229e-11,\n",
      "         4.7242e-09],\n",
      "        [3.3807e-10, 1.1360e-09, 4.1225e-12,  ..., 5.0444e-16, 4.1939e-11,\n",
      "         9.2160e-11],\n",
      "        [6.7753e-09, 1.3531e-08, 2.6253e-11,  ..., 3.0283e-15, 5.1480e-10,\n",
      "         3.1010e-09]], device='cuda:0')}, 1: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 3.3321e-05, -3.7686e-07, -4.4373e-08, -3.4664e-06,  4.5441e-05,\n",
      "        -3.8885e-07,  8.0297e-07,  1.2779e-07,  2.8688e-06, -2.0813e-07,\n",
      "         1.7806e-04, -4.3752e-06, -2.5660e-07,  1.2406e-09,  2.3268e-07,\n",
      "         2.6687e-05, -2.6707e-05,  7.5395e-09,  3.3874e-07, -8.2683e-05,\n",
      "         4.7681e-07, -7.1014e-05, -1.8144e-04,  2.8821e-08,  4.4353e-07,\n",
      "         8.3646e-08,  1.8929e-07,  7.3010e-07,  6.5293e-07, -5.8900e-08,\n",
      "         4.3301e-07, -1.1897e-08,  8.3989e-08,  5.4934e-07,  2.4298e-06,\n",
      "         6.8121e-08, -5.5883e-05, -9.6443e-05,  1.3619e-06, -1.2012e-04,\n",
      "        -1.1679e-05,  5.1384e-07, -1.8851e-07, -1.0983e-08,  1.7718e-08,\n",
      "         4.1511e-08,  2.0403e-07, -1.1097e-08,  6.3924e-05,  1.0777e-06,\n",
      "        -2.0717e-07, -4.0201e-07,  8.7930e-09, -3.0834e-07, -3.5149e-07,\n",
      "        -1.1527e-05, -6.1946e-09,  8.7585e-05,  8.6975e-09,  2.1095e-07,\n",
      "        -9.3524e-09,  3.1180e-06,  1.7258e-09, -8.0542e-08,  4.5667e-07,\n",
      "        -7.8069e-05, -5.9693e-08,  3.5692e-05,  9.7116e-07, -2.2476e-07,\n",
      "         4.0919e-07,  1.7541e-05,  3.9148e-06, -8.1276e-05,  6.0135e-09,\n",
      "        -4.8887e-08,  1.5463e-06,  3.9405e-05, -6.8522e-05,  6.0386e-07,\n",
      "         1.4012e-07,  5.6584e-07,  6.7778e-09, -3.2327e-08, -2.6673e-07,\n",
      "         1.1046e-08, -3.2213e-05, -2.7311e-08,  1.6728e-06,  2.2659e-07,\n",
      "         1.0154e-07,  3.0327e-05,  6.8826e-07,  5.6472e-07, -3.2576e-07,\n",
      "        -6.4298e-05,  1.2798e-07,  6.9616e-07,  6.2160e-07,  1.6670e-06,\n",
      "         2.8169e-06, -4.8235e-07,  1.0778e-08,  8.7662e-07,  4.0724e-06,\n",
      "         3.9226e-07,  5.9024e-05,  1.5516e-04, -6.9783e-05,  9.8563e-06,\n",
      "         2.0400e-06,  9.0485e-06, -4.6483e-07, -7.9404e-05,  3.1303e-07,\n",
      "        -8.4563e-09, -2.2626e-07, -6.3239e-07,  6.6404e-08,  5.8701e-07,\n",
      "        -1.5228e-05,  7.0196e-07,  1.4666e-08,  9.5502e-07,  1.6268e-06,\n",
      "        -1.0796e-04,  3.2706e-08, -3.7901e-09,  3.7694e-07, -2.0768e-05,\n",
      "         8.5556e-05,  1.8006e-06,  3.4940e-07,  6.2016e-07, -2.6298e-07,\n",
      "        -2.1055e-07,  7.4304e-07,  4.2648e-08,  4.6212e-05,  1.1474e-07,\n",
      "         2.3133e-05,  1.2360e-04, -6.9018e-05,  1.4144e-07, -4.4749e-06,\n",
      "        -2.6027e-07,  9.4345e-09,  8.1640e-07, -3.8020e-05, -2.5756e-08,\n",
      "         3.5081e-05,  6.9417e-07,  8.4401e-06,  1.1514e-07,  1.1650e-07,\n",
      "         6.9562e-07, -1.5588e-07, -2.6259e-10,  3.6482e-07,  4.6570e-07,\n",
      "        -4.3202e-09, -7.4694e-05,  4.7944e-06, -8.7265e-05,  3.5550e-08,\n",
      "         6.1222e-06,  1.1357e-06,  4.0578e-05, -8.0270e-05,  6.1548e-08,\n",
      "        -1.5210e-07, -1.8295e-07,  6.3481e-07,  3.2487e-07, -3.2087e-05,\n",
      "         1.4670e-06, -1.5563e-07, -3.0391e-08,  6.8116e-07, -1.5967e-07,\n",
      "         6.4751e-07,  4.1017e-07,  5.1073e-07,  1.3945e-06,  2.3146e-07,\n",
      "         6.0673e-07,  8.5083e-08,  1.0495e-07, -2.2922e-07,  2.3666e-10,\n",
      "        -5.2472e-08,  7.5857e-07, -1.9711e-07, -7.8975e-07, -2.6727e-07,\n",
      "        -8.1676e-06, -1.1317e-07,  9.5889e-06,  6.1577e-07, -1.2513e-07,\n",
      "        -2.5452e-08, -1.2156e-05,  7.0962e-07, -2.4908e-07,  6.8371e-09,\n",
      "         5.8139e-07,  1.0332e-06,  1.1435e-09,  3.1261e-08, -5.0755e-07,\n",
      "         4.9418e-05,  8.4149e-07,  4.8352e-07,  8.8409e-05, -3.0400e-07,\n",
      "         2.3073e-07,  3.7649e-06,  5.4732e-07,  3.9784e-07,  2.2763e-07,\n",
      "        -1.8521e-08, -4.5442e-08,  1.3343e-06, -1.3642e-04,  1.0305e-07,\n",
      "         1.7524e-05,  8.5770e-07, -8.9978e-09,  7.0875e-07,  3.3781e-07,\n",
      "         1.0399e-08,  4.9155e-07,  2.8105e-07,  1.0120e-06, -8.3876e-05,\n",
      "         2.6826e-07,  6.2512e-07,  7.6428e-08,  8.5165e-07, -5.2868e-07,\n",
      "         5.4108e-07, -2.6036e-05,  2.6911e-07,  1.3358e-06, -9.1143e-06,\n",
      "        -6.2771e-07, -8.3469e-05,  8.3282e-07,  9.2928e-07,  8.8260e-08,\n",
      "        -2.2202e-08,  1.7065e-06,  1.0580e-07, -1.4661e-07,  1.0112e-06,\n",
      "         4.9893e-06,  9.9886e-07,  5.7381e-07, -9.5139e-08,  1.8917e-06,\n",
      "         9.9094e-06,  1.6730e-08, -9.1534e-09,  3.3471e-05, -6.5826e-08,\n",
      "         1.2091e-05, -4.4270e-07,  4.5963e-05,  1.4720e-04,  1.7346e-07,\n",
      "         9.8313e-05, -3.0037e-07,  1.0430e-06,  9.7616e-05,  5.2454e-07,\n",
      "        -1.2996e-05,  2.7623e-07, -1.2935e-07, -2.1155e-07, -9.1775e-10,\n",
      "        -1.0268e-09,  1.8969e-05,  1.2563e-06, -1.8788e-08, -1.4114e-07,\n",
      "        -2.6494e-07, -6.2267e-05,  5.5972e-07,  3.8533e-07,  2.9484e-05,\n",
      "        -9.7730e-08,  1.2517e-09, -7.8305e-06,  3.1072e-07,  6.3082e-09,\n",
      "         5.9823e-07, -1.2941e-04, -7.4375e-05, -1.0074e-07,  6.8803e-08,\n",
      "        -4.3191e-05,  3.1814e-07, -6.3167e-08,  6.6892e-06,  1.5282e-06,\n",
      "         1.0560e-04, -1.8959e-05, -4.8984e-05, -1.6568e-05, -3.1060e-05,\n",
      "        -3.6862e-05,  4.9166e-08, -1.0674e-04, -2.1761e-07, -5.3233e-09,\n",
      "         7.5153e-07,  1.8026e-07, -7.1572e-05, -2.6053e-07,  8.8231e-07,\n",
      "        -7.7998e-05,  4.0345e-07,  7.2980e-07, -3.8051e-08, -2.7355e-09,\n",
      "        -3.1093e-11,  7.8147e-07, -1.5313e-07,  2.1911e-05,  1.5049e-05,\n",
      "         1.4786e-06,  7.7872e-05, -3.6556e-07,  1.0598e-06,  5.0016e-08,\n",
      "        -3.6650e-05,  2.7756e-09,  1.0502e-06,  1.4082e-07, -1.0038e-07,\n",
      "         1.7739e-06,  2.5175e-07,  2.8988e-06, -5.8288e-05,  5.2044e-06,\n",
      "         1.4794e-07,  2.5957e-07,  6.2054e-07,  1.2726e-08,  5.0601e-05,\n",
      "         1.4328e-06, -4.2489e-07, -5.1986e-06,  5.8534e-07,  1.2494e-06,\n",
      "         7.9391e-08,  4.6694e-07, -1.9747e-09,  4.4631e-07,  9.8491e-07,\n",
      "        -1.6295e-05,  1.3421e-06,  1.5100e-07, -2.3951e-07, -1.4437e-09,\n",
      "        -1.5286e-07, -1.8684e-07,  3.7820e-07,  4.1274e-07, -4.2078e-05,\n",
      "         7.7462e-09,  6.7919e-07, -4.7490e-07, -1.7388e-07, -1.3046e-06,\n",
      "         3.0105e-08,  6.9255e-08,  9.4103e-08,  9.9716e-07, -7.5400e-08,\n",
      "        -2.6459e-05,  9.9563e-07, -2.1815e-09,  7.5968e-05,  2.1237e-05,\n",
      "         8.8452e-05, -1.9522e-07,  6.4485e-07,  2.0474e-05,  1.7416e-06,\n",
      "         8.4945e-05,  2.8144e-09, -1.5266e-07,  2.6107e-07,  2.2533e-05,\n",
      "         9.5825e-09, -1.0025e-07, -1.2012e-04, -3.2826e-07,  3.6471e-05,\n",
      "         3.2206e-07, -4.4077e-10, -8.8697e-08,  6.3946e-07,  3.2014e-09,\n",
      "         1.0458e-04,  4.7587e-07, -3.3772e-07,  6.5603e-07,  1.7896e-05,\n",
      "        -3.0930e-07,  2.8987e-07,  7.1686e-10, -1.1698e-07,  7.6268e-08,\n",
      "        -4.5254e-07, -1.8834e-06, -1.2876e-07,  7.3174e-07,  3.7283e-05,\n",
      "         3.1402e-07, -6.7151e-09,  4.9395e-07,  3.4434e-05,  6.1228e-07,\n",
      "         1.1081e-08,  1.1975e-06,  2.5164e-07, -4.6909e-07, -1.7111e-07,\n",
      "         1.9158e-05,  1.2452e-05, -1.0521e-08,  1.4079e-05,  3.0669e-05,\n",
      "        -8.4717e-08,  7.5131e-09,  7.5160e-07,  6.5493e-07,  1.7836e-05,\n",
      "        -2.4040e-07,  2.8422e-05,  4.9929e-07, -5.0507e-08, -9.8565e-08,\n",
      "         1.2599e-05, -2.6970e-07, -2.0959e-07, -2.4489e-05, -4.8193e-05,\n",
      "        -1.2029e-07, -3.0422e-09, -1.8885e-07,  5.3579e-07, -4.2100e-07,\n",
      "        -4.2677e-05,  7.0711e-06,  2.5208e-07,  2.2145e-06,  5.7487e-07,\n",
      "        -9.2974e-09,  5.0198e-07,  6.5186e-09, -3.3761e-05,  1.7613e-05,\n",
      "         3.0637e-07,  2.9120e-07,  8.7652e-05, -4.8691e-08, -2.6863e-09,\n",
      "         3.5689e-07,  2.4081e-08,  1.8033e-06,  5.1189e-08,  6.6674e-07,\n",
      "         9.2841e-08, -8.0668e-08,  1.2364e-07,  8.2466e-08,  6.0476e-08,\n",
      "         7.4044e-08,  1.2146e-06,  3.5572e-07,  7.2648e-05,  1.3587e-07,\n",
      "        -1.7397e-07, -2.3846e-05,  1.0048e-07,  2.1079e-06, -1.1123e-08,\n",
      "         3.3518e-07,  1.9407e-07,  1.2022e-07, -1.9829e-07,  9.3196e-07,\n",
      "         9.4446e-06,  5.8640e-07,  1.0942e-08, -1.4817e-07, -4.1776e-07,\n",
      "         5.0245e-05,  4.5253e-07,  3.7948e-07,  9.6245e-07,  9.3392e-06,\n",
      "         4.8068e-06,  2.1927e-09,  3.4752e-05,  8.6463e-05,  5.0337e-07,\n",
      "         2.3343e-07,  1.5917e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.9475e-08, 1.5444e-09, 2.5461e-15, 1.5437e-08, 1.1663e-08, 2.2066e-11,\n",
      "        1.6100e-09, 1.0523e-14, 6.7865e-09, 9.0216e-15, 2.5817e-08, 1.0017e-08,\n",
      "        1.1507e-14, 1.4595e-16, 4.6062e-09, 1.6514e-08, 9.5563e-09, 3.1278e-16,\n",
      "        7.2271e-09, 1.5015e-08, 7.8011e-10, 2.5518e-08, 1.4070e-08, 3.7058e-11,\n",
      "        2.4983e-09, 3.8470e-15, 7.4150e-10, 3.3649e-10, 1.3334e-08, 8.7580e-11,\n",
      "        1.0513e-09, 3.5261e-14, 1.3519e-13, 4.6618e-10, 1.5641e-08, 3.3165e-15,\n",
      "        1.5740e-08, 1.0340e-08, 1.3658e-08, 2.2860e-08, 1.1596e-08, 2.9830e-11,\n",
      "        8.0916e-15, 5.9936e-16, 1.7202e-15, 1.8204e-09, 1.9365e-09, 5.7269e-16,\n",
      "        2.0709e-08, 2.4915e-09, 8.9753e-15, 2.7790e-12, 1.4408e-15, 1.5865e-09,\n",
      "        2.0299e-10, 7.2952e-12, 2.7822e-16, 1.8164e-08, 3.4937e-16, 2.7427e-09,\n",
      "        3.7485e-16, 1.4580e-08, 1.2083e-15, 9.5646e-11, 5.7985e-11, 1.9974e-08,\n",
      "        1.5069e-09, 1.1407e-08, 1.2066e-09, 4.2619e-12, 4.9424e-09, 1.1846e-08,\n",
      "        2.4749e-08, 1.3892e-08, 1.3512e-15, 2.6892e-15, 7.8712e-09, 3.3011e-08,\n",
      "        1.6082e-08, 4.7573e-10, 4.8359e-11, 2.6541e-09, 2.9243e-16, 1.1496e-08,\n",
      "        1.2060e-14, 1.5122e-15, 2.7202e-08, 2.0156e-15, 1.3728e-08, 1.8455e-10,\n",
      "        6.1675e-10, 2.6572e-08, 4.4946e-09, 6.6839e-09, 1.4303e-09, 2.6895e-08,\n",
      "        1.3368e-08, 3.1387e-10, 7.6310e-09, 3.7014e-09, 8.7093e-09, 4.8671e-10,\n",
      "        5.5103e-10, 8.8599e-09, 6.6678e-10, 4.6541e-11, 9.9951e-09, 2.8534e-08,\n",
      "        1.8632e-08, 1.8635e-08, 2.3914e-08, 1.6865e-08, 5.2294e-10, 1.8993e-08,\n",
      "        6.3679e-11, 7.9949e-16, 9.9199e-15, 2.0777e-12, 3.2593e-15, 1.1633e-08,\n",
      "        1.8870e-08, 1.5513e-09, 4.5657e-10, 7.4309e-12, 3.9858e-09, 1.8785e-08,\n",
      "        5.8634e-11, 2.2788e-16, 1.2616e-09, 4.0963e-11, 2.0317e-08, 6.3675e-09,\n",
      "        1.9053e-09, 1.5965e-09, 5.9991e-13, 2.5336e-11, 1.7010e-09, 2.4918e-15,\n",
      "        1.5195e-08, 4.9817e-15, 1.7319e-08, 1.9246e-08, 1.1667e-08, 2.8502e-12,\n",
      "        1.4366e-08, 1.6715e-09, 7.4195e-16, 8.1486e-09, 1.8986e-08, 1.4790e-08,\n",
      "        1.9578e-08, 1.0103e-09, 1.1362e-08, 4.9968e-15, 5.0492e-15, 1.0245e-09,\n",
      "        1.5694e-10, 1.1576e-15, 1.0638e-09, 4.2903e-09, 1.2956e-15, 2.3271e-08,\n",
      "        1.0211e-08, 1.0268e-08, 2.2701e-15, 3.3100e-08, 6.8340e-09, 1.3778e-08,\n",
      "        3.8005e-09, 2.6352e-09, 6.4814e-15, 7.8362e-15, 1.3840e-09, 4.7125e-09,\n",
      "        1.4969e-08, 4.6106e-11, 6.6309e-15, 2.1105e-15, 6.4052e-09, 4.8520e-10,\n",
      "        2.0523e-08, 4.2209e-10, 1.9314e-09, 5.2551e-10, 4.4655e-10, 3.0017e-11,\n",
      "        3.8973e-15, 5.3625e-10, 1.0070e-14, 2.6732e-18, 1.0944e-11, 1.0757e-08,\n",
      "        8.4940e-15, 2.0825e-08, 1.2090e-14, 4.6520e-09, 1.6803e-11, 7.1546e-11,\n",
      "        4.6634e-09, 5.3837e-15, 1.9584e-15, 2.4337e-08, 2.4804e-09, 1.7217e-09,\n",
      "        2.9393e-16, 4.9763e-09, 6.7921e-09, 2.0969e-17, 7.7337e-11, 9.4017e-10,\n",
      "        1.2880e-08, 3.4053e-09, 1.3185e-09, 2.3486e-08, 2.1834e-12, 1.1830e-08,\n",
      "        1.3739e-08, 4.5324e-09, 9.2472e-10, 1.3635e-08, 1.8557e-15, 2.5799e-15,\n",
      "        2.0455e-08, 1.0022e-08, 9.3645e-10, 1.2273e-08, 4.0904e-08, 7.6887e-16,\n",
      "        5.3756e-09, 3.7801e-11, 6.6958e-16, 2.4873e-11, 1.0711e-11, 5.1663e-09,\n",
      "        1.5180e-08, 3.7461e-09, 3.0670e-10, 3.5975e-15, 1.2594e-09, 4.2194e-11,\n",
      "        9.3418e-10, 2.7805e-08, 1.4133e-09, 1.4596e-08, 1.8628e-08, 4.3708e-13,\n",
      "        9.2145e-10, 1.1071e-09, 4.4555e-09, 7.3047e-11, 1.8584e-15, 4.7457e-09,\n",
      "        4.9861e-09, 5.9955e-09, 7.7835e-10, 1.5675e-08, 3.8085e-09, 7.9329e-14,\n",
      "        4.2548e-15, 1.9600e-08, 3.3109e-09, 1.6896e-15, 7.5955e-16, 2.0240e-08,\n",
      "        3.2401e-15, 3.0390e-09, 9.6772e-10, 1.5214e-08, 1.9036e-08, 3.3061e-11,\n",
      "        8.8787e-09, 3.7799e-11, 5.8293e-09, 1.6694e-08, 1.4540e-11, 1.3082e-08,\n",
      "        2.7356e-12, 5.5501e-15, 8.9230e-15, 2.4464e-17, 2.3033e-17, 1.0540e-10,\n",
      "        1.3130e-08, 1.6110e-10, 1.7669e-09, 8.8849e-10, 1.0132e-08, 1.3336e-09,\n",
      "        3.6631e-11, 1.6878e-08, 4.3486e-15, 1.3966e-16, 1.6621e-08, 6.7766e-09,\n",
      "        1.3608e-15, 6.2257e-09, 1.9722e-08, 2.1855e-08, 4.4585e-15, 1.4763e-11,\n",
      "        1.7058e-08, 1.7095e-09, 1.6239e-10, 2.2614e-08, 6.9953e-09, 2.7440e-08,\n",
      "        9.2474e-09, 1.1506e-08, 1.6933e-08, 2.9051e-08, 1.3363e-08, 8.6113e-09,\n",
      "        2.1143e-08, 9.4865e-15, 9.4618e-16, 1.1147e-08, 1.3106e-11, 1.3589e-08,\n",
      "        1.1720e-14, 9.2400e-09, 1.4647e-08, 6.4153e-10, 8.8732e-09, 3.9829e-09,\n",
      "        6.2962e-17, 8.2015e-18, 1.6100e-08, 6.5250e-15, 3.5083e-08, 2.0629e-08,\n",
      "        1.8783e-10, 1.6481e-08, 8.8828e-10, 4.4038e-09, 5.7989e-12, 4.1211e-09,\n",
      "        1.0653e-16, 1.2972e-08, 9.3926e-09, 4.4452e-15, 1.8713e-08, 1.1246e-14,\n",
      "        1.6286e-08, 8.6430e-10, 1.3447e-08, 2.3555e-10, 1.1668e-14, 1.2209e-09,\n",
      "        1.5650e-15, 1.2745e-08, 6.1800e-09, 3.8746e-11, 1.0608e-08, 7.2984e-10,\n",
      "        1.5665e-08, 3.6993e-15, 2.0661e-08, 1.2571e-16, 2.0421e-09, 9.4393e-09,\n",
      "        1.0234e-08, 5.2327e-13, 6.4349e-15, 1.0600e-14, 1.0966e-15, 6.5134e-15,\n",
      "        6.3352e-09, 4.7846e-10, 1.7646e-09, 1.5652e-08, 3.1874e-16, 3.5793e-14,\n",
      "        1.5391e-09, 9.4087e-15, 9.8189e-09, 2.1017e-15, 4.7391e-09, 4.2175e-15,\n",
      "        2.1796e-08, 3.5624e-15, 2.0071e-08, 5.4104e-09, 5.4071e-17, 1.5090e-08,\n",
      "        1.2241e-08, 1.6667e-08, 2.5744e-10, 9.1663e-12, 1.2802e-08, 5.2757e-09,\n",
      "        1.9912e-08, 6.4653e-17, 6.5050e-15, 1.3364e-09, 3.4755e-08, 7.3223e-16,\n",
      "        4.4407e-15, 1.9378e-08, 7.5958e-12, 2.0903e-08, 1.2121e-08, 3.6953e-18,\n",
      "        7.5526e-10, 4.5151e-10, 8.2070e-17, 1.7203e-08, 3.3584e-10, 2.2024e-09,\n",
      "        2.0003e-11, 3.8373e-10, 2.8567e-09, 4.4751e-09, 1.1735e-15, 5.0675e-15,\n",
      "        3.5921e-15, 1.7427e-10, 1.8267e-08, 6.2948e-09, 2.2700e-09, 1.6566e-08,\n",
      "        8.6428e-10, 2.9085e-16, 1.3162e-09, 2.2712e-08, 4.3647e-10, 5.0906e-16,\n",
      "        3.5278e-08, 1.1240e-14, 5.2918e-12, 5.0998e-11, 1.6797e-10, 1.6786e-08,\n",
      "        6.5804e-16, 1.2359e-08, 1.0769e-08, 3.8844e-15, 1.3998e-15, 1.1301e-09,\n",
      "        1.4363e-09, 8.8132e-09, 1.4858e-10, 1.2518e-08, 5.6297e-10, 2.7409e-15,\n",
      "        4.3790e-15, 2.2596e-08, 3.1507e-09, 1.2305e-09, 2.0389e-08, 1.1506e-08,\n",
      "        1.6557e-08, 9.7057e-17, 9.8767e-11, 5.7406e-09, 2.2081e-09, 3.4689e-08,\n",
      "        7.7378e-12, 1.1264e-14, 9.9716e-09, 3.8555e-10, 7.5066e-16, 1.8291e-12,\n",
      "        4.1841e-12, 2.2889e-08, 1.0642e-08, 1.4824e-09, 1.4868e-09, 1.7325e-08,\n",
      "        1.1151e-10, 5.9030e-13, 5.0015e-12, 1.0450e-09, 1.6584e-08, 2.7627e-15,\n",
      "        4.2929e-09, 7.1499e-13, 3.7435e-15, 3.6572e-10, 5.0091e-09, 3.0635e-15,\n",
      "        5.9213e-13, 2.2776e-09, 8.1224e-11, 2.0224e-08, 1.4561e-09, 1.6159e-10,\n",
      "        1.7835e-08, 4.4490e-15, 6.5183e-09, 5.2317e-16, 8.2393e-09, 8.8661e-09,\n",
      "        7.1045e-09, 8.5496e-15, 5.0327e-09, 3.8049e-08, 1.3168e-08, 6.0651e-16,\n",
      "        6.3163e-15, 5.8267e-12, 1.7668e-08, 2.6764e-09, 2.2615e-09, 2.3443e-09,\n",
      "        2.3781e-08, 1.8787e-08, 1.2108e-16, 6.0656e-10, 1.5559e-08, 3.6550e-09,\n",
      "        6.9091e-10, 2.0533e-08], device='cuda:0')}, 2: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-7.0885e-05,  1.9382e-06,  2.3958e-06,  ...,  6.4877e-07,\n",
      "         -5.0527e-06, -5.1087e-03],\n",
      "        [-1.6259e-03,  3.9547e-06, -1.9923e-06,  ...,  1.1930e-06,\n",
      "         -3.9144e-06,  9.1979e-05],\n",
      "        [-2.3663e-03,  2.3255e-07,  2.5453e-06,  ..., -9.1580e-07,\n",
      "         -1.4278e-06, -1.3927e-04],\n",
      "        ...,\n",
      "        [-5.8886e-05, -3.4155e-07,  1.8545e-06,  ..., -2.2695e-06,\n",
      "         -2.4254e-06,  1.3068e-02],\n",
      "        [-2.8450e-04,  3.2381e-06, -2.0263e-09,  ...,  2.5485e-06,\n",
      "          7.1331e-06, -6.7890e-03],\n",
      "        [-5.0742e-04,  1.7111e-06,  2.4671e-06,  ..., -3.5836e-06,\n",
      "         -1.6462e-06, -1.2962e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.7347e-06, 9.9549e-10, 3.9410e-13,  ..., 4.1628e-14, 1.0690e-08,\n",
      "         1.2433e-04],\n",
      "        [5.1879e-06, 4.2363e-09, 2.8099e-13,  ..., 2.6273e-10, 1.3684e-07,\n",
      "         5.4846e-05],\n",
      "        [3.8722e-06, 1.1912e-09, 4.4084e-13,  ..., 1.5259e-10, 3.0898e-09,\n",
      "         1.1698e-04],\n",
      "        ...,\n",
      "        [6.4887e-06, 2.6558e-08, 2.4674e-13,  ..., 5.3097e-10, 2.3140e-08,\n",
      "         2.2644e-04],\n",
      "        [4.1514e-06, 1.8872e-08, 1.9606e-16,  ..., 4.4187e-13, 3.3500e-07,\n",
      "         1.6837e-05],\n",
      "        [1.4846e-06, 1.3911e-08, 4.1607e-13,  ..., 8.3757e-13, 1.2886e-07,\n",
      "         6.2204e-05]], device='cuda:0')}, 3: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-9.0630e-05, -8.2463e-05, -6.5935e-05, -3.7614e-06, -1.5848e-04,\n",
      "         3.7759e-05, -2.8120e-05, -6.5349e-05,  1.3497e-04,  7.0591e-05,\n",
      "        -1.4777e-05,  6.8557e-05, -6.3710e-05, -1.5780e-04, -3.6659e-04,\n",
      "         1.5892e-04,  5.8129e-05,  2.8079e-04,  1.6937e-05, -1.4957e-05,\n",
      "         1.4009e-04,  1.5493e-04, -1.6039e-04, -1.3312e-04,  1.1718e-04,\n",
      "         4.6659e-05, -1.0301e-04, -1.7242e-04,  1.0807e-04,  2.7684e-05,\n",
      "        -7.8761e-07,  9.1820e-06,  6.9551e-05,  4.6182e-05,  4.2810e-05,\n",
      "         2.5587e-04,  2.6596e-04,  6.4407e-05, -2.1044e-05,  1.6913e-04,\n",
      "        -1.8660e-04, -1.4859e-05, -1.1078e-05,  4.6134e-05, -6.2789e-05,\n",
      "         1.2649e-04, -3.6814e-05, -1.5560e-04, -2.3235e-04, -1.2560e-04,\n",
      "        -1.2338e-04,  4.0399e-05,  1.6267e-05, -1.0499e-04,  1.4573e-04,\n",
      "        -7.6611e-06, -1.7175e-04, -1.1729e-04, -1.5458e-04, -1.4304e-04,\n",
      "         1.1065e-05,  7.9151e-05,  9.5645e-05, -7.2828e-06,  1.3659e-05,\n",
      "         7.7614e-06,  3.6450e-05, -5.1003e-05, -4.7341e-05,  1.8705e-04,\n",
      "        -1.9411e-04,  4.0068e-05, -2.2976e-04,  1.1586e-04,  6.0447e-05,\n",
      "        -7.6478e-06,  1.3718e-05,  1.4064e-05, -4.4013e-05,  3.9908e-05,\n",
      "         1.3334e-04,  3.7597e-06, -5.5593e-05,  5.2709e-05, -1.2615e-04,\n",
      "        -1.9487e-05,  3.7585e-05, -4.4460e-05, -1.6561e-04, -1.6286e-05,\n",
      "         3.3757e-04, -1.7329e-04, -1.4216e-04,  6.2494e-06, -7.8969e-05,\n",
      "         9.1260e-05, -7.6186e-06,  3.9955e-05, -7.0968e-05, -5.7366e-05,\n",
      "         5.2100e-05, -8.2981e-05,  2.6111e-06, -3.1266e-07,  5.6870e-05,\n",
      "        -1.1335e-04, -4.5924e-05,  1.4068e-04, -7.9769e-05, -1.6669e-04,\n",
      "        -4.4812e-05, -1.1286e-04,  1.6943e-05, -1.7139e-04, -1.8690e-05,\n",
      "         3.0187e-05, -4.8903e-05,  1.8303e-04,  1.4580e-05, -2.7081e-05,\n",
      "        -4.7781e-05, -4.7728e-05, -9.2357e-05,  3.4352e-05, -2.9365e-04,\n",
      "        -2.3690e-05,  6.5317e-06, -2.3155e-05,  4.7645e-05,  1.1186e-04,\n",
      "         1.0927e-05, -1.2869e-04,  1.9157e-04, -2.8289e-05, -1.6248e-04,\n",
      "         1.2489e-04,  1.4723e-05,  2.4633e-05,  1.3178e-05, -2.8078e-05,\n",
      "        -4.0029e-07, -5.4249e-05,  7.7503e-05,  1.7782e-04, -5.3480e-05,\n",
      "         4.9426e-05, -6.2166e-05, -1.3158e-04, -4.4809e-06,  7.8319e-05,\n",
      "        -1.0435e-04, -7.7949e-05, -1.0718e-05,  9.5387e-05,  2.7870e-05,\n",
      "         2.1754e-04, -1.4540e-04, -3.6189e-05,  4.8091e-05,  4.2456e-05,\n",
      "         1.6728e-04, -4.1773e-05,  1.1414e-04, -1.1599e-04, -1.7556e-05,\n",
      "        -8.3762e-05,  5.2520e-05,  5.8491e-05, -5.6069e-05, -4.3788e-05,\n",
      "         6.7750e-05, -1.0410e-04,  6.6774e-06, -1.6015e-04, -8.1964e-06,\n",
      "        -4.8983e-05,  1.7092e-04, -8.6622e-05,  1.7754e-05,  1.9363e-05,\n",
      "         2.0118e-04, -1.6577e-04, -1.5569e-04,  5.9818e-06, -8.2363e-05,\n",
      "         1.1726e-04,  2.8181e-04, -1.0500e-05,  5.4702e-05,  2.5023e-07,\n",
      "         2.3844e-04,  1.0276e-06, -1.7557e-04, -4.9033e-05, -2.7095e-04,\n",
      "        -2.1244e-05,  7.9905e-06, -2.7262e-04,  6.4500e-05, -1.3465e-04,\n",
      "        -3.8423e-05,  6.6617e-05, -9.3965e-05, -1.3500e-05, -2.2715e-04,\n",
      "         5.4323e-05, -6.7484e-05, -1.1394e-05,  7.5758e-05,  1.2195e-04,\n",
      "         1.3827e-04, -2.0201e-04,  3.3044e-05,  1.0815e-04,  2.8460e-05,\n",
      "        -2.5741e-05, -3.1971e-05, -7.8876e-05, -3.7883e-05, -6.0731e-06,\n",
      "        -1.6393e-04,  2.9525e-04,  6.1074e-06,  1.8213e-04,  3.3876e-04,\n",
      "         4.7168e-05,  2.7326e-05,  3.0875e-05,  1.7125e-04,  3.3064e-06,\n",
      "        -1.2067e-04, -8.8793e-05, -7.2999e-05,  1.0922e-05,  1.7527e-04,\n",
      "         4.7624e-05,  1.0496e-04, -9.2417e-05,  1.0708e-04,  1.3662e-05,\n",
      "         1.4051e-05, -2.9181e-05, -6.1628e-05,  2.2159e-05,  2.6971e-04,\n",
      "        -4.4387e-05,  7.1700e-05,  1.6286e-05, -6.9442e-05,  1.2081e-04,\n",
      "         5.7337e-05,  1.0167e-05,  6.8306e-06,  9.8018e-05, -8.6945e-05,\n",
      "        -5.0196e-06], device='cuda:0'), 'exp_avg_sq': tensor([2.7836e-08, 3.2104e-08, 1.5171e-08, 3.0025e-08, 1.0082e-07, 7.7607e-08,\n",
      "        4.1920e-08, 1.6620e-08, 2.8190e-08, 3.7252e-08, 6.6185e-08, 8.6212e-08,\n",
      "        7.2737e-08, 7.4530e-08, 8.0713e-08, 2.0471e-07, 1.1847e-07, 6.8023e-08,\n",
      "        2.3372e-08, 6.9765e-08, 5.0552e-08, 7.8758e-08, 3.9807e-08, 1.9086e-07,\n",
      "        7.9373e-08, 3.9325e-08, 7.4603e-08, 6.6980e-08, 3.7786e-08, 2.2862e-09,\n",
      "        4.0456e-08, 7.3508e-10, 1.0371e-07, 5.9242e-08, 6.2553e-08, 1.0716e-07,\n",
      "        9.4552e-08, 1.0758e-08, 1.6098e-08, 5.4571e-08, 5.4793e-08, 3.4193e-08,\n",
      "        3.3565e-09, 3.2277e-08, 3.8290e-08, 1.1999e-08, 2.7061e-09, 4.9140e-08,\n",
      "        5.0647e-08, 3.6058e-08, 5.4060e-08, 5.0093e-08, 7.5133e-08, 1.1196e-07,\n",
      "        8.7409e-08, 7.4184e-09, 1.5871e-07, 1.2684e-08, 4.9893e-08, 1.1905e-07,\n",
      "        2.0370e-08, 9.9625e-08, 3.6952e-08, 2.1898e-08, 7.4136e-08, 1.5703e-08,\n",
      "        2.2510e-07, 7.0069e-08, 1.3185e-07, 8.9492e-08, 1.1021e-07, 7.3799e-09,\n",
      "        7.3307e-08, 4.7488e-08, 2.5926e-08, 2.6305e-08, 3.1554e-08, 2.6528e-08,\n",
      "        1.1654e-07, 7.6284e-09, 8.0913e-08, 3.7095e-08, 3.7230e-08, 2.8752e-08,\n",
      "        6.6846e-08, 2.6851e-08, 5.9667e-08, 9.8686e-08, 4.6565e-08, 9.1315e-08,\n",
      "        2.2441e-07, 1.1867e-07, 4.9317e-08, 7.2483e-08, 5.2235e-08, 3.8584e-08,\n",
      "        7.3374e-08, 1.7164e-07, 1.1922e-08, 6.5387e-08, 4.4074e-08, 6.4455e-08,\n",
      "        5.8647e-08, 1.1113e-08, 1.2940e-08, 7.9528e-08, 2.1508e-08, 7.0611e-08,\n",
      "        7.7881e-08, 5.6419e-08, 6.7540e-08, 2.4683e-08, 2.9451e-08, 4.7426e-08,\n",
      "        8.2816e-08, 6.6763e-08, 7.3337e-08, 7.9901e-08, 2.2595e-08, 1.0049e-07,\n",
      "        1.7338e-08, 3.4265e-08, 2.5077e-08, 1.8169e-08, 1.3652e-07, 7.1800e-08,\n",
      "        5.6176e-08, 3.4647e-08, 3.8287e-08, 1.0287e-07, 5.0772e-08, 8.7005e-08,\n",
      "        3.3986e-08, 9.9843e-08, 5.9042e-08, 9.5202e-08, 1.1066e-08, 7.5467e-08,\n",
      "        1.8559e-08, 2.5736e-10, 1.7218e-08, 4.5786e-08, 5.7184e-08, 6.9608e-08,\n",
      "        9.1053e-08, 1.3803e-08, 7.3853e-08, 2.4137e-07, 7.2754e-12, 2.5176e-08,\n",
      "        5.0293e-08, 5.5706e-08, 5.2334e-08, 5.0077e-08, 2.6533e-08, 3.5747e-08,\n",
      "        6.3107e-08, 3.5502e-08, 1.6449e-08, 1.3920e-08, 2.6082e-08, 1.3415e-08,\n",
      "        2.2090e-08, 8.6883e-08, 3.1002e-08, 3.6898e-08, 6.9702e-08, 9.0589e-08,\n",
      "        2.4676e-08, 7.2374e-08, 1.2142e-07, 5.5506e-08, 3.2536e-08, 2.6914e-08,\n",
      "        5.8630e-08, 1.4724e-08, 6.7849e-08, 5.8386e-08, 5.2033e-08, 3.1742e-08,\n",
      "        9.2121e-08, 8.3734e-08, 8.0559e-08, 1.3549e-08, 4.5673e-08, 6.8090e-08,\n",
      "        1.4963e-07, 2.3207e-08, 1.4916e-08, 2.4791e-09, 4.7374e-08, 2.9253e-08,\n",
      "        1.5707e-07, 1.0215e-07, 8.4001e-08, 2.6950e-08, 2.3250e-08, 6.6744e-08,\n",
      "        1.8215e-08, 7.9516e-08, 4.4257e-08, 8.0384e-08, 2.6981e-08, 7.2700e-09,\n",
      "        7.0404e-08, 3.2937e-08, 2.0021e-08, 1.1928e-07, 5.3126e-08, 2.5639e-08,\n",
      "        6.0383e-08, 5.2236e-08, 7.0382e-08, 2.6918e-08, 7.2446e-08, 6.8658e-08,\n",
      "        1.8522e-08, 4.4455e-08, 4.7025e-08, 1.1004e-07, 1.7168e-08, 5.8546e-08,\n",
      "        1.5236e-07, 6.4070e-08, 1.2457e-07, 2.8889e-08, 7.5063e-09, 1.3424e-07,\n",
      "        5.5190e-08, 4.5479e-09, 1.0412e-07, 5.8982e-08, 8.3826e-08, 4.4569e-09,\n",
      "        2.1893e-07, 5.6042e-08, 4.8177e-08, 3.7077e-08, 6.3057e-08, 3.9061e-08,\n",
      "        7.9683e-08, 6.2009e-08, 3.3328e-08, 1.0802e-07, 6.7066e-08, 1.4420e-07,\n",
      "        2.9261e-08, 4.4613e-09, 4.5210e-08, 2.6472e-08, 1.9376e-08, 1.2229e-08,\n",
      "        3.0263e-08, 6.8087e-08, 4.8069e-08, 4.2416e-08], device='cuda:0')}, 4: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 2.6819e-04,  3.3653e-04,  2.7517e-04,  ..., -6.3895e-04,\n",
      "          1.4300e-05,  5.3362e-04],\n",
      "        [-7.2848e-04,  3.5699e-05, -1.5064e-03,  ..., -1.5402e-03,\n",
      "          1.0514e-03, -1.5118e-03],\n",
      "        [ 5.9928e-05,  6.5540e-04,  9.1780e-04,  ...,  1.9640e-04,\n",
      "         -8.5244e-04,  6.8605e-04],\n",
      "        ...,\n",
      "        [-5.9897e-04,  1.0121e-03,  1.3802e-03,  ...,  3.7632e-04,\n",
      "          1.1002e-05, -4.8673e-04],\n",
      "        [ 1.2226e-03,  5.2945e-06, -1.6198e-03,  ..., -2.1844e-03,\n",
      "          2.2888e-04,  1.5717e-04],\n",
      "        [-3.0159e-04, -4.8449e-04,  7.6874e-04,  ..., -4.0860e-04,\n",
      "          3.3017e-04,  1.4730e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[4.3101e-06, 8.0263e-07, 1.1224e-06,  ..., 6.1028e-06, 4.3170e-08,\n",
      "         1.2582e-07],\n",
      "        [1.5830e-06, 5.6718e-06, 4.6721e-06,  ..., 2.0525e-05, 2.3085e-07,\n",
      "         1.6736e-06],\n",
      "        [1.1937e-05, 1.9165e-06, 5.1593e-06,  ..., 1.4309e-05, 2.4244e-07,\n",
      "         1.4555e-06],\n",
      "        ...,\n",
      "        [2.5625e-06, 3.6881e-06, 4.6249e-06,  ..., 1.8867e-05, 4.1853e-08,\n",
      "         1.4484e-06],\n",
      "        [1.8671e-06, 1.5636e-07, 1.1377e-06,  ..., 1.5382e-05, 9.6649e-08,\n",
      "         5.8961e-07],\n",
      "        [3.8197e-06, 5.9316e-07, 1.1496e-06,  ..., 9.1228e-06, 5.9496e-07,\n",
      "         4.0961e-07]], device='cuda:0')}, 5: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-3.5364e-05, -2.1701e-04,  4.6371e-05, -6.4855e-06,  6.2433e-05,\n",
      "        -2.2000e-04, -3.5384e-04,  1.7044e-04,  4.0032e-06,  2.7686e-04,\n",
      "        -2.9467e-05, -4.5638e-04,  9.6819e-05,  7.9988e-04, -1.4377e-04,\n",
      "        -3.3496e-05, -3.4637e-05,  3.5943e-05,  9.0447e-05, -2.0898e-04,\n",
      "         3.9487e-04, -2.4132e-05,  2.4526e-04, -2.2065e-04,  1.4865e-04,\n",
      "        -7.2423e-05, -2.2745e-04,  1.4530e-04,  5.1264e-05, -1.8716e-04,\n",
      "         1.6976e-04, -1.5010e-04, -3.6285e-04,  1.4471e-05,  2.0509e-04,\n",
      "        -3.3236e-04, -2.3848e-04, -2.2486e-04,  3.1088e-04,  3.8301e-04,\n",
      "        -2.9006e-05,  1.3296e-04,  3.2908e-04,  1.0571e-04, -1.8148e-05,\n",
      "        -1.4037e-04,  1.9340e-05,  8.3154e-07, -1.5448e-05, -7.0474e-06,\n",
      "         1.3394e-04,  9.5125e-05,  8.4088e-05, -1.6368e-04, -2.7607e-04,\n",
      "         9.0531e-05, -3.2059e-04, -3.9297e-05,  5.9297e-05, -2.0004e-05,\n",
      "         1.1437e-04,  2.0444e-04, -1.2819e-04,  1.8975e-04, -2.6775e-04,\n",
      "         8.6959e-05, -2.6291e-05,  3.4842e-05,  9.1032e-05,  1.8297e-04,\n",
      "        -2.8652e-04,  2.0749e-05, -1.2496e-04, -1.7893e-04,  1.1843e-04,\n",
      "         1.0530e-04,  2.5920e-05,  9.3638e-05,  3.9743e-04, -1.6861e-05,\n",
      "         3.9959e-05, -2.7007e-04, -2.9063e-05, -5.3750e-05,  1.1719e-04,\n",
      "         1.5114e-04,  2.5390e-04, -7.6898e-05,  4.5444e-05, -4.4221e-04,\n",
      "        -3.6477e-04, -1.7792e-04, -1.4604e-05, -2.4181e-04,  2.0607e-05,\n",
      "         2.6909e-05,  5.1049e-05,  2.1821e-05,  2.5516e-05,  2.0404e-04,\n",
      "         1.9807e-04, -8.7685e-06,  1.0355e-04,  5.9208e-05,  4.0236e-04,\n",
      "         1.4866e-04,  2.1209e-04,  5.6568e-05,  1.0790e-04, -1.9180e-04,\n",
      "         1.8568e-04,  3.4416e-05, -1.6822e-04, -2.7071e-05,  3.9064e-05,\n",
      "         1.8207e-04,  1.2305e-04,  3.1949e-04, -5.4992e-05, -1.2138e-04,\n",
      "         1.1821e-05, -1.8757e-04, -9.0649e-05,  4.5202e-04,  4.0644e-05,\n",
      "         3.2327e-05, -4.5888e-04,  2.1928e-04,  1.6951e-05,  2.1267e-04,\n",
      "        -2.7878e-04, -1.4929e-04, -1.9387e-04, -3.5037e-05,  5.0496e-05,\n",
      "         9.0704e-05, -1.0553e-04,  1.4006e-05, -1.1091e-04,  1.2007e-04,\n",
      "        -1.1994e-04,  1.4270e-05,  1.2451e-04,  5.0767e-05,  2.4671e-04,\n",
      "        -3.3278e-04,  5.6631e-05,  2.4980e-04, -1.0528e-05,  1.5659e-04,\n",
      "         3.0223e-04, -1.9576e-04, -3.9024e-04,  4.6215e-05, -9.7864e-05,\n",
      "        -2.6786e-04,  1.5306e-04, -1.2189e-04,  4.8387e-05,  4.9100e-06,\n",
      "        -2.8613e-05, -9.1024e-05,  7.4068e-05, -2.0504e-04, -3.1291e-05,\n",
      "         1.3484e-04,  2.7030e-04, -1.4649e-04, -2.0876e-04, -2.0567e-04,\n",
      "         8.1067e-05,  4.5813e-04, -9.1429e-05, -4.6303e-04,  2.8864e-05,\n",
      "        -8.8509e-05, -4.2629e-06,  2.6915e-05,  1.0171e-04, -1.6965e-04,\n",
      "         2.9336e-04, -1.0201e-04, -5.6071e-05, -1.0280e-04, -2.6957e-04,\n",
      "        -2.1011e-04, -4.9432e-04,  2.9478e-04, -1.6217e-04, -1.0510e-04,\n",
      "        -3.5005e-04,  1.4634e-04,  1.6187e-04,  6.8610e-05,  2.0790e-04,\n",
      "        -1.6805e-04,  3.8894e-05,  4.0846e-05, -3.7228e-04,  1.4277e-04,\n",
      "         1.1328e-04,  2.7417e-04,  2.3159e-04,  2.3030e-04,  4.0929e-05,\n",
      "         7.5582e-05,  1.6786e-04, -2.4534e-05,  2.7146e-04,  1.8607e-04,\n",
      "        -2.1930e-04, -5.4461e-04, -9.4046e-05, -1.0815e-04,  1.9208e-04,\n",
      "        -1.7648e-04, -2.8489e-04, -1.3203e-04, -6.4738e-06, -1.4136e-04,\n",
      "        -2.0734e-04,  7.9415e-05,  6.7714e-05, -2.2975e-04, -2.9401e-04,\n",
      "        -2.5770e-04, -2.0166e-04, -1.4095e-04,  4.1741e-05,  9.5609e-05,\n",
      "         1.2639e-04, -6.3138e-05,  1.9400e-05, -1.5103e-04,  8.7333e-06,\n",
      "        -5.0419e-04, -1.8546e-05, -1.9185e-04,  6.4941e-05,  1.3883e-04,\n",
      "         1.1817e-04, -1.2150e-04, -1.0770e-04,  8.0935e-05, -1.8614e-04,\n",
      "        -2.5566e-04,  4.9484e-05,  9.1406e-05,  1.3132e-04,  2.0594e-04,\n",
      "         6.3662e-05, -7.7937e-05,  2.4093e-04,  3.7155e-05,  3.4442e-06,\n",
      "         1.5283e-04], device='cuda:0'), 'exp_avg_sq': tensor([6.2378e-08, 1.6769e-07, 9.5971e-08, 4.4029e-08, 3.9030e-07, 9.3997e-08,\n",
      "        1.1686e-07, 1.6596e-07, 7.6376e-08, 2.1939e-07, 1.1763e-07, 2.2323e-07,\n",
      "        8.8605e-08, 2.1446e-07, 2.8095e-07, 1.1165e-07, 1.3112e-07, 4.8587e-08,\n",
      "        5.1182e-08, 1.6930e-07, 2.5734e-07, 2.4195e-07, 3.4464e-07, 3.3416e-07,\n",
      "        1.3818e-07, 1.7623e-07, 1.0023e-07, 1.7569e-07, 1.9969e-07, 2.8849e-07,\n",
      "        3.3790e-07, 3.8082e-07, 2.6424e-07, 1.0550e-07, 2.2495e-07, 1.7839e-07,\n",
      "        3.1403e-07, 1.4052e-07, 2.4020e-07, 1.2316e-07, 2.2939e-07, 3.3013e-08,\n",
      "        1.8363e-07, 1.4689e-07, 1.2160e-07, 2.8819e-07, 4.3904e-08, 5.7717e-08,\n",
      "        1.6225e-07, 4.4906e-08, 1.1213e-07, 3.9142e-07, 2.4229e-07, 6.7837e-08,\n",
      "        3.0618e-07, 1.7644e-07, 2.5390e-07, 1.0505e-07, 7.4437e-08, 8.2033e-08,\n",
      "        2.2133e-07, 3.5475e-07, 1.7347e-07, 1.7336e-07, 6.9790e-08, 2.0031e-08,\n",
      "        1.2958e-07, 1.0251e-07, 5.9828e-08, 1.3794e-07, 1.7631e-07, 3.6007e-08,\n",
      "        9.8802e-08, 1.0667e-07, 2.1898e-07, 6.5071e-08, 1.3023e-07, 3.3744e-08,\n",
      "        6.7046e-07, 1.5142e-07, 1.9216e-07, 6.4353e-08, 1.9419e-07, 3.9285e-07,\n",
      "        2.0161e-07, 3.2690e-07, 1.2579e-07, 6.3979e-08, 2.4677e-07, 2.6139e-07,\n",
      "        1.0500e-07, 1.3860e-07, 1.0808e-07, 1.4154e-07, 1.6775e-07, 1.3229e-07,\n",
      "        1.8182e-07, 5.0009e-08, 2.8256e-08, 1.2190e-07, 9.6212e-08, 1.0032e-07,\n",
      "        1.4391e-07, 1.9370e-07, 2.4800e-07, 5.6241e-08, 2.1799e-07, 1.3930e-07,\n",
      "        5.1798e-08, 1.3756e-07, 1.4815e-07, 4.9901e-08, 1.4045e-07, 3.6779e-08,\n",
      "        1.3145e-07, 2.0461e-07, 1.1974e-07, 1.9733e-07, 3.3949e-08, 8.8541e-08,\n",
      "        1.6310e-07, 2.7026e-07, 7.7653e-08, 2.6812e-07, 3.1179e-07, 1.7296e-08,\n",
      "        1.3326e-07, 2.1960e-07, 1.4762e-07, 6.1644e-08, 8.6513e-08, 7.1863e-08,\n",
      "        2.3503e-07, 2.7755e-07, 1.1913e-07, 2.1306e-07, 1.0448e-07, 2.4968e-07,\n",
      "        1.1973e-07, 5.7256e-08, 2.5890e-07, 9.1979e-08, 2.4306e-07, 1.1822e-07,\n",
      "        9.2205e-08, 2.6551e-07, 3.0931e-08, 1.2158e-07, 9.2446e-09, 4.6729e-07,\n",
      "        1.2635e-07, 1.1515e-07, 7.5001e-08, 1.8549e-07, 2.3112e-07, 1.4074e-07,\n",
      "        1.3309e-07, 1.8972e-07, 1.4653e-07, 7.8514e-08, 3.9777e-08, 6.3633e-08,\n",
      "        2.1284e-07, 2.4021e-07, 9.7134e-08, 2.7368e-07, 1.8823e-07, 2.4056e-07,\n",
      "        1.5031e-07, 1.1141e-07, 2.6078e-07, 1.4823e-07, 2.8615e-08, 2.7039e-07,\n",
      "        7.9845e-08, 2.4362e-07, 1.9010e-07, 1.0283e-07, 1.9775e-07, 2.4360e-07,\n",
      "        2.0137e-07, 4.0701e-07, 1.3205e-07, 1.5753e-07, 2.4464e-07, 3.3916e-07,\n",
      "        1.5893e-07, 1.7515e-07, 1.0924e-07, 1.1246e-07, 1.6635e-07, 1.7738e-07,\n",
      "        1.5252e-07, 8.9902e-08, 1.6092e-07, 1.6430e-07, 4.2484e-07, 7.1168e-08,\n",
      "        1.7397e-07, 1.2926e-07, 1.2915e-07, 4.6798e-08, 1.2036e-07, 1.8943e-07,\n",
      "        8.5200e-09, 1.3789e-07, 1.3012e-07, 2.3438e-07, 7.7490e-08, 3.0812e-07,\n",
      "        1.9139e-07, 4.8601e-07, 2.6182e-07, 1.5306e-07, 1.1895e-07, 2.2099e-07,\n",
      "        1.1071e-07, 5.5264e-07, 1.4878e-07, 2.6564e-07, 2.9628e-07, 4.0321e-07,\n",
      "        1.1311e-07, 2.9969e-07, 1.1265e-07, 8.5412e-08, 1.8604e-07, 3.3819e-07,\n",
      "        1.0113e-07, 1.4504e-07, 4.8145e-07, 3.4256e-07, 1.1935e-07, 1.8909e-07,\n",
      "        6.4251e-08, 2.2262e-07, 1.3126e-07, 2.2351e-07, 7.2924e-08, 7.6561e-08,\n",
      "        3.9476e-08, 1.0385e-07, 1.7161e-07, 1.0305e-07, 1.2579e-07, 1.6269e-07,\n",
      "        7.5738e-08, 1.7077e-07, 7.5277e-08, 5.4988e-07, 2.8097e-07, 1.6067e-07,\n",
      "        2.3776e-07, 1.3147e-07, 2.0927e-07, 7.0528e-08], device='cuda:0')}, 6: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-0.0003, -0.0012, -0.0036,  ...,  0.0061,  0.0011,  0.0003],\n",
      "        [ 0.0016, -0.0079, -0.0010,  ..., -0.0063,  0.0008,  0.0004],\n",
      "        [ 0.0012, -0.0039, -0.0006,  ...,  0.0013, -0.0005, -0.0011],\n",
      "        ...,\n",
      "        [-0.0007, -0.0046,  0.0074,  ...,  0.0088, -0.0039, -0.0006],\n",
      "        [-0.0006,  0.0085,  0.0013,  ..., -0.0042, -0.0041, -0.0033],\n",
      "        [ 0.0015,  0.0039, -0.0034,  ..., -0.0007, -0.0004,  0.0002]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[3.2504e-06, 6.4548e-06, 1.4868e-05,  ..., 4.5104e-05, 9.2011e-07,\n",
      "         1.8170e-06],\n",
      "        [7.7476e-06, 1.4261e-04, 1.6673e-05,  ..., 4.5199e-04, 6.4095e-05,\n",
      "         5.6421e-06],\n",
      "        [3.9870e-06, 1.2693e-05, 1.2497e-05,  ..., 3.7751e-05, 1.5373e-06,\n",
      "         3.7749e-06],\n",
      "        ...,\n",
      "        [1.6897e-06, 1.0160e-05, 1.4665e-05,  ..., 4.6379e-05, 1.0172e-05,\n",
      "         1.4271e-06],\n",
      "        [8.0487e-06, 1.0983e-04, 2.2993e-05,  ..., 1.7507e-04, 2.5053e-05,\n",
      "         5.4009e-06],\n",
      "        [6.5306e-06, 1.2477e-05, 1.1895e-05,  ..., 4.0435e-05, 1.7431e-06,\n",
      "         1.2987e-06]], device='cuda:0')}, 7: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-2.6442e-04, -7.5459e-04,  1.0512e-04,  7.0600e-04,  9.4020e-04,\n",
      "         5.9184e-04, -4.8354e-04,  5.8312e-04, -6.1133e-04, -4.2958e-04,\n",
      "         3.3716e-04, -9.3848e-04,  6.2133e-04, -8.0447e-04,  4.7254e-04,\n",
      "        -5.1334e-04, -6.2395e-04,  4.7245e-04,  1.3141e-03,  2.2811e-04,\n",
      "        -7.4097e-04, -6.9498e-04, -5.9210e-04,  2.2636e-04,  2.4695e-04,\n",
      "        -6.0090e-04, -4.2947e-04, -8.6988e-05,  5.0244e-04,  3.4238e-04,\n",
      "        -4.7457e-04,  4.4232e-04,  5.9560e-04,  2.4807e-04, -5.0771e-04,\n",
      "        -7.6893e-04, -3.1888e-04, -5.4231e-04,  1.1390e-03, -1.5676e-04,\n",
      "        -1.1948e-03,  2.4171e-04, -1.1110e-04,  2.5843e-04, -4.6004e-05,\n",
      "        -2.3079e-04,  2.1541e-04, -2.8414e-04, -6.0022e-04,  1.5030e-03,\n",
      "         1.0618e-03,  4.0981e-04, -7.0086e-04,  1.4855e-03, -2.4727e-04,\n",
      "         1.3441e-03,  8.0683e-05, -2.3712e-05,  1.0676e-03,  1.9405e-04,\n",
      "         5.6954e-05,  5.9889e-04, -6.8509e-05,  8.1204e-06], device='cuda:0'), 'exp_avg_sq': tensor([4.6769e-07, 3.4715e-06, 6.4086e-07, 1.1229e-06, 2.4650e-06, 9.8711e-07,\n",
      "        1.0121e-06, 1.3657e-06, 2.1257e-06, 1.5985e-06, 7.4626e-07, 2.7418e-06,\n",
      "        4.9419e-06, 2.5062e-06, 3.4682e-06, 4.0726e-06, 7.3815e-07, 1.3564e-06,\n",
      "        2.7962e-06, 2.0812e-06, 2.3344e-06, 8.0202e-07, 1.1070e-06, 8.1645e-07,\n",
      "        6.4986e-07, 1.5263e-06, 1.2171e-06, 1.5933e-06, 4.8590e-07, 3.4971e-06,\n",
      "        2.7123e-06, 9.5633e-07, 1.0372e-06, 1.0272e-06, 5.4622e-06, 2.9240e-06,\n",
      "        2.1263e-06, 3.4461e-06, 2.0130e-06, 2.9054e-06, 1.4583e-06, 1.9111e-06,\n",
      "        1.2645e-06, 1.4591e-06, 4.2699e-06, 2.8688e-06, 1.2415e-06, 3.2400e-06,\n",
      "        9.4330e-07, 2.7615e-06, 3.7666e-06, 1.1842e-06, 1.6039e-06, 1.3309e-06,\n",
      "        1.7421e-06, 1.5287e-06, 1.9018e-06, 2.3241e-06, 1.3441e-06, 1.7441e-07,\n",
      "        1.2995e-06, 8.7593e-07, 1.2889e-06, 1.0619e-06], device='cuda:0')}, 8: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-7.1174e-04,  7.2123e-04, -7.3162e-05,  ..., -2.4679e-03,\n",
      "          3.4754e-03, -1.0647e-03],\n",
      "        [ 4.0397e-04,  2.4023e-02, -3.0689e-03,  ...,  5.9578e-03,\n",
      "          4.7273e-03,  2.2783e-03],\n",
      "        [ 3.0886e-03,  4.4198e-03, -4.6706e-04,  ...,  1.1049e-03,\n",
      "         -4.1390e-03,  1.1746e-03],\n",
      "        ...,\n",
      "        [-2.6728e-03, -1.7891e-02, -3.4279e-03,  ...,  5.8098e-03,\n",
      "         -4.3984e-03, -8.2910e-03],\n",
      "        [-7.7937e-04,  7.0478e-03, -9.8843e-04,  ...,  1.6623e-03,\n",
      "          9.7732e-03,  2.6076e-04],\n",
      "        [ 1.3288e-03, -3.7775e-03,  1.3508e-03,  ..., -2.7779e-03,\n",
      "         -4.4309e-03,  1.0047e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.1694e-05, 5.8300e-05, 2.2406e-06,  ..., 3.7505e-06, 2.0092e-05,\n",
      "         1.4365e-05],\n",
      "        [2.9383e-05, 2.8493e-04, 3.1410e-05,  ..., 1.1518e-05, 1.6731e-04,\n",
      "         9.4452e-06],\n",
      "        [3.1398e-05, 1.6327e-04, 1.1479e-05,  ..., 1.7723e-05, 1.4538e-04,\n",
      "         2.4003e-05],\n",
      "        ...,\n",
      "        [3.0229e-05, 1.4531e-04, 1.2963e-05,  ..., 5.6566e-06, 8.1298e-05,\n",
      "         3.6048e-05],\n",
      "        [1.5517e-05, 7.0314e-05, 4.6687e-06,  ..., 6.0864e-06, 2.6372e-05,\n",
      "         8.7674e-06],\n",
      "        [2.2484e-05, 1.1981e-04, 3.1877e-05,  ..., 4.5790e-06, 1.7651e-04,\n",
      "         1.8568e-05]], device='cuda:0')}, 9: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 4.2475e-04,  6.5168e-04, -1.3725e-03,  4.3809e-04,  1.4419e-03,\n",
      "        -1.6819e-03,  2.1708e-04, -7.1564e-04, -7.7725e-04, -1.0481e-03,\n",
      "        -2.3884e-03, -1.6075e-03,  1.0201e-03, -1.1486e-03,  1.6200e-04,\n",
      "         1.1045e-03,  1.6530e-03,  8.9472e-04,  2.8168e-04,  1.6240e-03,\n",
      "         3.5194e-04, -7.1380e-04, -7.3182e-05,  9.6115e-04,  1.1903e-03,\n",
      "        -1.0103e-03, -8.3657e-04,  2.3341e-04,  8.2783e-04,  1.0965e-03,\n",
      "         1.4688e-03,  5.3350e-04,  3.6693e-03,  1.2666e-04, -1.9662e-04,\n",
      "         1.0227e-03, -9.2857e-05,  6.0212e-04,  3.2424e-04,  6.6736e-04,\n",
      "        -7.9866e-04,  1.2829e-03,  1.1217e-04, -6.8597e-05,  2.5483e-04,\n",
      "        -5.5735e-04,  2.4603e-04,  1.7837e-03, -1.3042e-04, -1.8571e-04,\n",
      "        -1.2148e-04, -1.4364e-03, -2.8703e-03,  9.5949e-04,  8.9408e-04,\n",
      "         2.9032e-04,  6.4781e-04,  6.0092e-05, -1.0687e-03, -5.4466e-04,\n",
      "         3.9397e-04, -8.0678e-04,  5.0366e-04, -1.4425e-03], device='cuda:0'), 'exp_avg_sq': tensor([6.5422e-06, 7.3644e-06, 1.0258e-05, 2.2340e-06, 4.6647e-06, 5.8545e-06,\n",
      "        3.1415e-06, 9.2636e-06, 4.6681e-06, 3.2654e-06, 5.0806e-06, 2.3771e-06,\n",
      "        4.8301e-06, 6.1500e-06, 3.7274e-06, 1.8868e-06, 7.9507e-06, 3.6181e-06,\n",
      "        3.9373e-06, 7.5713e-06, 6.3186e-06, 3.1775e-06, 1.2245e-05, 2.0675e-06,\n",
      "        3.1207e-06, 7.7072e-06, 5.5936e-06, 7.0796e-06, 8.7437e-06, 4.8139e-06,\n",
      "        1.0883e-05, 6.3055e-06, 1.2438e-05, 7.9617e-06, 3.1992e-06, 2.8525e-06,\n",
      "        1.8883e-06, 6.8301e-06, 3.2061e-06, 4.6860e-06, 2.5403e-06, 3.4072e-06,\n",
      "        7.5699e-06, 3.5197e-06, 3.3579e-06, 2.6192e-06, 1.1370e-06, 5.9079e-06,\n",
      "        2.8934e-06, 4.6916e-06, 5.9203e-06, 3.2054e-06, 1.1530e-05, 2.6140e-06,\n",
      "        4.0537e-06, 5.4461e-06, 1.0230e-05, 5.3972e-06, 3.5172e-06, 5.9270e-06,\n",
      "        1.5196e-06, 5.7454e-06, 3.9383e-06, 3.0240e-06], device='cuda:0')}, 10: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 0.0014,  0.0157,  0.0071,  ...,  0.0095,  0.0035, -0.0020],\n",
      "        [ 0.0005, -0.0010, -0.0031,  ..., -0.0051, -0.0015, -0.0135],\n",
      "        [ 0.0011,  0.0032,  0.0002,  ...,  0.0002,  0.0020, -0.0018],\n",
      "        ...,\n",
      "        [-0.0013, -0.0012,  0.0013,  ...,  0.0039,  0.0005, -0.0063],\n",
      "        [ 0.0046,  0.0166, -0.0183,  ...,  0.0075,  0.0057, -0.0082],\n",
      "        [-0.0048,  0.0109, -0.0007,  ...,  0.0031, -0.0038,  0.0064]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[3.0052e-05, 3.4367e-04, 1.3513e-04,  ..., 2.2426e-04, 1.7059e-05,\n",
      "         1.3588e-04],\n",
      "        [1.8418e-05, 1.4495e-04, 6.3923e-05,  ..., 2.3020e-04, 1.2075e-05,\n",
      "         2.5697e-04],\n",
      "        [1.6996e-05, 1.0792e-04, 5.8241e-05,  ..., 3.5268e-05, 1.8442e-06,\n",
      "         2.1441e-04],\n",
      "        ...,\n",
      "        [1.5936e-05, 2.0981e-04, 1.8656e-04,  ..., 2.3466e-04, 1.2799e-05,\n",
      "         5.1361e-04],\n",
      "        [2.9097e-05, 3.7156e-04, 4.6122e-04,  ..., 3.4306e-04, 2.0480e-05,\n",
      "         5.1975e-04],\n",
      "        [1.9942e-05, 1.1164e-04, 6.0649e-05,  ..., 4.9474e-05, 1.2801e-05,\n",
      "         5.7045e-05]], device='cuda:0')}, 11: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 0.0012, -0.0004,  0.0016, -0.0037, -0.0006,  0.0011,  0.0004,  0.0017,\n",
      "         0.0015,  0.0005,  0.0032, -0.0014, -0.0024, -0.0032,  0.0046, -0.0020,\n",
      "        -0.0025,  0.0008,  0.0009, -0.0001,  0.0040,  0.0028, -0.0025,  0.0042,\n",
      "        -0.0031, -0.0042, -0.0009,  0.0002,  0.0023, -0.0002,  0.0023, -0.0001],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([3.6078e-05, 4.3379e-05, 1.5882e-05, 1.2423e-05, 3.9161e-05, 3.2228e-05,\n",
      "        1.1956e-05, 1.6863e-05, 2.8551e-05, 7.2619e-05, 1.6737e-05, 1.2164e-05,\n",
      "        4.7892e-05, 6.1168e-05, 1.5445e-05, 3.6881e-05, 2.4465e-05, 4.8280e-05,\n",
      "        1.1296e-05, 1.3070e-05, 2.3183e-05, 3.7512e-05, 3.3716e-05, 4.5776e-05,\n",
      "        1.7060e-05, 2.6700e-05, 2.3407e-05, 1.0906e-05, 2.8537e-05, 3.4552e-05,\n",
      "        6.5498e-05, 1.5077e-05], device='cuda:0')}, 12: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 5.5867e-03,  6.4343e-03,  3.5592e-03,  ...,  1.4023e-03,\n",
      "          1.1587e-02, -3.5292e-03],\n",
      "        [-7.0586e-03, -8.2417e-04,  1.8652e-03,  ..., -1.2375e-04,\n",
      "          1.6585e-03,  1.7177e-03],\n",
      "        [-4.1020e-03, -5.8897e-03, -2.3451e-04,  ..., -4.8800e-03,\n",
      "          4.7283e-03, -6.6054e-04],\n",
      "        ...,\n",
      "        [ 5.2394e-03,  5.0035e-03,  1.4857e-02,  ...,  3.7123e-02,\n",
      "          1.7495e-02,  2.6912e-04],\n",
      "        [-2.5002e-04,  2.0217e-03,  1.7813e-03,  ...,  8.7852e-03,\n",
      "          1.3489e-02,  4.3133e-03],\n",
      "        [ 1.5282e-02,  1.1173e-02,  5.9831e-03,  ...,  2.5282e-03,\n",
      "          8.0669e-05,  1.1202e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[3.0213e-04, 2.4354e-05, 2.6690e-05,  ..., 2.5865e-04, 3.3627e-05,\n",
      "         1.1224e-05],\n",
      "        [4.2960e-05, 3.5264e-06, 1.1841e-06,  ..., 2.9234e-05, 2.9311e-06,\n",
      "         6.0167e-06],\n",
      "        [8.8614e-05, 4.4392e-05, 2.7976e-05,  ..., 6.9691e-05, 3.7510e-05,\n",
      "         1.1641e-05],\n",
      "        ...,\n",
      "        [8.3155e-04, 1.1012e-04, 5.7052e-05,  ..., 2.7489e-04, 2.3547e-04,\n",
      "         3.7142e-05],\n",
      "        [1.8898e-04, 3.3292e-05, 2.3364e-05,  ..., 7.1343e-05, 6.3013e-05,\n",
      "         1.0090e-05],\n",
      "        [2.5998e-04, 8.1081e-05, 1.9227e-04,  ..., 1.8948e-04, 8.1440e-05,\n",
      "         5.2163e-05]], device='cuda:0')}, 13: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 4.1985e-03,  1.8585e-04, -1.8944e-04, -1.9678e-03, -2.2914e-03,\n",
      "         1.2855e-03,  7.5970e-03,  3.2352e-04,  3.4648e-03,  3.8172e-03,\n",
      "         5.4379e-04,  1.2686e-03, -7.6197e-03,  3.9908e-03,  2.3333e-03,\n",
      "        -6.3945e-05, -2.9904e-03, -1.5594e-03, -1.4197e-04, -2.8398e-03,\n",
      "         5.6048e-03, -2.0680e-03,  2.3115e-03,  1.6873e-03, -1.4992e-03,\n",
      "         6.5483e-03, -1.5969e-04, -1.9271e-03, -4.0922e-03,  1.4260e-02,\n",
      "         1.2620e-02, -5.9467e-06], device='cuda:0'), 'exp_avg_sq': tensor([4.5207e-05, 8.9930e-06, 3.8341e-05, 4.2269e-05, 6.1425e-05, 1.7678e-05,\n",
      "        9.0728e-05, 8.6697e-05, 1.1482e-05, 5.5756e-05, 6.6596e-05, 1.2650e-05,\n",
      "        8.6471e-05, 4.9600e-05, 3.1730e-05, 7.2270e-05, 2.8551e-04, 4.6787e-05,\n",
      "        8.8116e-05, 1.7081e-05, 1.9831e-04, 9.6861e-05, 3.5338e-05, 8.4707e-05,\n",
      "        9.6558e-06, 4.4523e-05, 2.0165e-05, 6.5198e-05, 1.7714e-05, 1.9830e-04,\n",
      "        3.7399e-05, 2.0310e-04], device='cuda:0')}, 14: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[[ 2.1962e-03],\n",
      "         [ 8.1478e-04],\n",
      "         [ 9.9806e-04],\n",
      "         ...,\n",
      "         [ 1.9013e-03],\n",
      "         [ 5.9584e-04],\n",
      "         [ 1.1855e-03]],\n",
      "\n",
      "        [[-5.0935e-05],\n",
      "         [-2.4955e-04],\n",
      "         [-2.9881e-04],\n",
      "         ...,\n",
      "         [-1.6408e-04],\n",
      "         [ 6.6579e-04],\n",
      "         [ 8.7679e-05]],\n",
      "\n",
      "        [[ 4.3487e-03],\n",
      "         [ 8.3144e-04],\n",
      "         [ 1.1937e-03],\n",
      "         ...,\n",
      "         [ 2.9281e-03],\n",
      "         [ 1.0543e-03],\n",
      "         [ 7.5090e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4569e-03],\n",
      "         [ 6.2571e-04],\n",
      "         [ 7.2322e-04],\n",
      "         ...,\n",
      "         [ 2.0349e-03],\n",
      "         [ 5.8567e-04],\n",
      "         [ 1.5110e-03]],\n",
      "\n",
      "        [[ 4.4913e-03],\n",
      "         [ 6.4389e-04],\n",
      "         [ 4.3598e-04],\n",
      "         ...,\n",
      "         [ 3.1676e-03],\n",
      "         [ 5.7814e-04],\n",
      "         [ 6.3784e-04]],\n",
      "\n",
      "        [[ 1.8344e-04],\n",
      "         [ 2.5326e-05],\n",
      "         [ 7.8968e-06],\n",
      "         ...,\n",
      "         [ 2.1144e-04],\n",
      "         [ 1.5368e-05],\n",
      "         [ 1.6286e-04]]], device='cuda:0'), 'exp_avg_sq': tensor([[[3.4308e-05],\n",
      "         [3.9470e-06],\n",
      "         [3.3494e-06],\n",
      "         ...,\n",
      "         [2.4107e-05],\n",
      "         [1.5080e-06],\n",
      "         [3.6027e-06]],\n",
      "\n",
      "        [[4.5383e-05],\n",
      "         [1.1967e-06],\n",
      "         [2.3025e-06],\n",
      "         ...,\n",
      "         [2.5635e-05],\n",
      "         [1.4530e-06],\n",
      "         [4.3164e-06]],\n",
      "\n",
      "        [[3.4726e-05],\n",
      "         [2.8053e-06],\n",
      "         [3.2035e-06],\n",
      "         ...,\n",
      "         [2.0245e-05],\n",
      "         [1.5836e-06],\n",
      "         [3.1901e-06]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.1930e-05],\n",
      "         [7.1891e-07],\n",
      "         [9.0873e-07],\n",
      "         ...,\n",
      "         [8.2645e-06],\n",
      "         [3.2456e-07],\n",
      "         [1.0023e-06]],\n",
      "\n",
      "        [[2.0359e-05],\n",
      "         [2.8830e-06],\n",
      "         [2.2731e-06],\n",
      "         ...,\n",
      "         [1.6779e-05],\n",
      "         [1.3294e-06],\n",
      "         [2.6468e-06]],\n",
      "\n",
      "        [[2.7919e-08],\n",
      "         [1.7263e-09],\n",
      "         [5.7685e-09],\n",
      "         ...,\n",
      "         [2.5127e-08],\n",
      "         [3.1860e-09],\n",
      "         [8.9279e-09]]], device='cuda:0')}, 15: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 2.1698e-03,  7.4045e-04,  2.4069e-03,  1.2350e-03,  2.9425e-04,\n",
      "         7.7621e-03, -6.1585e-03, -2.5590e-03,  1.0889e-03,  4.7105e-03,\n",
      "         2.4996e-03,  8.3691e-03,  2.4621e-04,  2.4451e-04,  1.0842e-04,\n",
      "         2.7473e-03, -1.7003e-03, -4.1019e-03,  7.2297e-04, -5.0023e-04,\n",
      "         1.0665e-03, -1.3737e-03, -4.6980e-03, -3.0183e-04, -1.0425e-03,\n",
      "        -5.0449e-03,  2.0173e-03,  3.9916e-03,  5.4867e-05,  5.5241e-03,\n",
      "        -1.0200e-03, -3.0649e-05,  4.5350e-03,  1.9505e-04,  1.6883e-03,\n",
      "        -4.7290e-03,  6.5594e-04, -1.5280e-04, -3.1646e-03, -3.2076e-03,\n",
      "        -2.3007e-04,  3.3467e-03, -2.1686e-04,  1.4442e-03, -1.1559e-03,\n",
      "        -4.4977e-04, -4.4913e-03,  2.3112e-03,  1.1539e-06, -6.2945e-06,\n",
      "        -4.3294e-04, -8.3558e-04, -1.4630e-03, -2.8704e-03, -4.6728e-04,\n",
      "        -4.1159e-05,  2.0023e-03, -1.1342e-03, -4.3897e-03, -2.8450e-03,\n",
      "        -3.5912e-04,  3.7683e-03,  1.4429e-03,  5.0280e-04], device='cuda:0'), 'exp_avg_sq': tensor([3.2732e-05, 3.5095e-05, 2.0446e-05, 7.7330e-06, 6.4777e-06, 5.9128e-05,\n",
      "        5.4839e-05, 1.8194e-05, 3.0075e-06, 3.0532e-05, 4.9779e-05, 4.3087e-05,\n",
      "        6.4621e-06, 2.7749e-06, 3.3686e-08, 1.5095e-05, 2.0595e-06, 1.6314e-05,\n",
      "        4.6711e-06, 3.6835e-05, 4.9720e-06, 5.8129e-05, 1.2387e-05, 4.3473e-07,\n",
      "        1.2511e-05, 2.5703e-05, 1.6613e-05, 3.5462e-05, 7.4658e-08, 6.9890e-05,\n",
      "        1.1501e-05, 3.8732e-07, 5.0610e-05, 8.9431e-06, 1.7797e-05, 1.3626e-05,\n",
      "        5.8275e-05, 8.1859e-06, 3.2353e-05, 1.9206e-05, 3.9217e-06, 1.8761e-05,\n",
      "        2.3226e-05, 2.2441e-05, 1.9469e-05, 4.5309e-07, 2.2891e-05, 2.9935e-05,\n",
      "        3.1931e-09, 1.1104e-10, 2.3845e-05, 9.4539e-06, 1.4717e-05, 5.1821e-05,\n",
      "        6.5520e-07, 3.9713e-08, 3.2438e-06, 5.0758e-05, 3.9570e-05, 3.2587e-05,\n",
      "        9.5207e-06, 1.0542e-05, 2.1618e-05, 1.4112e-07], device='cuda:0')}, 16: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 2.5588e-06, -8.0213e-06,  2.3678e-05,  ..., -4.2923e-05,\n",
      "         -7.1870e-05,  2.2106e-05],\n",
      "        [ 3.4148e-05, -1.4609e-05,  6.2637e-05,  ..., -5.7242e-05,\n",
      "         -9.6146e-05,  1.0038e-05],\n",
      "        [-3.8128e-05,  1.3694e-05, -5.4260e-05,  ...,  5.8743e-05,\n",
      "          8.9143e-05, -2.2399e-05],\n",
      "        ...,\n",
      "        [-3.6983e-05,  3.9920e-06, -5.4765e-05,  ...,  7.3900e-05,\n",
      "          8.3138e-05, -3.6052e-06],\n",
      "        [-1.0500e-05,  9.8834e-06, -1.9578e-05,  ...,  3.5346e-05,\n",
      "          4.8091e-05, -3.0575e-06],\n",
      "        [ 2.3178e-06,  5.7743e-07,  1.4818e-05,  ..., -2.0998e-05,\n",
      "         -3.8931e-05,  1.2364e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[2.4157e-09, 2.2995e-09, 4.6249e-09,  ..., 1.0553e-09, 1.6827e-09,\n",
      "         1.4140e-10],\n",
      "        [5.5135e-09, 4.9647e-09, 1.0454e-08,  ..., 1.8700e-09, 2.7727e-09,\n",
      "         1.7921e-10],\n",
      "        [5.0486e-09, 4.4422e-09, 9.6284e-09,  ..., 1.7264e-09, 2.4298e-09,\n",
      "         1.8021e-10],\n",
      "        ...,\n",
      "        [5.4384e-09, 4.9882e-09, 1.0693e-08,  ..., 1.9291e-09, 2.4154e-09,\n",
      "         1.6761e-10],\n",
      "        [1.8642e-09, 1.8891e-09, 3.6620e-09,  ..., 6.7413e-10, 9.2390e-10,\n",
      "         5.9988e-11],\n",
      "        [5.6842e-10, 6.3675e-10, 1.3170e-09,  ..., 3.3615e-10, 5.5151e-10,\n",
      "         4.5601e-11]], device='cuda:0')}, 17: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-5.0081e-05,  1.4926e-05, -6.7031e-06,  1.1261e-05, -2.9497e-06,\n",
      "         1.5241e-07, -1.0650e-05,  5.6751e-06, -5.1275e-06,  9.7981e-06,\n",
      "         1.7086e-06, -1.2205e-05,  6.2396e-06, -7.7994e-06,  8.3721e-06,\n",
      "         1.0940e-05, -7.4733e-06, -2.3900e-06, -3.7487e-05,  6.5354e-06,\n",
      "        -1.8212e-05, -1.3768e-05,  7.5895e-06, -4.4438e-07, -2.5056e-05,\n",
      "         1.1047e-05, -9.4996e-06, -3.7627e-05, -1.9941e-05, -1.9526e-05,\n",
      "         1.5068e-05, -1.8235e-05, -9.1455e-06, -1.6288e-05, -1.4994e-05,\n",
      "        -1.7895e-05, -1.1632e-06, -1.0876e-05, -2.0075e-08, -1.1586e-05,\n",
      "        -6.1340e-06, -3.5608e-06,  1.3445e-06, -1.0365e-05, -8.0026e-06,\n",
      "        -1.0019e-05,  1.0892e-05, -7.3912e-06,  8.0868e-06, -7.4825e-06,\n",
      "        -2.1809e-06,  1.1613e-05, -9.9335e-05,  1.4345e-05,  1.0726e-05,\n",
      "        -5.7498e-06,  1.6080e-05,  1.6100e-05, -1.0539e-05, -6.2054e-06,\n",
      "        -7.5932e-06, -1.1616e-05,  4.2893e-06, -3.5526e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.8301e-09, 1.8331e-10, 1.7184e-10, 4.8864e-11, 4.0710e-11, 6.3852e-11,\n",
      "        1.7368e-11, 7.2943e-11, 1.8729e-12, 2.1153e-09, 2.3482e-13, 1.7403e-11,\n",
      "        2.9764e-12, 7.3777e-11, 5.2187e-12, 7.8654e-11, 5.2663e-12, 7.2376e-13,\n",
      "        2.7798e-09, 3.2167e-10, 7.3947e-11, 4.7338e-11, 4.4755e-11, 1.3662e-11,\n",
      "        7.7109e-10, 8.9752e-11, 4.1917e-10, 9.2374e-10, 1.6084e-10, 8.8446e-11,\n",
      "        1.0501e-10, 2.2235e-10, 5.9506e-12, 5.1671e-11, 1.2793e-09, 5.3413e-10,\n",
      "        2.6896e-12, 3.5194e-11, 3.2780e-12, 3.0880e-11, 3.7674e-10, 2.4815e-12,\n",
      "        3.1006e-11, 7.1966e-12, 8.7216e-12, 1.1015e-10, 2.2193e-11, 1.1429e-10,\n",
      "        4.0444e-12, 3.5224e-10, 1.0326e-10, 8.3919e-10, 9.3850e-09, 2.3373e-10,\n",
      "        1.3756e-10, 3.7807e-11, 6.1168e-11, 6.4190e-11, 5.4674e-10, 3.2997e-12,\n",
      "        1.4517e-10, 1.1668e-10, 9.3613e-11, 8.2920e-10], device='cuda:0')}, 18: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 5.3092e-04,  8.6477e-05, -2.3864e-05,  5.4166e-04,  4.5614e-04,\n",
      "         -2.4574e-04, -2.3370e-04,  8.7990e-05,  6.3983e-04, -1.6659e-05,\n",
      "         -4.0829e-04,  5.9922e-04, -7.6050e-04, -1.9525e-04, -7.3392e-05,\n",
      "         -2.6838e-04, -8.5122e-05, -4.9381e-04,  6.4896e-04, -1.1098e-04,\n",
      "          3.1147e-04, -2.8519e-04,  2.4384e-05, -6.1772e-05, -5.8446e-04,\n",
      "          6.5431e-04, -6.0028e-04, -6.8497e-04, -5.0420e-04,  5.2809e-04,\n",
      "         -4.1938e-04, -6.8916e-04, -7.1769e-04,  6.9273e-04,  2.5804e-04,\n",
      "          2.9443e-04,  5.2084e-04, -8.3064e-04, -3.7023e-06,  1.0299e-04,\n",
      "          3.1854e-04,  1.7324e-04, -5.5090e-04,  1.3149e-04, -3.9080e-04,\n",
      "          1.5651e-04,  1.2510e-04,  1.8652e-04,  3.3487e-04,  2.0938e-07,\n",
      "          4.1866e-05, -1.0865e-04, -9.4901e-04, -5.9112e-05, -2.3058e-04,\n",
      "         -2.9796e-04, -3.1943e-04, -2.5909e-04, -2.3262e-04,  5.1259e-04,\n",
      "         -3.7127e-04,  7.5352e-04, -3.6493e-04, -4.5810e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[4.0654e-07, 4.0741e-07, 4.1881e-07, 2.8630e-07, 8.1457e-07, 7.2042e-08,\n",
      "         6.0423e-08, 5.7726e-07, 1.7379e-07, 5.6093e-07, 3.2125e-07, 8.3094e-08,\n",
      "         2.5110e-07, 2.5892e-07, 1.6328e-07, 1.9930e-07, 1.2597e-07, 4.6790e-07,\n",
      "         6.7294e-07, 7.1817e-07, 1.0227e-07, 1.8271e-07, 6.5900e-08, 3.8942e-07,\n",
      "         6.9224e-07, 2.8801e-07, 5.7880e-07, 6.6750e-07, 3.1555e-07, 1.8387e-07,\n",
      "         2.8319e-07, 8.6284e-07, 2.5580e-07, 1.7512e-07, 4.5656e-07, 6.9114e-07,\n",
      "         1.5157e-07, 1.3679e-06, 1.1709e-07, 1.0014e-07, 4.3479e-07, 1.1113e-07,\n",
      "         5.7249e-07, 1.8516e-07, 9.8684e-08, 7.4335e-07, 3.1730e-07, 3.1843e-07,\n",
      "         2.9287e-07, 2.3319e-07, 1.3878e-07, 1.3005e-06, 1.3360e-06, 5.5719e-07,\n",
      "         1.9676e-07, 1.1281e-07, 7.3116e-08, 4.3259e-07, 6.7138e-07, 1.7906e-07,\n",
      "         2.3709e-07, 5.5291e-07, 1.3236e-07, 4.0198e-07]], device='cuda:0')}, 19: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-5.6096e-06], device='cuda:0'), 'exp_avg_sq': tensor([1.9756e-12], device='cuda:0')}, 20: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 3.6237e-03,  1.6270e-04,  1.7079e-02,  ..., -5.7400e-04,\n",
      "          8.2586e-04, -2.4806e-05],\n",
      "        [-2.1220e-02,  1.3760e-03, -9.4988e-04,  ..., -2.1404e-03,\n",
      "          5.6277e-04, -9.2687e-05],\n",
      "        [ 8.5389e-03,  1.0569e-03, -1.1574e-02,  ...,  1.0654e-03,\n",
      "         -4.2141e-04,  1.2360e-04],\n",
      "        ...,\n",
      "        [ 7.6999e-04, -4.3681e-04,  1.8176e-04,  ...,  9.4464e-04,\n",
      "          1.9068e-04,  9.5006e-05],\n",
      "        [ 7.4580e-03,  6.2540e-03,  2.1937e-02,  ...,  4.4547e-03,\n",
      "          3.5250e-03,  1.5512e-04],\n",
      "        [ 5.5407e-03, -1.1239e-03, -1.8830e-02,  ...,  2.6345e-03,\n",
      "         -5.4063e-04,  3.4554e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[9.2998e-05, 3.9168e-06, 4.9404e-04,  ..., 9.4804e-06, 2.9715e-05,\n",
      "         1.6629e-08],\n",
      "        [2.1367e-04, 7.9488e-06, 6.0878e-04,  ..., 1.1609e-05, 2.2076e-05,\n",
      "         4.1081e-08],\n",
      "        [4.8216e-05, 7.3526e-06, 3.2965e-04,  ..., 3.2723e-06, 5.6029e-06,\n",
      "         1.3261e-08],\n",
      "        ...,\n",
      "        [1.4722e-06, 5.7345e-07, 1.2828e-06,  ..., 1.0492e-06, 8.2383e-07,\n",
      "         1.6083e-09],\n",
      "        [2.2393e-04, 1.0078e-05, 3.0397e-04,  ..., 1.2694e-05, 8.7898e-06,\n",
      "         2.2992e-08],\n",
      "        [2.3100e-04, 6.5034e-06, 2.5151e-04,  ..., 9.9705e-06, 1.6627e-05,\n",
      "         2.2748e-08]], device='cuda:0')}, 21: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-0.0014, -0.0059,  0.0035,  0.0002,  0.0065, -0.0005, -0.0083, -0.0070,\n",
      "        -0.0030,  0.0144, -0.0050,  0.0086,  0.0020,  0.0043,  0.0033,  0.0086,\n",
      "         0.0015, -0.0089,  0.0024,  0.0139, -0.0070, -0.0083,  0.0019, -0.0011,\n",
      "        -0.0039, -0.0114, -0.0033,  0.0007,  0.0055,  0.0005, -0.0088,  0.0091,\n",
      "         0.0032,  0.0081,  0.0010,  0.0232,  0.0032,  0.0056,  0.0036, -0.0113,\n",
      "        -0.0013, -0.0021, -0.0018, -0.0030,  0.0031, -0.0044, -0.0010,  0.0020,\n",
      "         0.0024, -0.0052,  0.0103,  0.0016, -0.0019, -0.0079, -0.0005,  0.0055,\n",
      "         0.0185,  0.0041, -0.0021,  0.0073,  0.0195,  0.0038,  0.0149,  0.0071],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([6.2731e-04, 5.3194e-04, 8.0491e-05, 1.1077e-06, 1.3338e-04, 6.6268e-05,\n",
      "        9.1708e-05, 5.6400e-05, 1.8491e-04, 3.0442e-04, 8.5595e-05, 9.1036e-05,\n",
      "        7.2798e-05, 1.3635e-04, 2.7807e-05, 1.3860e-04, 1.5460e-04, 1.1687e-04,\n",
      "        1.0888e-04, 6.3538e-04, 4.3780e-05, 1.4223e-03, 2.4945e-04, 2.2684e-04,\n",
      "        7.5860e-05, 8.4017e-05, 2.9586e-04, 3.6358e-04, 1.3707e-04, 7.1821e-05,\n",
      "        1.3120e-04, 4.3426e-04, 1.7788e-04, 5.4106e-04, 3.4852e-05, 4.3031e-04,\n",
      "        3.8740e-04, 2.2255e-04, 3.7314e-05, 1.3608e-04, 7.4123e-06, 5.2100e-06,\n",
      "        1.5892e-04, 6.0044e-06, 1.1131e-04, 6.9277e-05, 8.3063e-05, 5.8017e-05,\n",
      "        4.8226e-06, 3.5914e-05, 1.5275e-04, 1.0306e-04, 1.6886e-04, 1.4649e-04,\n",
      "        6.1120e-05, 2.0883e-04, 2.6919e-04, 1.0540e-04, 6.0230e-05, 2.8456e-05,\n",
      "        4.2320e-04, 1.9854e-05, 2.7584e-04, 3.2744e-04], device='cuda:0')}, 22: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-6.1049e-03, -6.6066e-04, -8.8669e-04,  ..., -2.9342e-03,\n",
      "         -2.5106e-03,  1.0670e-02],\n",
      "        [-3.2123e-03, -3.8389e-03, -7.2085e-03,  ..., -1.0071e-03,\n",
      "         -1.8399e-03, -4.4993e-03],\n",
      "        [-2.5072e-03, -5.0593e-04, -2.9998e-03,  ..., -3.9254e-04,\n",
      "          6.8676e-05, -1.7632e-03],\n",
      "        ...,\n",
      "        [-2.1548e-04,  4.6592e-03, -6.9282e-04,  ...,  3.5017e-04,\n",
      "          5.0070e-04, -7.9736e-03],\n",
      "        [-3.5906e-03,  1.9572e-05, -4.5617e-05,  ..., -8.5597e-04,\n",
      "          7.2417e-06,  6.1231e-03],\n",
      "        [-1.7263e-03,  5.7296e-04, -4.3641e-04,  ..., -1.7018e-04,\n",
      "         -3.9567e-05,  4.0307e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[6.3413e-05, 2.0080e-05, 2.9456e-05,  ..., 6.5880e-06, 1.1467e-05,\n",
      "         1.4391e-04],\n",
      "        [7.9653e-05, 2.5276e-05, 6.4117e-05,  ..., 8.6946e-07, 9.0010e-06,\n",
      "         6.1987e-05],\n",
      "        [3.1917e-06, 1.0899e-06, 3.7625e-06,  ..., 4.1557e-08, 8.4161e-07,\n",
      "         1.1167e-05],\n",
      "        ...,\n",
      "        [9.1408e-05, 4.0041e-05, 7.6467e-05,  ..., 5.2718e-06, 3.2459e-06,\n",
      "         1.4211e-04],\n",
      "        [6.3250e-06, 1.1379e-08, 2.2275e-06,  ..., 4.5723e-07, 3.2399e-12,\n",
      "         1.1108e-05],\n",
      "        [4.5926e-06, 1.3314e-06, 8.1281e-07,  ..., 1.2015e-07, 4.1060e-07,\n",
      "         2.1462e-05]], device='cuda:0')}, 23: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 4.5282e-02, -2.5882e-02, -7.8645e-03, -8.3118e-06,  1.8346e-02,\n",
      "         1.4177e-02,  6.7042e-03,  9.3513e-03,  6.3676e-03, -2.0429e-03,\n",
      "        -3.0163e-03,  3.2151e-02,  2.8600e-02, -1.5601e-02, -6.4315e-03,\n",
      "         8.6440e-04,  3.9262e-02,  1.9528e-03, -2.7442e-03,  2.1561e-02,\n",
      "        -8.1947e-03,  6.0862e-03, -1.0623e-02,  4.3047e-02, -3.4747e-03,\n",
      "         1.6646e-02,  2.3619e-02,  4.4814e-03, -2.1212e-02,  1.2266e-02,\n",
      "         2.1929e-03, -1.6951e-03], device='cuda:0'), 'exp_avg_sq': tensor([1.3957e-03, 8.1683e-04, 5.6594e-05, 4.2384e-12, 7.9640e-04, 5.3765e-04,\n",
      "        3.1491e-04, 5.3917e-04, 6.8418e-04, 2.9982e-05, 1.0997e-04, 9.0084e-04,\n",
      "        1.4179e-03, 2.5119e-04, 1.9294e-04, 7.4251e-04, 1.1128e-03, 3.7745e-04,\n",
      "        7.9820e-04, 1.1342e-03, 2.2600e-04, 4.4057e-03, 3.7837e-04, 1.5682e-03,\n",
      "        3.8121e-05, 2.0591e-04, 1.0050e-03, 1.7847e-04, 1.1772e-03, 2.6137e-03,\n",
      "        6.0284e-05, 4.6103e-05], device='cuda:0')}, 24: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-5.8440e-05, -1.6243e-04, -2.1123e-04,  1.1218e-05, -1.7842e-04,\n",
      "         -6.8056e-04, -2.6775e-05, -1.1828e-03, -8.7630e-04, -4.2691e-06,\n",
      "         -2.4892e-03, -6.6339e-03, -3.2996e-03,  7.4141e-05, -3.4871e-03,\n",
      "         -2.1431e-04, -2.8488e-03, -2.3680e-03, -1.6812e-03, -6.5061e-03,\n",
      "         -3.2361e-03, -4.5930e-04, -2.5083e-03, -1.4350e-03,  4.6633e-05,\n",
      "         -5.6981e-05, -4.0974e-03, -2.4640e-03,  1.5973e-05,  2.2330e-05,\n",
      "          3.6733e-05, -2.2785e-04],\n",
      "        [ 1.4175e-03,  3.6876e-03, -1.3322e-03, -1.0929e-05,  2.6031e-03,\n",
      "          4.4008e-04, -9.0656e-04,  8.2412e-03,  7.1254e-03,  1.0270e-04,\n",
      "         -1.7516e-03,  4.2838e-03, -1.1757e-03, -1.1685e-03,  2.5355e-03,\n",
      "         -4.4031e-04,  1.5291e-02,  8.4527e-05, -1.8179e-03,  4.4373e-03,\n",
      "         -1.2765e-03, -2.8607e-03, -1.9063e-03, -9.6198e-04, -5.5654e-04,\n",
      "          2.0640e-03, -2.0109e-03,  5.0145e-03,  7.1935e-04, -5.3259e-04,\n",
      "         -2.7909e-04,  2.5608e-05],\n",
      "        [ 4.7228e-06,  9.7917e-06, -1.4501e-05,  9.1567e-06,  2.8606e-06,\n",
      "          3.3375e-06,  1.3562e-05, -9.5227e-06, -1.6212e-05, -8.1906e-06,\n",
      "          5.5760e-06, -8.9589e-06, -8.7552e-06,  1.0732e-05,  1.0921e-08,\n",
      "         -1.3805e-05, -1.1674e-05,  1.6616e-06, -8.8335e-06, -4.3365e-06,\n",
      "         -1.3947e-05,  1.0387e-05,  1.6474e-05, -1.5112e-05,  7.8263e-06,\n",
      "         -1.4027e-05, -1.4701e-05,  1.3673e-06, -1.6418e-05,  8.1224e-06,\n",
      "         -3.0154e-06, -1.1908e-05],\n",
      "        [ 1.0023e-03,  1.6806e-02, -3.1231e-02, -5.8529e-06,  3.8573e-02,\n",
      "         -2.4442e-02, -6.3045e-03,  3.7917e-02, -1.2047e-02,  7.4999e-04,\n",
      "         -3.8255e-03, -1.5818e-02, -1.7737e-02, -1.6598e-02, -8.9027e-03,\n",
      "         -9.6768e-03,  4.6012e-02, -2.4348e-02,  4.3028e-03, -5.3201e-02,\n",
      "         -6.4423e-02, -2.1744e-02, -1.0329e-02,  9.9034e-03,  1.5772e-03,\n",
      "          1.1530e-02, -9.2430e-03,  1.0923e-02,  1.1420e-03, -9.8877e-03,\n",
      "         -1.4498e-03,  7.6978e-04],\n",
      "        [-4.0904e-04, -8.4676e-03,  4.3422e-03, -1.2479e-05, -6.2326e-03,\n",
      "          7.4624e-03,  3.3000e-03, -6.2930e-03,  4.8094e-03, -2.1563e-04,\n",
      "          1.5444e-03,  2.0548e-03,  5.2698e-03, -1.0175e-03,  7.8900e-03,\n",
      "          3.3466e-03, -1.1037e-02,  6.1459e-03, -8.5610e-03,  1.4768e-02,\n",
      "          7.9954e-03,  8.3193e-03,  8.0992e-04, -1.7465e-03,  3.1564e-05,\n",
      "         -6.0735e-04,  1.1827e-02, -3.7011e-03, -9.1641e-04,  9.5734e-04,\n",
      "          4.6546e-04, -1.4483e-03],\n",
      "        [ 1.3345e-03, -6.2708e-04,  3.9028e-03,  9.0533e-06, -1.4579e-05,\n",
      "         -3.7155e-03,  2.4258e-03, -1.1196e-03,  8.7108e-03, -1.2229e-05,\n",
      "         -1.6696e-03, -2.9552e-03,  8.6319e-03,  5.2592e-03,  4.2778e-03,\n",
      "          4.4749e-03, -1.5747e-04,  1.8809e-03,  2.1229e-03, -9.6983e-04,\n",
      "          4.3448e-03,  3.9454e-03,  2.5962e-04,  6.0490e-04,  5.3067e-04,\n",
      "          1.6805e-03,  8.0457e-03, -8.2399e-04, -5.6419e-04,  4.2035e-03,\n",
      "          7.3809e-04, -1.9301e-03],\n",
      "        [ 2.8432e-04,  1.5697e-04,  4.7613e-04,  1.1738e-05, -4.1093e-06,\n",
      "          1.2067e-03,  7.0026e-05,  2.9490e-05,  5.0162e-04,  7.4829e-06,\n",
      "          1.3038e-04,  3.1261e-03,  1.2012e-03,  4.1453e-04,  2.1337e-03,\n",
      "          3.5389e-04,  1.0749e-03,  1.6968e-03,  4.4689e-05,  4.4375e-03,\n",
      "          3.6202e-03,  5.1315e-05,  1.3845e-04,  4.7504e-05, -1.7013e-06,\n",
      "          2.7689e-05,  7.2561e-04,  2.2478e-03,  3.0767e-04,  7.1503e-05,\n",
      "         -4.4684e-06,  8.0120e-05],\n",
      "        [ 3.2388e-04,  1.7012e-03, -3.1046e-03,  5.6190e-06,  3.6680e-03,\n",
      "         -3.1365e-03, -6.1616e-04,  3.1457e-03, -3.4964e-03,  8.5709e-05,\n",
      "         -6.3534e-04, -4.6890e-03, -3.3929e-03, -2.1133e-03, -1.5598e-03,\n",
      "         -1.4670e-03,  3.1353e-03, -4.2100e-03,  3.5601e-04, -8.0221e-03,\n",
      "         -8.1567e-03, -2.2996e-03, -1.9565e-03,  9.0178e-05,  1.9958e-04,\n",
      "          3.8541e-04, -1.4271e-03,  6.8024e-04, -9.0556e-05, -7.9125e-04,\n",
      "         -1.5457e-04,  5.6350e-05],\n",
      "        [ 3.8263e-03,  2.0733e-02, -9.1062e-03,  1.4987e-05,  2.2481e-02,\n",
      "          1.3012e-02, -3.3957e-03,  3.6308e-02,  1.7341e-02,  5.3660e-04,\n",
      "         -7.0952e-03,  2.6446e-02,  2.9512e-04, -5.6230e-03,  1.1024e-03,\n",
      "          7.8224e-03,  4.3026e-02, -7.9867e-03,  3.3255e-02,  9.0179e-03,\n",
      "         -1.3563e-02, -1.5253e-02, -2.2395e-03,  8.8931e-03,  7.5328e-04,\n",
      "          1.2854e-02,  1.6510e-02,  2.2494e-02,  5.2746e-05, -2.3106e-04,\n",
      "         -6.1188e-04, -1.6764e-03],\n",
      "        [-7.2736e-04, -1.0836e-02,  2.0177e-02, -3.9526e-06, -2.5524e-02,\n",
      "          1.5063e-02,  4.0980e-03, -2.5238e-02,  7.3061e-03, -4.8215e-04,\n",
      "          9.4402e-04,  5.4578e-03,  9.0711e-03,  1.0429e-02,  3.8586e-03,\n",
      "          6.1422e-03, -3.1512e-02,  1.4353e-02, -4.1526e-03,  3.0471e-02,\n",
      "          3.9673e-02,  1.3889e-02,  4.6869e-03, -7.6846e-03, -1.0010e-03,\n",
      "         -7.4595e-03,  3.1241e-03, -8.7026e-03, -7.2522e-04,  6.3964e-03,\n",
      "          9.1525e-04, -7.0516e-04],\n",
      "        [-9.9148e-05, -9.4734e-04, -1.4095e-03,  8.3146e-06, -8.1929e-05,\n",
      "         -1.8461e-03, -2.2078e-06, -3.5645e-04, -5.7862e-04, -1.6396e-05,\n",
      "          2.7723e-09, -1.3976e-03, -1.1136e-03,  5.3189e-05, -1.0539e-03,\n",
      "         -4.6312e-04, -3.7240e-04, -8.9386e-06, -1.4356e-03, -1.8270e-03,\n",
      "         -2.1981e-03, -4.7569e-06, -6.9808e-04,  9.4420e-06,  8.5278e-05,\n",
      "         -5.8721e-05, -1.5614e-03, -1.3464e-03, -4.1975e-04, -1.2094e-04,\n",
      "          5.3934e-06, -1.1267e-05],\n",
      "        [-1.6113e-03,  2.5957e-03, -4.8099e-03, -6.8056e-06,  7.1767e-03,\n",
      "         -4.8819e-03, -9.2392e-05,  6.7706e-03,  5.2296e-05,  3.0372e-04,\n",
      "          6.3604e-04, -2.8175e-03,  4.2151e-05, -3.0469e-03,  3.1484e-03,\n",
      "         -1.9233e-03,  7.8706e-03, -7.0913e-03,  3.9976e-03, -5.2154e-03,\n",
      "         -4.0425e-03, -3.1710e-03, -1.6918e-03,  2.5404e-03, -6.9205e-05,\n",
      "          6.6639e-04,  2.3251e-03,  4.5119e-03,  1.9541e-03, -4.3276e-04,\n",
      "         -2.3543e-04, -3.5811e-05],\n",
      "        [-1.2616e-05,  4.8414e-06,  6.6718e-06, -3.2924e-06,  1.5374e-05,\n",
      "         -1.4729e-05,  1.6018e-05,  1.3924e-05, -1.4234e-05,  1.2219e-05,\n",
      "         -6.3932e-06, -4.4335e-06, -2.6790e-06,  5.1587e-07, -1.1850e-05,\n",
      "         -9.7774e-06, -1.5898e-05, -9.7762e-06,  8.4290e-06, -1.9328e-07,\n",
      "          5.2413e-06, -2.5499e-07,  9.9445e-06,  4.5216e-06, -1.0780e-05,\n",
      "         -8.1937e-06,  1.0320e-05, -9.6790e-06, -1.6355e-05, -1.0285e-05,\n",
      "         -1.0289e-05,  5.5184e-06],\n",
      "        [-1.1424e-05,  4.3500e-07, -8.7543e-06,  1.1682e-05, -9.1288e-06,\n",
      "          9.2190e-06,  6.6571e-06, -5.4371e-06,  1.2472e-05, -7.9150e-06,\n",
      "          1.3670e-05, -1.1748e-05, -1.6942e-05, -6.7885e-06,  7.4992e-09,\n",
      "          7.6023e-06,  3.4027e-06, -6.6531e-06, -6.0711e-07, -1.2112e-05,\n",
      "          1.5471e-05,  3.9184e-06, -1.6725e-05, -6.4673e-06, -5.8640e-06,\n",
      "         -1.2173e-06,  8.2369e-06, -1.3400e-05,  1.5784e-05,  1.3442e-06,\n",
      "         -1.1670e-05, -8.9744e-06],\n",
      "        [-2.7007e-04, -1.9506e-03,  5.0527e-03,  1.5014e-05, -4.6543e-03,\n",
      "          2.6493e-03,  7.0062e-04, -5.1895e-03,  3.4260e-04, -1.4617e-04,\n",
      "         -1.0955e-03, -6.2723e-03, -4.9344e-04,  1.9114e-03,  9.5334e-05,\n",
      "          1.5649e-03, -9.8817e-03,  5.0952e-04,  4.3666e-04,  8.6689e-04,\n",
      "          6.0933e-03,  3.1831e-03, -4.3810e-04, -2.1985e-03,  3.8968e-05,\n",
      "         -1.4940e-03, -2.0292e-03, -3.6151e-03,  5.0754e-05,  1.9442e-03,\n",
      "          3.5307e-04, -8.4382e-04],\n",
      "        [-1.5719e-04, -9.7790e-06, -9.9546e-04,  1.5909e-06, -1.0270e-05,\n",
      "         -5.4007e-04, -3.4121e-05, -3.2497e-04, -2.8123e-04, -1.1368e-05,\n",
      "         -1.2450e-03, -1.1085e-03, -3.3772e-04, -1.0346e-03, -1.8435e-03,\n",
      "          1.7766e-06, -6.8717e-04, -1.2674e-03, -1.2860e-03, -2.6537e-03,\n",
      "         -2.2441e-03, -3.5104e-06, -9.9403e-06, -7.2778e-06,  3.6770e-06,\n",
      "         -7.9508e-05, -5.5799e-04, -2.0964e-04, -3.0644e-04, -8.3096e-04,\n",
      "          4.8763e-07, -9.7919e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[3.6453e-08, 2.8779e-08, 2.1961e-08, 7.6258e-12, 1.3174e-07, 4.0728e-07,\n",
      "         8.6370e-09, 4.4713e-07, 1.5203e-06, 2.8129e-11, 1.0007e-06, 8.7703e-06,\n",
      "         2.0978e-06, 1.2692e-09, 2.0030e-06, 9.4998e-08, 2.3621e-06, 1.2242e-06,\n",
      "         9.3768e-07, 8.9931e-06, 1.6390e-06, 1.2685e-07, 1.3194e-06, 4.9092e-07,\n",
      "         2.6684e-09, 6.1500e-09, 3.2891e-06, 1.0463e-06, 1.9805e-08, 1.0456e-08,\n",
      "         8.0807e-10, 1.4021e-07],\n",
      "        [7.5396e-07, 1.7364e-05, 9.0043e-06, 7.2449e-12, 3.6215e-05, 1.0215e-04,\n",
      "         9.9903e-06, 8.2608e-05, 8.9547e-05, 1.2930e-07, 1.4679e-05, 9.0342e-05,\n",
      "         5.4145e-05, 1.2857e-05, 8.6861e-05, 4.3321e-05, 5.1927e-05, 1.9319e-05,\n",
      "         1.2870e-04, 2.2526e-04, 1.1891e-04, 3.9111e-05, 5.7899e-05, 1.6300e-05,\n",
      "         4.8996e-07, 1.1307e-05, 1.1266e-04, 6.9352e-05, 1.8484e-05, 9.9382e-06,\n",
      "         3.2691e-07, 3.7595e-06],\n",
      "        [1.4179e-12, 5.8401e-12, 1.2641e-11, 5.1215e-12, 5.4799e-13, 7.3212e-13,\n",
      "         1.1078e-11, 5.5299e-12, 1.5754e-11, 4.1186e-12, 1.9519e-12, 4.9073e-12,\n",
      "         4.6915e-12, 6.9907e-12, 6.0986e-16, 1.1473e-11, 8.2480e-12, 2.0250e-13,\n",
      "         4.7739e-12, 1.2041e-12, 1.1706e-11, 6.5562e-12, 1.6261e-11, 1.3714e-11,\n",
      "         3.7688e-12, 1.1839e-11, 1.2988e-11, 1.4339e-13, 1.6153e-11, 4.0520e-12,\n",
      "         6.0484e-13, 8.5758e-12],\n",
      "        [8.2582e-05, 6.0979e-04, 5.5945e-04, 2.1436e-12, 1.0215e-03, 1.9617e-03,\n",
      "         3.6602e-04, 1.9272e-03, 1.6909e-03, 4.2838e-06, 4.0926e-04, 1.3482e-03,\n",
      "         1.1945e-03, 8.8887e-04, 2.0943e-03, 1.1924e-03, 1.2960e-03, 8.3886e-04,\n",
      "         2.3089e-03, 2.6974e-03, 7.3280e-03, 1.0722e-03, 1.5819e-03, 6.1666e-04,\n",
      "         1.7292e-05, 4.2048e-04, 2.0557e-03, 2.4264e-03, 4.7950e-04, 3.4781e-04,\n",
      "         9.5823e-06, 1.1628e-04],\n",
      "        [1.3688e-06, 4.5590e-05, 2.9977e-05, 9.4035e-12, 1.5940e-04, 5.4978e-04,\n",
      "         6.7262e-05, 4.6188e-04, 8.5393e-04, 4.2684e-07, 3.6841e-05, 6.6536e-04,\n",
      "         5.9243e-04, 3.8446e-05, 6.4958e-04, 1.7917e-04, 2.0031e-04, 3.5488e-05,\n",
      "         7.8459e-04, 2.5394e-03, 5.1126e-04, 1.3859e-04, 1.9738e-04, 5.9021e-05,\n",
      "         1.6874e-06, 2.6640e-05, 1.1504e-03, 3.8930e-04, 9.8108e-05, 3.6046e-05,\n",
      "         1.0442e-06, 1.2094e-05],\n",
      "        [5.6619e-07, 7.6241e-07, 4.0577e-05, 5.0090e-12, 1.2776e-11, 1.3345e-05,\n",
      "         5.3336e-06, 1.8450e-05, 1.0837e-04, 9.0356e-12, 2.5007e-05, 2.8975e-05,\n",
      "         7.2949e-05, 1.3408e-04, 1.4267e-04, 5.1401e-06, 4.4668e-06, 4.7163e-05,\n",
      "         8.9106e-05, 6.4706e-04, 6.5212e-04, 1.1776e-05, 1.4081e-08, 4.3635e-07,\n",
      "         1.0113e-07, 2.1936e-05, 4.3215e-05, 8.2960e-06, 3.1663e-06, 2.2843e-05,\n",
      "         2.1987e-07, 8.4921e-06],\n",
      "        [2.1343e-08, 2.0384e-07, 5.0456e-08, 8.3367e-12, 5.2590e-10, 1.1814e-06,\n",
      "         1.7181e-07, 2.3419e-09, 1.3359e-07, 3.4533e-12, 5.2227e-08, 5.6216e-06,\n",
      "         1.0040e-06, 8.3979e-07, 2.1626e-06, 2.3400e-07, 5.9227e-07, 6.4609e-07,\n",
      "         9.0019e-07, 1.0814e-05, 6.8604e-06, 9.6070e-10, 1.0177e-07, 1.2563e-07,\n",
      "         2.1124e-13, 1.8963e-10, 1.3642e-06, 3.6746e-06, 1.9940e-07, 3.5237e-08,\n",
      "         5.1028e-11, 6.4494e-09],\n",
      "        [7.4476e-07, 5.3815e-06, 5.1797e-06, 1.9811e-12, 8.9220e-06, 1.6844e-05,\n",
      "         3.1426e-06, 1.7091e-05, 1.5480e-05, 4.0977e-08, 3.8061e-06, 1.1441e-05,\n",
      "         1.1202e-05, 8.0732e-06, 1.9110e-05, 1.0355e-05, 1.1277e-05, 7.9280e-06,\n",
      "         2.0169e-05, 2.7004e-05, 6.7841e-05, 9.2343e-06, 1.3571e-05, 5.3497e-06,\n",
      "         1.6724e-07, 3.6599e-06, 1.8046e-05, 2.0863e-05, 4.1185e-06, 3.0199e-06,\n",
      "         9.0679e-08, 1.0274e-06],\n",
      "        [2.3565e-05, 2.2363e-04, 1.5617e-04, 1.3490e-11, 5.3762e-04, 1.2214e-03,\n",
      "         2.5032e-04, 8.8894e-04, 2.7114e-03, 1.9876e-06, 1.4467e-04, 2.0334e-03,\n",
      "         1.7058e-03, 1.3068e-04, 1.8272e-03, 6.4312e-04, 7.9691e-04, 2.3940e-04,\n",
      "         1.2531e-03, 6.3452e-03, 2.6259e-03, 4.0675e-04, 6.0265e-04, 4.0140e-04,\n",
      "         8.1025e-06, 1.0371e-04, 2.8617e-03, 1.6205e-03, 2.3166e-04, 1.0174e-04,\n",
      "         3.9572e-06, 1.9337e-05],\n",
      "        [3.5191e-05, 2.6038e-04, 2.3698e-04, 1.0089e-12, 4.3551e-04, 8.4148e-04,\n",
      "         1.5649e-04, 8.2383e-04, 7.1564e-04, 1.8043e-06, 1.7158e-04, 5.8870e-04,\n",
      "         5.0715e-04, 3.7971e-04, 8.8696e-04, 5.1057e-04, 5.5388e-04, 3.5465e-04,\n",
      "         9.8310e-04, 1.1632e-03, 3.1103e-03, 4.5907e-04, 6.7243e-04, 2.6294e-04,\n",
      "         7.2732e-06, 1.7889e-04, 8.7553e-04, 1.0372e-03, 2.0480e-04, 1.4808e-04,\n",
      "         4.0521e-06, 4.9326e-05],\n",
      "        [3.9716e-09, 3.4171e-07, 3.0901e-07, 4.2412e-12, 3.7319e-08, 8.0181e-07,\n",
      "         5.7361e-11, 6.3437e-08, 2.6900e-07, 1.6109e-11, 6.3730e-17, 3.9524e-07,\n",
      "         6.2530e-07, 1.2199e-07, 3.9865e-07, 2.1034e-07, 3.5756e-07, 2.5682e-08,\n",
      "         7.8141e-07, 5.9607e-07, 1.5820e-06, 1.4376e-12, 3.5914e-07, 5.4385e-12,\n",
      "         2.5191e-09, 2.4220e-08, 5.3688e-07, 7.1841e-07, 8.9651e-08, 3.9289e-08,\n",
      "         1.8305e-12, 7.6919e-12],\n",
      "        [3.6848e-06, 2.4999e-05, 7.6206e-06, 2.8713e-12, 4.6520e-05, 2.1860e-04,\n",
      "         1.5204e-05, 1.5680e-04, 3.3831e-04, 2.1605e-07, 8.5428e-06, 3.6952e-04,\n",
      "         1.7293e-04, 1.5734e-05, 1.4552e-04, 1.1659e-04, 1.4986e-04, 2.7959e-05,\n",
      "         2.8131e-04, 8.8789e-04, 1.8244e-04, 3.0556e-05, 9.1095e-05, 3.2199e-05,\n",
      "         5.1982e-08, 1.0663e-05, 3.2267e-04, 1.5815e-04, 2.4842e-05, 9.6542e-06,\n",
      "         4.1326e-07, 3.3081e-06],\n",
      "        [9.6081e-12, 1.4870e-12, 2.7627e-12, 7.1360e-13, 1.4187e-11, 1.3036e-11,\n",
      "         1.5384e-11, 1.1669e-11, 1.2186e-11, 9.0220e-12, 2.5432e-12, 1.2561e-12,\n",
      "         4.8487e-13, 2.9415e-14, 8.4930e-12, 5.8235e-12, 1.5157e-11, 5.8220e-12,\n",
      "         4.3560e-12, 8.3138e-15, 1.7323e-12, 1.1420e-14, 6.0201e-12, 1.3044e-12,\n",
      "         7.0527e-12, 4.1217e-12, 6.4735e-12, 5.7091e-12, 1.6031e-11, 6.4313e-12,\n",
      "         6.4354e-12, 1.9132e-12],\n",
      "        [7.9039e-12, 2.2991e-14, 4.6905e-12, 8.2582e-12, 5.0910e-12, 5.1900e-12,\n",
      "         2.7509e-12, 1.8592e-12, 9.3935e-12, 3.8526e-12, 1.1252e-11, 8.3511e-12,\n",
      "         1.7188e-11, 2.8573e-12, 3.1165e-16, 3.5614e-12, 7.5938e-13, 2.7477e-12,\n",
      "         3.7578e-14, 8.8673e-12, 1.4365e-11, 9.9235e-13, 1.6755e-11, 2.6008e-12,\n",
      "         2.1514e-12, 1.1717e-13, 4.1642e-12, 1.0819e-11, 1.4944e-11, 1.3919e-13,\n",
      "         8.2414e-12, 4.9239e-12],\n",
      "        [1.5555e-06, 1.1381e-05, 1.0716e-05, 1.3539e-11, 1.9514e-05, 4.1353e-05,\n",
      "         7.0664e-06, 3.7626e-05, 3.2959e-05, 7.9371e-08, 7.9190e-06, 4.9460e-05,\n",
      "         2.9048e-05, 1.7594e-05, 4.1033e-05, 2.3399e-05, 2.6147e-05, 1.6441e-05,\n",
      "         4.6597e-05, 7.2361e-05, 1.3859e-04, 2.1383e-05, 3.2012e-05, 1.1844e-05,\n",
      "         2.5467e-07, 7.4935e-06, 4.3902e-05, 4.5273e-05, 8.7994e-06, 6.6233e-06,\n",
      "         1.6297e-07, 2.1130e-06],\n",
      "        [1.0209e-07, 1.2350e-10, 2.4986e-06, 1.8737e-13, 6.4126e-12, 9.8279e-07,\n",
      "         4.1175e-09, 3.7941e-07, 2.4830e-07, 7.8277e-12, 5.1160e-06, 3.7800e-06,\n",
      "         2.6299e-07, 2.2060e-06, 8.6085e-06, 2.2834e-13, 1.6985e-06, 3.8711e-06,\n",
      "         6.0196e-06, 2.3272e-05, 1.6940e-05, 8.0549e-13, 6.0151e-12, 3.2714e-12,\n",
      "         8.7944e-13, 2.1034e-08, 7.0987e-07, 1.9163e-07, 4.3057e-07, 1.3768e-06,\n",
      "         2.7085e-14, 2.6966e-08]], device='cuda:0')}, 25: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-7.9438e-03,  2.8339e-02, -8.5604e-06,  1.1182e-02,  4.9502e-03,\n",
      "         9.1165e-03,  4.9161e-03, -3.6068e-03,  1.5708e-01, -1.2591e-02,\n",
      "        -6.1936e-03,  1.7676e-02, -1.1712e-05, -4.9066e-06, -8.5304e-03,\n",
      "        -3.7456e-03], device='cuda:0'), 'exp_avg_sq': tensor([1.5581e-05, 1.3495e-03, 4.4896e-12, 2.4066e-05, 1.8606e-02, 1.8412e-03,\n",
      "        1.7124e-05, 6.4345e-06, 4.3906e-02, 3.0606e-05, 8.9101e-06, 5.8425e-03,\n",
      "        8.3004e-12, 1.5257e-12, 4.0731e-05, 3.4347e-05], device='cuda:0')}, 26: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-2.2379e-06, -9.2561e-07,  4.1678e-06,  5.1120e-06, -1.8145e-05,\n",
      "         -6.5024e-06,  5.8177e-06,  1.5071e-05, -9.9897e-06, -1.8193e-05,\n",
      "         -2.0630e-05, -1.4822e-06, -8.1664e-06,  1.3567e-05, -2.0148e-05,\n",
      "         -2.4130e-05],\n",
      "        [-1.1422e-02,  5.2696e-04,  9.3968e-06,  5.1072e-03,  4.7007e-03,\n",
      "         -7.7942e-03, -2.9803e-03,  2.7340e-03, -2.0156e-02, -7.9650e-03,\n",
      "         -5.3788e-04,  7.1510e-04, -2.2096e-05,  1.4601e-05,  2.0833e-03,\n",
      "          5.4542e-04],\n",
      "        [-2.1181e-06, -9.0052e-06, -1.2170e-05,  1.9594e-05,  5.1183e-06,\n",
      "         -4.0362e-06,  8.4191e-06,  1.7467e-06, -4.8211e-06,  6.1841e-06,\n",
      "          2.2217e-05,  9.2381e-06, -1.8391e-05, -2.3072e-05, -5.7308e-06,\n",
      "          7.5077e-06],\n",
      "        [ 6.4676e-06,  1.3616e-05, -1.5175e-05, -1.9721e-05,  1.4831e-05,\n",
      "         -8.1838e-06,  2.1874e-05, -1.2118e-05, -2.1720e-05,  2.2180e-07,\n",
      "         -8.5429e-06,  1.2991e-05,  3.2919e-06, -2.6434e-06, -1.3303e-05,\n",
      "          1.5222e-05],\n",
      "        [ 1.6818e-06, -1.9934e-05,  1.0341e-05, -2.2195e-05,  2.1973e-05,\n",
      "         -7.6568e-06,  7.3082e-06, -1.4868e-05, -1.8734e-05,  1.6946e-05,\n",
      "         -1.1962e-05, -6.3243e-06, -1.8565e-05, -2.1474e-05, -2.0766e-05,\n",
      "          6.1842e-06],\n",
      "        [-1.1757e-02,  4.8850e-04,  1.4080e-06,  5.2443e-03,  4.8708e-03,\n",
      "         -8.0262e-03, -3.0419e-03,  2.8491e-03, -2.0690e-02, -8.1683e-03,\n",
      "         -5.4203e-04,  7.2244e-04, -1.9873e-05, -7.9561e-06,  2.1732e-03,\n",
      "          5.8142e-04],\n",
      "        [ 7.2505e-03, -5.1987e-03,  5.5805e-06, -2.6567e-02, -5.1421e-03,\n",
      "          7.1798e-03,  2.7469e-03, -1.9180e-02,  5.1021e-03,  3.1597e-03,\n",
      "          4.9437e-04, -3.3933e-03, -4.6398e-06, -1.4894e-05, -3.1441e-03,\n",
      "         -5.1774e-04],\n",
      "        [-1.0120e-02,  4.5211e-04, -1.6131e-06,  4.5269e-03,  4.2160e-03,\n",
      "         -6.9059e-03, -2.6340e-03,  2.4355e-03, -1.7818e-02, -7.0703e-03,\n",
      "         -4.7824e-04,  6.0953e-04,  3.5479e-06,  1.1056e-05,  1.8546e-03,\n",
      "          4.7624e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[3.4759e-13, 7.3693e-14, 1.1162e-12, 1.6509e-12, 1.9683e-11, 2.6282e-12,\n",
      "         2.1187e-12, 1.3641e-11, 6.0738e-12, 1.9788e-11, 2.5376e-11, 1.6527e-13,\n",
      "         4.0949e-12, 1.1087e-11, 2.4217e-11, 3.4620e-11],\n",
      "        [5.6728e-05, 9.3633e-04, 5.3876e-12, 2.5108e-04, 6.7739e-04, 6.0350e-05,\n",
      "         2.8378e-06, 6.9782e-04, 5.4869e-04, 4.5999e-04, 2.6241e-06, 2.1665e-04,\n",
      "         2.9073e-11, 1.2814e-11, 5.0507e-04, 8.7626e-07],\n",
      "        [3.1422e-13, 4.9570e-12, 8.9501e-12, 2.2915e-11, 1.6549e-12, 1.0500e-12,\n",
      "         4.3460e-12, 2.2147e-13, 1.4751e-12, 2.3845e-12, 2.9392e-11, 5.2110e-12,\n",
      "         2.0216e-11, 3.1675e-11, 2.0580e-12, 3.4756e-12],\n",
      "        [2.6010e-12, 1.1165e-11, 1.3827e-11, 2.3211e-11, 1.3216e-11, 4.1119e-12,\n",
      "         2.8497e-11, 8.8756e-12, 2.8102e-11, 9.6952e-15, 4.4717e-12, 1.0178e-11,\n",
      "         7.1340e-13, 4.7294e-13, 1.0666e-11, 1.3912e-11],\n",
      "        [2.0693e-13, 2.3709e-11, 6.4997e-12, 2.9334e-11, 2.8755e-11, 3.6113e-12,\n",
      "         3.2980e-12, 1.3281e-11, 2.0968e-11, 1.7196e-11, 8.6521e-12, 2.4904e-12,\n",
      "         2.0596e-11, 2.7474e-11, 2.5710e-11, 2.3846e-12],\n",
      "        [5.9363e-05, 9.7304e-04, 1.5097e-13, 2.6176e-04, 7.0743e-04, 6.3489e-05,\n",
      "         2.9941e-06, 7.2609e-04, 5.6990e-04, 4.7943e-04, 2.7664e-06, 2.2534e-04,\n",
      "         2.3567e-11, 3.8917e-12, 5.2332e-04, 9.2706e-07],\n",
      "        [4.6448e-05, 8.0067e-04, 1.9550e-12, 3.0973e-04, 5.7838e-04, 5.1401e-05,\n",
      "         2.4194e-06, 6.2702e-04, 4.9431e-04, 3.9875e-04, 2.2431e-06, 1.8138e-04,\n",
      "         1.3705e-12, 1.3326e-11, 4.2758e-04, 7.4909e-07],\n",
      "        [4.4242e-05, 7.2713e-04, 1.9205e-13, 1.9534e-04, 5.2754e-04, 4.7220e-05,\n",
      "         2.2244e-06, 5.4201e-04, 4.2589e-04, 3.5792e-04, 2.0553e-06, 1.6833e-04,\n",
      "         8.2185e-13, 7.4112e-12, 3.9124e-04, 6.8663e-07]], device='cuda:0')}, 27: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-1.5308e-05,  1.9962e-06, -1.7681e-05, -8.5066e-06, -1.5551e-05,\n",
      "         1.5507e-05, -5.1584e-02,  2.0858e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.4067e-11, 3.5179e-13, 1.8701e-11, 4.4346e-12, 1.4511e-11, 1.4755e-11,\n",
      "        5.1274e-04, 2.6268e-11], device='cuda:0')}, 28: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 8.2560e-06,  2.5795e-03, -7.2830e-06, -2.9981e-05,  3.3703e-05,\n",
      "          5.4095e-03,  1.1957e-02,  3.2437e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[4.1830e-12, 1.4525e-03, 3.2759e-12, 5.3271e-11, 6.7219e-11, 1.7606e-03,\n",
      "         5.6927e-04, 1.1544e-03]], device='cuda:0')}, 29: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([9.2782e-06], device='cuda:0'), 'exp_avg_sq': tensor([5.0308e-12], device='cuda:0')}}, 'param_groups': [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}]}\n",
      "batch_size 32\n",
      "dropout_ratio 0.3\n",
      "learning_rate 0.0001\n",
      "weight_decay 0.0001\n",
      "n_epochs 5\n",
      "random_seed 0\n",
      "val_c_index 0.6721470019342359\n"
     ]
    }
   ],
   "source": [
    "model_chkpt = FusionNetwork()\n",
    "model_chkpt.to(device)\n",
    "optimizer = optim.Adam(model_chkpt.parameters())\n",
    "\n",
    "# load from last check point\n",
    "checkpoint_path = r\"C:\\Users\\ASUS\\pprojects\\project_kirc\\checkpoints\\trained-model_2025-03-01_0.672147.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_chkpt.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "val_c_index = checkpoint['val_c_index']\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical_rna_feedforward.feedforward.0.weight: torch.Size([512, 19975])\n",
      "clinical_rna_feedforward.feedforward.0.bias: torch.Size([512])\n",
      "clinical_rna_feedforward.feedforward.3.weight: torch.Size([256, 512])\n",
      "clinical_rna_feedforward.feedforward.3.bias: torch.Size([256])\n",
      "clinical_rna_feedforward.feedforward.6.weight: torch.Size([256, 256])\n",
      "clinical_rna_feedforward.feedforward.6.bias: torch.Size([256])\n",
      "clinical_rna_feedforward.feedforward.9.weight: torch.Size([64, 256])\n",
      "clinical_rna_feedforward.feedforward.9.bias: torch.Size([64])\n",
      "clinical_rna_feedforward.feedforward.12.weight: torch.Size([64, 64])\n",
      "clinical_rna_feedforward.feedforward.12.bias: torch.Size([64])\n",
      "clinical_rna_feedforward.feedforward.15.weight: torch.Size([32, 64])\n",
      "clinical_rna_feedforward.feedforward.15.bias: torch.Size([32])\n",
      "clinical_rna_feedforward.feedforward.18.weight: torch.Size([32, 32])\n",
      "clinical_rna_feedforward.feedforward.18.bias: torch.Size([32])\n",
      "wsi_fcn.conv.weight: torch.Size([64, 512, 1])\n",
      "wsi_fcn.conv.bias: torch.Size([64])\n",
      "attention.attention.0.weight: torch.Size([64, 64])\n",
      "attention.attention.0.bias: torch.Size([64])\n",
      "attention.attention.2.weight: torch.Size([1, 64])\n",
      "attention.attention.2.bias: torch.Size([1])\n",
      "baby_feed_forward.0.weight: torch.Size([64, 96])\n",
      "baby_feed_forward.0.bias: torch.Size([64])\n",
      "baby_feed_forward.2.weight: torch.Size([32, 64])\n",
      "baby_feed_forward.2.bias: torch.Size([32])\n",
      "baby_feed_forward.4.weight: torch.Size([16, 32])\n",
      "baby_feed_forward.4.bias: torch.Size([16])\n",
      "baby_feed_forward.6.weight: torch.Size([8, 16])\n",
      "baby_feed_forward.6.bias: torch.Size([8])\n",
      "baby_feed_forward.8.weight: torch.Size([1, 8])\n",
      "baby_feed_forward.8.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in checkpoint['model_state_dict'].items():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from utils import display_km_curves_fusion\n",
    "\n",
    "# from models import *\n",
    "# from train import test_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############################# FINAL ###########################################\n",
    "\n",
    "checkpoint_path = _ # TODO: to choose\n",
    "\n",
    "model_chkpt = FusionNetwork()\n",
    "model_chkpt.to(device)\n",
    "optimizer = optim.Adam(model_chkpt.parameters())\n",
    "\n",
    "# load from last check point\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_chkpt.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "val_c_index = checkpoint['val_c_index']\n",
    "\n",
    "model_chkpt.eval()\n",
    "\n",
    "test_risks = []\n",
    "test_times = []\n",
    "test_events = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        # unpack the batch\n",
    "        batch_clinical_rna_features, batch_lists_phenotype_clusters, batch_times, batch_events = batch\n",
    "        \n",
    "        # move times and events to the device\n",
    "        batch_times = batch_times.to(device)\n",
    "        batch_events = batch_events.to(device)\n",
    "        \n",
    "        # tterate over each sample in the batch\n",
    "        for i, (clinical_rna_features, list_of_phenotype_tensors) in enumerate(zip(batch_clinical_rna_features, batch_lists_phenotype_clusters)):\n",
    "            \n",
    "            risk_score = model_chkpt(clinical_rna_features, list_of_phenotype_tensors)\n",
    "            \n",
    "            test_risks.append(risk_score.item())\n",
    "            test_times.append(batch_times[i].item())\n",
    "            test_events.append(batch_events[i].item())\n",
    "\n",
    "test_c_index = concordance_index(test_times, -np.array(test_risks), test_events)\n",
    "print(f\"test c-index: {test_c_index}\")\n",
    "display_km_curves_fusion(test_risks, test_times, test_events, \"test set\", save_figure=True)\n",
    "\n",
    "\n",
    "\n",
    "# append to csv file\n",
    "with open(\"../evaluation-results/c-index-results.csv\", \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"model\", \"c-index\"])\n",
    "    writer.writerow({\"model\": \"fusion_network\", \"c-index\": test_c_index})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_index_custom(hazards, times, events):\n",
    "    \"\"\"\n",
    "    computes the c-index for survival prediction\n",
    "    - hazards: predicted risk scores (higher means higher risk)\n",
    "    - times: observed survival times\n",
    "    - events: event indicators (1 if event occurred, 0 if censored)\n",
    "    \"\"\"\n",
    "    n = len(times)\n",
    "    concordant = 0.0\n",
    "    permissible = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # only compare if i had an event and its time is earlier than j\n",
    "            if times[i] < times[j] and events[i] == 1:\n",
    "                permissible += 1\n",
    "                if hazards[i] > hazards[j]:\n",
    "                    concordant += 1\n",
    "                elif hazards[i] == hazards[j]:\n",
    "                    concordant += 0.5\n",
    "    return concordant / permissible if permissible > 0 else 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
