{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PatientClinicalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    from csv, so getitem would be something like .loc[idx]\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file_path):\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.df = pd.read_csv(self.csv_file_path).drop([\"time\", \"event\"], axis=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_series = self.df.iloc[idx]\n",
    "        return patient_series # includes the submitter_id!\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "class PatientRNASeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    a csv file, 534 rows and ~20000 columns for normalized RNA-seq counts\n",
    "    \"\"\"\n",
    "    def __init__(self, rna_file_path):\n",
    "        self.rna_file_path = rna_file_path\n",
    "        self.df = pd.read_csv(self.rna_file_path)\n",
    "        self.df.set_index(\"submitter_id\", inplace=True)\n",
    "\n",
    "    def __getitem__(self, case_id):\n",
    "        gene_expressions = list(self.df.loc[case_id])\n",
    "        tensor_gene_expressions = torch.tensor(gene_expressions, dtype=torch.float32).unsqueeze(0)\n",
    "        return tensor_gene_expressions # [1, 19962]\n",
    "\n",
    "\n",
    "class PatientWSIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataset for accessing a patient's list of patches features, each is of shape (1, n_patches, n_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, wsi_dir):\n",
    "\n",
    "        self.wsi_dir = wsi_dir\n",
    "        self.case_ids = list(os.listdir(self.wsi_dir))\n",
    "        self.dict_case_id_path = {\n",
    "            c: os.path.join(self.wsi_dir, c) + \"/patches_features.npy\" for c in self.case_ids\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, case_id):\n",
    "        # grab the list of 5 clusters for this case_id\n",
    "\n",
    "        case_npy_file = self.dict_case_id_path[case_id]\n",
    "        patches_features = np.load(case_npy_file, allow_pickle=True).item()\n",
    "        \n",
    "        cluster_ids = self.clustering(patches_features)\n",
    "\n",
    "        features_list = list(patches_features.values())\n",
    "        unique_clusters = np.unique(cluster_ids)\n",
    "        n_clusters = len(unique_clusters)\n",
    "\n",
    "        list_phenotype_tensors = [] # list of tensors, each tensor is a cluster's features of shape i.e. (1, 15 patches in this cluster, 512 as output of resnet18)\n",
    "\n",
    "        for cluster in unique_clusters:\n",
    "            cluster_features = [features for features, c in zip(features_list, cluster_ids) if c == cluster]\n",
    "            tensor_cluster_features = torch.from_numpy(np.array(cluster_features)).float().unsqueeze(0) # (1, n_patches, n_features)\n",
    "\n",
    "            list_phenotype_tensors.append(tensor_cluster_features.to(device))\n",
    "\n",
    "        return list_phenotype_tensors # [t1,t2,t3,t4,t5]\n",
    "\n",
    "    def clustering(self, patches_features, n_clusters=5):\n",
    "        feature_vectors = list(patches_features.values())\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        cluster_ids = kmeans.fit_predict(feature_vectors)\n",
    "        return cluster_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.case_ids)\n",
    "\n",
    "\n",
    "\n",
    "## Fusion multimodal\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    takes three data paths (clinical, rna-seq, histopath images)\n",
    "    build a data out of 'em\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        clinical_data_path, \n",
    "        rna_seq_data_path, \n",
    "        wsi_data_path\n",
    "    ):\n",
    "        # prepare labels from the clinical data path\n",
    "        self.LABELS_DF = pd.read_csv(clinical_data_path)[[\"submitter_id\", \"event\", \"time\"]]\n",
    "        # then by initializing the clinical_dataset, remove the time and event from the clinical features:\n",
    "        self.clinical_dataset = PatientClinicalDataset(clinical_data_path)\n",
    "\n",
    "        # initialize the datasets for each modality\n",
    "        self.wsi_dataset = PatientWSIDataset(wsi_data_path)\n",
    "        self.rna_dataset = PatientRNASeqDataset(rna_seq_data_path)\n",
    "\n",
    "        # label dictionary with key=submitter_id and value=(event,time) for easy lookup\n",
    "        self.labels_dict = {}\n",
    "        for submitter_id, event, time in zip(self.LABELS_DF[\"submitter_id\"], self.LABELS_DF[\"event\"], self.LABELS_DF[\"time\"]):\n",
    "            self.labels_dict[submitter_id] = {\"event\": event, \"time\": time}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clinical_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # (1) start from clinical dataset\n",
    "        patient_series = self.clinical_dataset[idx]\n",
    "        case_id = patient_series[\"submitter_id\"]\n",
    "        clinical_features = list(patient_series.drop([\"submitter_id\"]))\n",
    "        tensor_clinical_features = torch.tensor(clinical_features, dtype=torch.float32).unsqueeze(0) \n",
    "        # above: add batch dim (1, 13) instead of (13)\n",
    "\n",
    "        # (2) grab the tensor for 20000 (processed) gene counts for that case id\n",
    "        tensor_rna_genes = self.rna_dataset[case_id] # (1, 19962)\n",
    "\n",
    "        # (2.5) NOTE: to save time for this moment, I will concat the clinical and rna together \n",
    "        # and build one feed-forward for the combined\n",
    "        tensor_clinical_rna = torch.cat((tensor_clinical_features, tensor_rna_genes), dim=1) # (1, 19975)\n",
    "\n",
    "        # (3) collect the list of phenotype tensor for that case id\n",
    "        list_of_phenotype_tensors = self.wsi_dataset[case_id]\n",
    "\n",
    "        # (4) labels\n",
    "        time = self.labels_dict[case_id][\"time\"]\n",
    "        event = self.labels_dict[case_id][\"event\"]\n",
    "\n",
    "        return (\n",
    "            tensor_clinical_rna,\n",
    "            list_of_phenotype_tensors,\n",
    "            time,\n",
    "            event\n",
    "        )\n",
    "\n",
    "        # return (\n",
    "        #     tensor_clinical_features, \n",
    "        #     tensor_rna_genes, \n",
    "        #     list_of_phenotype_tensors,\n",
    "        #     time,\n",
    "        #     event\n",
    "        # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = MultimodalDataset(\n",
    "    config[\"clinical\"][\"cleaned_clinical_json\"],\n",
    "    config[\"rna\"][\"cleaned_rna\"],\n",
    "    config[\"wsi\"][\"wsi_slides\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 first gene counts:\n",
      "tensor(2.4332)\n",
      "tensor(0.)\n",
      "tensor(3.4071)\n",
      "tensor(2.7216)\n",
      "tensor(1.6839)\n",
      "tensor(1.7558)\n",
      "tensor(3.9939)\n",
      "first 13 in clinical and rna:\n",
      "tensor(0.)\n",
      "tensor(1.1109)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "case0 = check_data[5]\n",
    "clin_rna, list_tensors = case0[0], case0[1]\n",
    "\n",
    "print(\"5 first gene counts:\")\n",
    "for i in clin_rna.flatten()[13:20]:\n",
    "    print(i)\n",
    "\n",
    "print(\"first 13 in clinical and rna:\")\n",
    "for i in clin_rna.flatten()[0:13]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1148, 512])\n",
      "torch.Size([1, 1078, 512])\n",
      "torch.Size([1, 984, 512])\n",
      "torch.Size([1, 966, 512])\n",
      "torch.Size([1, 824, 512])\n",
      "\n",
      "torch.Size([1, 1148, 512])\n",
      "torch.Size([1, 1078, 512])\n",
      "torch.Size([1, 984, 512])\n",
      "torch.Size([1, 966, 512])\n",
      "torch.Size([1, 824, 512])\n"
     ]
    }
   ],
   "source": [
    "for tensor in list_tensors:\n",
    "    print(tensor.shape)\n",
    "\n",
    "print()\n",
    "check_wsi = PatientWSIDataset(config[\"wsi\"][\"wsi_slides\"])[\"TCGA-BP-4352\"]\n",
    "for tensor in check_wsi:\n",
    "    print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class WSI_FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2009.11169\n",
    "    fully convolutional network for WSI\n",
    "    takes 1 phenotype tensor/cluster of shape (1, n_patches, 512)\n",
    "    outputs a local representation of that phenotype tensor of shape (1, 64)\n",
    "    why FCN? on the numerical vectors? \n",
    "        - utilize the kernel, and especially kernel_size=1 because we can't have kernel_size>1 for randomly picked patches from the histopathology slides\n",
    "        - so why not a simple fully connected network (MLP)? it's because it requires inputs with fixed dimension and we have varying number of patches for each cluster\n",
    "    also, note that a patch -> FCN -> (1,64) shape. So if we have 300 patches or (300,64) shape, we would use avgpooling and get (1,64) as the final output for that cluster\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features=64):\n",
    "        super(WSI_FCN, self).__init__()\n",
    "        # conv1d because we only have a tensor of shape (N, C, L) = (1, 512, i.e. 272)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, \n",
    "            kernel_size=1 # kernel size = 1 is extremely important because we only want to the a single patch to be learned, \n",
    "            # doing i.e. 3x3 is no use because the patches are picked randomly, so can't use spatial relationship here\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        # adaptive avg pooling to get a local representation of the phenotype tensor\n",
    "        # NOTE: adapative pooling from (64, 300 patches) to (64,1) as the final output of that cluster\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, n_patches, n_features)\n",
    "        # permute to (1, n_features, n_patches) so that n_features become channels why? because tensor in pytorch reads () https://stackoverflow.com/questions/51541532/which-part-of-pytorch-tensor-represents-channels\n",
    "        # n_patches is the length of the sequence. why?\n",
    "        # FYI: for a conv2D, input should be in (N, C, H, W) format. N is the number of samples/batch_size. C is the channels. H and W are height and width resp: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \n",
    "        # but here we have conv1d: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "\n",
    "        x = x.permute(0, 2, 1) # (1, 512, 300 patches)\n",
    "        x = self.conv(x) # (1, 64, 300 patches)\n",
    "        x = self.relu(x) # (1, 64, 300 patches)\n",
    "        x = self.pool(x) # (1, 64, 1)\n",
    "        x = x.view(x.size()[0], -1) # (1, 64)\n",
    "        return x # (1, 64)\n",
    "\n",
    "\n",
    "class WSI_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2009.11169 \n",
    "    pooling attention mechanism for WSI\n",
    "    takes a local representation of the phenotype tensor of shape (5, 64) in which 5 is the number of clusters\n",
    "    outputs a global representation of the phenotype tensors of shape (64-dim) which is a weighted sum across 5 clusters for 64 features\n",
    "        each case has a global representation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features=64):\n",
    "        super(WSI_Attention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Tanh(),  # tanh because we want to normalize the weights\n",
    "            # why tanh() >> output values in range (-1,1), allowing both neg and pos values, often used in attention scores\n",
    "            nn.Linear(out_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply softmax because we have different number of clusters for each case\n",
    "        # x: (5, 64) stack representation of 5 clusters/phenotypes\n",
    "        scores = self.attention(x) # (5, 1)\n",
    "        att_weights = torch.softmax(scores, dim=0).T # (1,5) which is probabilities\n",
    "        # weighted sum across the 5 clusters:\n",
    "        weights_applied = att_weights @ x  # (5, 64) = (1,5) @ (5,64)\n",
    "        # weighted_sum_vector = torch.sum(weights_applied, dim=0) # (1, 64) or (64)\n",
    "        return weights_applied, att_weights\n",
    "\n",
    "\n",
    "class Clinical_RNA_FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=32, dropout_ratio=dropout_ratio):\n",
    "        # https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "        # https://arxiv.org/pdf/1207.0580\n",
    "        # For fully connected layers, dropout in all hidden layers works\n",
    "        # better than dropout in only one hidden layer and more extreme probabilities tend to be worse,\n",
    "        # which is why we have used 0.5 throughout this paper\n",
    "        \n",
    "        super(Clinical_RNA_FeedForward, self).__init__()\n",
    "\n",
    "        # hidden = [512, 256, 256, 64, 64, 32]\n",
    "\n",
    "        # self.feedforward = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, hidden[0]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),\n",
    "        #     nn.Linear(hidden[0], hidden[1]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),\n",
    "        #     nn.Linear(hidden[1], hidden[2]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),\n",
    "        #     nn.Linear(hidden[2], hidden[3]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),\n",
    "        #     nn.Linear(hidden[3], hidden[4]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),   \n",
    "        #     nn.Linear(hidden[4], hidden[5]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),      \n",
    "        #     nn.Linear(hidden[5], output_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout_ratio),    \n",
    "        # )\n",
    "\n",
    "        hidden = [1024, 512, 512, 256, 256, 128, 128, 64, 64, 32]\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden[0]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[0], hidden[1]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[1], hidden[2]), nn.ReLU(), nn.Dropout(dropout_ratio),  \n",
    "            nn.Linear(hidden[2], hidden[3]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[3], hidden[4]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[4], hidden[5]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[5], hidden[6]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[6], hidden[7]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[7], hidden[8]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[8], hidden[9]), nn.ReLU(), nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(hidden[9], output_dim), nn.ReLU(), nn.Dropout(dropout_ratio),    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feedforward(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# FusionFeedForward\n",
    "class FusionNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "        input_dim_clinical_rna=19975, \n",
    "        input_dim_wsi_fcn=512, \n",
    "        input_dim_wsi_attention=64, \n",
    "        input_dim_final=96  # as from 32+64 = (out_dim of clinical_RNA) + (out_dim of WSI)\n",
    "    ): \n",
    "        # NOTE: no dropout for now\n",
    "        super(FusionNetwork, self).__init__()\n",
    "\n",
    "        # Clinical+RNA\n",
    "        self.clinical_rna_feedforward = Clinical_RNA_FeedForward(input_dim_clinical_rna, output_dim=32, dropout_ratio=dropout_ratio)\n",
    "        # WSI_FCN and WSI_Attention\n",
    "        self.wsi_fcn = WSI_FCN(input_dim_wsi_fcn, out_features=64)\n",
    "        self.attention = WSI_Attention(input_dim_wsi_attention, out_features=64)\n",
    "\n",
    "        # after fusion:\n",
    "        # TODO: rational -> book: many hidden neurons are good -> with regularization like dropout/weight decay\n",
    "        # for no. layers -> background knowledge and experimentation, for now 4 layers\n",
    "        hidden = [64, 32, 16, 8]\n",
    "\n",
    "        self.baby_feed_forward = nn.Sequential(\n",
    "            nn.Linear(input_dim_final, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[1], hidden[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[2], hidden[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[3], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_clinical_rna, list_of_phenotype_tensors):\n",
    "\n",
    "        # Clinical+RNA:\n",
    "        extracted_clinical_rna = self.clinical_rna_feedforward(tensor_clinical_rna.to(device)) # (1, 32) shape\n",
    "        \n",
    "        # WSI_FCN\n",
    "        local_reps = [] # len=5\n",
    "        # here since tensors have different no. images in each of them\n",
    "        # we use the \"flexiblity\" of the FCN to output 1x64 for each cluster\n",
    "        for tensor in list_of_phenotype_tensors:\n",
    "            tensor = tensor.to(device)\n",
    "            cluster_rep = self.wsi_fcn(tensor) # each of shape (1,64) by pooling from tensors with varying dim\n",
    "            local_reps.append(cluster_rep)\n",
    "        # stack 5 local representation of shape (1,64) >> tensor of shape (5,64)\n",
    "        tensor_local_reps = torch.cat(local_reps)\n",
    "\n",
    "        # WSI_Attention:\n",
    "        wsi_aggregated_vector, att_weights = self.attention(tensor_local_reps) # from (5,64) to weighted vector (1,64)\n",
    "\n",
    "        # concantenate:\n",
    "        concatenated_features = torch.cat((extracted_clinical_rna, wsi_aggregated_vector), dim=1) # shape (1, 96)\n",
    "\n",
    "        risk_score = self.baby_feed_forward(concatenated_features)\n",
    "        return risk_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2094]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " torch.Size([1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "x = torch.rand(5, 64)\n",
    "a2 = WSI_Attention(64)(x)\n",
    "print(a2[0].shape)\n",
    "\n",
    "x = torch.rand(1, 300, 512) # (1, n_patches, 512)\n",
    "a1 = WSI_FCN(512)(x)\n",
    "print(a1.shape)\n",
    "\n",
    "x = torch.rand(1, 19975)\n",
    "a3 = Clinical_RNA_FeedForward(19975)(x)\n",
    "print(a3.shape)\n",
    "\n",
    "x1 = torch.rand(1, 19975)\n",
    "x2 = [torch.rand(1, 300, 512), torch.rand(1, 200, 512), torch.rand(1, 50, 512), torch.rand(1, 150, 512), torch.rand(1, 25, 512)]\n",
    "m = FusionNetwork().to(device)\n",
    "a4 = m(x1,x2)\n",
    "a4, a4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "begin to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [16:23<00:00, 81.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 1.1725801428159077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [16:35<00:00, 82.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 1.0484600613514583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [11:48<00:00, 59.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 1.0618597169717152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [12:15<00:00, 61.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 1.0262074023485184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [11:02<00:00, 55.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 1.000241423646609\n",
      "finished training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from lifelines.utils import concordance_index\n",
    "from utils import display_km_curves_fusion\n",
    "\n",
    "# from models import *\n",
    "# from data_utils import *\n",
    "\n",
    "# import configparser\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read(\"config.ini\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "########################## LOSS ###############################################\n",
    "\n",
    "def negative_partial_log_likelihood(hazard_preds, times, events, device, eps=1e-8):\n",
    "\n",
    "    # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet\n",
    "    # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data\n",
    "    # flatten predictions\n",
    "\n",
    "    hazard_preds = hazard_preds.view(-1)\n",
    "    times = times.to(device, dtype=torch.float).view(-1)\n",
    "    events = events.to(device, dtype=torch.float).view(-1)\n",
    "\n",
    "    if events.sum() == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    # compute risk set: R[i, j] = 1 if times[j] >= times[i]\n",
    "    # https://stackoverflow.com/questions/56646261/can-someone-please-explain-np-less-equal-outerrange1-18-range1-13\n",
    "    R_mat = torch.tensor(\n",
    "        np.greater_equal.outer(times.cpu(), times.cpu()).T.astype(np.float32), \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # standardize theta/hazard prediction\n",
    "    theta = (hazard_preds - hazard_preds.mean()) / (hazard_preds.std(unbiased=False) + eps)\n",
    "\n",
    "    # compute the log risk set using the correct formula\n",
    "    # NOTE: use theta directly without an extra exp()\n",
    "    # First, mask the non-risk set entries by multiplying exp(theta) with R_mat,\n",
    "    # then take the log of the sum\n",
    "    log_risk_set = torch.log(torch.sum(torch.exp(theta) * R_mat, dim=1) + eps)\n",
    "\n",
    "    # negative partial likelihood only for events\n",
    "    loss = -torch.mean((theta - log_risk_set) * events)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# one-batch\n",
    "# hazard_pred = torch.tensor([2.1, 1.8, 3.0, 0.5, 2.5], device=device)\n",
    "# time = torch.tensor([5, 3, 6, 2, 4], device=device)\n",
    "# event = torch.tensor([1, 1, 0, 1, 0], device=device)\n",
    "\n",
    "# one_batch_loss = negative_partial_log_likelihood(hazard_pred, time, event, device)\n",
    "# print(one_batch_loss)\n",
    "\n",
    "################## HYPERPARAMS ################################################\n",
    "\n",
    "# https://arxiv.org/pdf/1206.5533 (guide to choose hyperparams)\n",
    "n_epochs = 5\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "# regularizations:\n",
    "dropout_ratio = 0.5\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# since we have relatively small dataset (~300 for training), high weidght decay may lead to udnerfitting\n",
    "# but we might have many interactions between parameters in the final feedforward, so let's try different ones\n",
    "# https://medium.com/towards-data-science/this-thing-called-weight-decay-a7cd4bcfccab\n",
    "# https://stackoverflow.com/questions/44452571/what-is-the-proper-way-to-weight-decay-for-adam-optimizer\n",
    "\n",
    "################### TRAIN #####################################################\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders \n",
    "    i.e. 32 batch_size\n",
    "    batch = [\n",
    "        (list_of_phenotype_tensors, time1, event1),    => case 1\n",
    "        (list_of_phenotype_tensors, time2, event2),    => case 2\n",
    "        ...                                            => case 32\n",
    "    ]\n",
    "\n",
    "    TODO: more explanation to come\n",
    "    \"\"\"\n",
    "    # each element in batch is a tuple: \n",
    "    # (clinical_rna_tensor, list_of_phenotype_tensors, time, event)\n",
    "    list_of_clinical_rna_features, list_of_lists_of_5_tensors, times, events = zip(*batch)\n",
    "    # i.e. [patient_1_clinical_rna_t1, patient_2_clinical_rna_t2,..., patient32_clinical_rna_t32]\n",
    "    # i.e. [[t1,t2,t3,t4,t5],[t1,t2,t3,t4,t5],...,[t1,t2,t3,t4,t5]] = [32 lists]\n",
    "    \n",
    "    return (\n",
    "        list(list_of_clinical_rna_features),\n",
    "        list(list_of_lists_of_5_tensors),\n",
    "        torch.tensor(times),\n",
    "        torch.tensor(events)\n",
    "    )\n",
    "\n",
    "# dataset and splitting\n",
    "dataset = MultimodalDataset(\n",
    "    config[\"clinical\"][\"cleaned_clinical_json\"],\n",
    "    config[\"rna\"][\"cleaned_rna\"],\n",
    "    config[\"wsi\"][\"wsi_slides\"]\n",
    ")\n",
    "\n",
    "train_size, val_size = int(0.7 * len(dataset)), int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train, val, test = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# loading data\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val, batch_size=val_size, shuffle=False, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test, batch_size=test_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# initiate model\n",
    "model = FusionNetwork(\n",
    "    input_dim_clinical_rna=19975,\n",
    "    input_dim_wsi_fcn=512,\n",
    "    input_dim_wsi_attention=64,\n",
    "    input_dim_final=96\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "print(\"begin to train\")\n",
    "# training loops\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0 \n",
    "\n",
    "    # each batch contains 32 cases!\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_clinical_rna_features, batch_lists_phenotype_clusters, batch_times, batch_events = batch\n",
    "\n",
    "        risk_scores = [] # list of  32 risk scores\n",
    "        for (clinical_rna_features, list_of_phenotype_tensors) in zip(batch_clinical_rna_features, batch_lists_phenotype_clusters):\n",
    "            # process each sample in the batch of 32\n",
    "            risk_score = model(clinical_rna_features, list_of_phenotype_tensors)\n",
    "            risk_scores.append(risk_score)\n",
    "\n",
    "        # convert to tensor type\n",
    "        risk_scores = torch.stack(risk_scores) # of shape (batch_size, 1) or (32,1) \n",
    "\n",
    "        # TODO: explain in detail: meaning of loss of 32 cases in the batch\n",
    "        optimizer.zero_grad() # zero the parameter gradients\n",
    "        loss = negative_partial_log_likelihood(risk_scores, batch_times.to(device), batch_events.to(device), device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch}, loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"finished training\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:13<00:00, 133.03s/it]\n"
     ]
    }
   ],
   "source": [
    "####################### VALIDATION ############################################\n",
    "\n",
    "print(\"begin to validate\")\n",
    "model.eval()\n",
    "\n",
    "val_risks = []\n",
    "val_times = []\n",
    "val_events = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        # unpack the batch\n",
    "        batch_clinical_rna_features, batch_lists_phenotype_clusters, batch_times, batch_events = batch\n",
    "        \n",
    "        # move times and events to the device\n",
    "        batch_times = batch_times.to(device)\n",
    "        batch_events = batch_events.to(device)\n",
    "        \n",
    "        # tterate over each sample in the batch\n",
    "        for idx, (clinical_rna_features, list_of_phenotype_tensors) in enumerate(zip(batch_clinical_rna_features, batch_lists_phenotype_clusters)):\n",
    "            \n",
    "            risk_score = model(clinical_rna_features, list_of_phenotype_tensors)\n",
    "            \n",
    "            val_risks.append(risk_score.item())\n",
    "            val_times.append(batch_times[idx].item())\n",
    "            val_events.append(batch_events[idx].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation c-index: 0.688588007736944\n",
      "validation c-index custom: 0.688953488372093\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model: ../checkpoints/trained-model_2025-03-01_0.688588.pth\n"
     ]
    }
   ],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_km_curves_fusion(risks, times, events, title_name, save_figure=False):\n",
    "    risks = np.array(risks)\n",
    "    times = np.array(times)\n",
    "    events = np.array(events)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8)) \n",
    "    high_risk_idx = risks > np.median(risks)\n",
    "    low_risk_idx = risks <= np.median(risks)\n",
    "    kmf_high = KaplanMeierFitter()\n",
    "    kmf_low = KaplanMeierFitter()\n",
    "    # fit low risk\n",
    "    kmf_low.fit(times[low_risk_idx], event_observed=events[low_risk_idx], label='Low risk')\n",
    "    kmf_low.plot_survival_function(ax=ax, ci_show=True, ci_alpha=0.15, show_censors=True, color='orange')\n",
    "    # fit high risk\n",
    "    kmf_high.fit(times[high_risk_idx], event_observed=events[high_risk_idx], label='High risk')\n",
    "    kmf_high.plot_survival_function(ax=ax, ci_alpha=0.15, ci_show=True,show_censors=True, color='blue')\n",
    "    ax.set_title(f\"Kaplan-Meier curve for final model on {title_name}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Survival probability\")\n",
    "    plt.legend()\n",
    "    if save_figure:\n",
    "        current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "        plt.savefig(f\"../evaluation-results/fusion_{title_name}_{current_time}.png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "val_c_index = concordance_index(val_times, -np.array(val_risks), val_events)\n",
    "print(f\"validation c-index: {val_c_index}\")\n",
    "\n",
    "val_c_index_custom = concordance_index_custom(val_risks, val_times, val_events)\n",
    "print(f\"validation c-index custom: {val_c_index_custom}\")\n",
    "\n",
    "display_km_curves_fusion(val_risks, val_times, val_events, \"validation set\", save_figure=False)\n",
    "\n",
    "\n",
    "saved_model = True\n",
    "\n",
    "if saved_model:\n",
    "    current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "    checkpoint_path = f\"../checkpoints/trained-model_{date.today()}_{val_c_index:4f}.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(), # all weights all models\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'batch_size': batch_size,\n",
    "        'dropout_ratio': dropout_ratio,\n",
    "        'learning_rate': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'n_epochs': n_epochs,\n",
    "        'random_seed': 0,\n",
    "        'val_c_index': val_c_index,\n",
    "        'hidden': [1024, 512, 512, 256, 256, 128, 128, 64, 64, 32]\n",
    "    }, checkpoint_path)\n",
    "    print(f\"saved model: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_42568\\1125866967.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_state_dict OrderedDict([('clinical_rna_feedforward.feedforward.0.weight', tensor([[-5.4621e-03, -6.1640e-03, -7.0199e-03,  ..., -1.5176e-03,\n",
      "          3.2561e-03, -7.5520e-03],\n",
      "        [-5.4274e-03, -3.6572e-03, -1.4722e-03,  ..., -1.3165e-05,\n",
      "          3.1794e-03,  4.6950e-03],\n",
      "        [ 1.3362e-05,  1.9953e-05,  8.7845e-07,  ..., -3.0789e-04,\n",
      "         -5.6543e-07, -1.5444e-04],\n",
      "        ...,\n",
      "        [-3.4046e-03,  1.8917e-03, -3.0014e-05,  ..., -5.7996e-05,\n",
      "          1.7551e-04,  1.9835e-03],\n",
      "        [ 2.8261e-03, -1.3597e-03,  2.4243e-03,  ..., -7.8967e-05,\n",
      "          1.2085e-03, -2.9615e-03],\n",
      "        [ 7.9624e-03, -4.7747e-03,  5.8598e-03,  ..., -2.0958e-04,\n",
      "         -5.5119e-03, -3.9736e-03]], device='cuda:0')), ('clinical_rna_feedforward.feedforward.0.bias', tensor([ 6.0108e-04, -6.3396e-03, -1.0113e-04, -3.1948e-03, -5.7784e-03,\n",
      "        -4.1889e-03,  5.4221e-03,  9.2889e-04, -8.7431e-04, -1.4771e-03,\n",
      "        -3.7117e-03, -7.9062e-04, -1.9237e-03,  7.7051e-05, -2.0966e-03,\n",
      "         7.2735e-03,  1.8441e-03, -1.3764e-06, -6.6735e-03,  2.3616e-03,\n",
      "         2.9491e-03,  4.7947e-03,  1.9209e-03, -1.1375e-04, -8.9705e-04,\n",
      "         3.9715e-04, -5.4176e-04,  6.1068e-03, -9.8825e-04, -1.2034e-03,\n",
      "         2.2182e-03, -1.2689e-04,  6.9694e-04,  4.0871e-03,  6.0484e-03,\n",
      "         2.7552e-04,  4.4751e-04,  5.3924e-03,  6.0238e-03, -5.5764e-03,\n",
      "        -2.0614e-03,  2.8101e-04, -1.2992e-03, -1.0609e-04, -6.6313e-05,\n",
      "        -2.3693e-03, -2.2058e-04, -9.9325e-05,  8.8606e-04,  5.2735e-03,\n",
      "        -1.4684e-03, -4.1147e-03, -1.0984e-04, -5.6881e-03, -4.4496e-03,\n",
      "         3.4841e-05,  1.8105e-05,  4.2792e-03,  1.6134e-05, -1.3046e-03,\n",
      "        -2.7897e-05,  5.9685e-03, -1.3551e-04, -1.4473e-03,  4.0617e-03,\n",
      "        -5.3270e-03, -4.2677e-03,  6.2813e-03,  5.1167e-03, -2.3674e-03,\n",
      "        -4.8703e-04, -1.7147e-03,  5.2298e-03, -2.8738e-03, -1.2113e-04,\n",
      "        -1.3291e-04, -1.8143e-03, -6.2790e-03, -2.7891e-03,  4.6185e-03,\n",
      "         9.4162e-04,  3.2265e-03, -1.1251e-05, -7.3151e-03, -2.0180e-03,\n",
      "        -9.9787e-05,  1.9040e-03,  1.1001e-05,  5.4200e-03,  1.3767e-03,\n",
      "        -6.0653e-04, -2.2736e-03,  2.5208e-03, -1.5649e-03, -5.7314e-03,\n",
      "         5.8495e-03, -4.2601e-03,  5.4532e-03,  5.2983e-04, -3.3936e-03,\n",
      "         5.9796e-04, -6.2707e-03, -1.4265e-03,  2.6434e-03,  1.6766e-03,\n",
      "         3.4685e-03, -1.9780e-03,  5.4033e-03, -4.4386e-04, -1.2175e-03,\n",
      "        -1.5817e-03, -2.1942e-03, -6.1480e-03,  5.1374e-03,  2.6031e-03,\n",
      "        -1.3850e-04, -1.6431e-03, -6.3376e-03,  2.6239e-04,  1.2891e-03,\n",
      "         5.2172e-03,  3.6044e-03, -1.2503e-03,  7.5750e-04,  6.7110e-03,\n",
      "         1.3609e-03, -1.7700e-04,  4.0900e-05,  1.4554e-03, -1.1585e-03,\n",
      "        -1.8499e-04, -2.9572e-03,  6.5019e-04,  3.6022e-03, -2.9945e-03,\n",
      "        -2.4333e-03,  4.7490e-03,  8.9177e-05, -1.3045e-03,  6.5290e-04,\n",
      "         3.0768e-03,  6.2436e-03, -3.0906e-03,  1.1833e-03, -2.4953e-04,\n",
      "        -5.2753e-03,  1.3219e-04,  5.3114e-04, -1.2312e-03,  1.2656e-03,\n",
      "        -4.7708e-03,  4.8748e-03, -4.1749e-03,  6.5624e-04,  6.6780e-04,\n",
      "         2.8766e-03, -2.9461e-03,  1.3933e-04,  1.5230e-03, -6.1672e-04,\n",
      "         1.2730e-04, -2.1393e-03, -1.3114e-03, -1.6394e-03,  4.1350e-05,\n",
      "        -2.6041e-03,  5.9851e-03, -8.1567e-04, -6.4837e-04, -2.7334e-03,\n",
      "        -9.7495e-04, -1.2492e-03,  3.9280e-03, -1.2239e-03,  7.1292e-03,\n",
      "        -5.2147e-04, -1.0060e-03, -8.1188e-06, -1.2855e-03, -3.0385e-03,\n",
      "        -2.8527e-03,  2.7620e-03,  2.2465e-03,  9.7763e-04,  9.3504e-04,\n",
      "         5.6934e-03,  4.0864e-04, -4.6313e-04, -1.6703e-03, -5.9577e-06,\n",
      "        -9.8054e-04,  8.3626e-04, -1.3770e-03,  5.8663e-03, -2.0230e-03,\n",
      "        -7.8204e-04, -1.3998e-03, -5.5738e-04,  1.7134e-03, -7.4115e-04,\n",
      "         2.2257e-05,  2.0885e-03,  3.8572e-03, -5.2031e-03, -1.0522e-05,\n",
      "        -5.0538e-05,  4.9752e-03,  1.6516e-05, -2.6540e-04, -7.0847e-03,\n",
      "        -2.4535e-03,  3.7474e-03,  2.4711e-03,  7.8393e-03, -3.1015e-03,\n",
      "        -4.7808e-03, -5.6938e-04,  1.0907e-03,  1.7008e-03, -5.9678e-03,\n",
      "        -1.2187e-04, -1.0858e-04,  4.6548e-03, -2.5736e-03, -9.6703e-04,\n",
      "         4.8707e-03, -4.0891e-03, -1.3541e-04,  2.3172e-03, -1.3415e-03,\n",
      "         1.2096e-04,  8.2663e-04,  2.5792e-03,  5.4492e-03, -1.2408e-04,\n",
      "        -1.3061e-03,  5.1100e-03,  3.3999e-04,  6.2111e-03, -5.7549e-03,\n",
      "         3.4213e-03,  2.6200e-03,  2.4043e-04,  5.5053e-03, -2.8105e-03,\n",
      "        -6.1342e-03,  3.6245e-04,  6.0339e-03,  4.9541e-03,  3.2037e-04,\n",
      "         4.1347e-05,  5.5361e-03, -3.5462e-03, -6.5189e-03,  3.8559e-03,\n",
      "         1.2621e-03,  5.7929e-03, -3.0167e-04, -4.9004e-04,  3.8006e-03,\n",
      "         8.1380e-04, -7.1575e-05, -1.3435e-04, -1.3762e-03, -2.5799e-04,\n",
      "        -3.1211e-03, -6.4645e-03, -5.3180e-03,  4.4114e-03,  1.1543e-03,\n",
      "        -6.3615e-03, -3.5543e-03,  9.2285e-04,  4.3357e-03,  4.9724e-03,\n",
      "         5.5191e-03, -1.4257e-04, -7.7728e-04, -1.5026e-03, -2.3181e-05,\n",
      "        -2.0769e-05,  1.8133e-04,  3.8803e-03, -1.0198e-03, -4.1573e-03,\n",
      "        -4.5999e-03, -2.5969e-05,  3.2206e-03,  3.4478e-03,  2.8376e-03,\n",
      "        -5.1127e-04,  5.8708e-05,  1.2438e-03, -1.5353e-03, -1.2000e-04,\n",
      "         1.2276e-03,  6.8540e-03, -1.7621e-03, -5.3607e-04,  4.3026e-04,\n",
      "         7.7216e-03,  4.8710e-04, -1.4670e-03,  1.0908e-03, -8.1293e-04,\n",
      "         5.6171e-03, -6.5327e-05, -1.8050e-03, -5.6114e-03,  4.3285e-04,\n",
      "         7.2704e-03, -5.5591e-03,  1.5413e-03, -1.5637e-03, -1.4580e-04,\n",
      "        -2.1650e-03,  3.1817e-05, -5.7495e-04, -1.9602e-03,  2.5703e-03,\n",
      "         6.2782e-03,  2.3839e-03,  1.1680e-03, -4.4983e-03, -1.0654e-06,\n",
      "         1.4865e-05, -4.4381e-04, -9.8401e-04,  2.1828e-03, -5.5826e-03,\n",
      "        -7.7023e-04,  5.2280e-03, -5.6071e-03,  6.2867e-03,  3.1690e-04,\n",
      "         6.8914e-04,  4.6263e-05, -3.6223e-03, -7.3568e-03, -5.3307e-04,\n",
      "        -1.4940e-03,  1.8786e-03,  6.6718e-03,  5.8581e-04,  3.9870e-03,\n",
      "         4.7532e-04,  1.9513e-03,  3.9323e-03, -9.1840e-05,  1.7754e-03,\n",
      "         5.6923e-03, -4.6527e-03, -1.2165e-04,  4.2611e-03,  3.8650e-03,\n",
      "         3.6334e-04, -4.6930e-03, -5.5449e-05, -1.9540e-03, -2.9536e-04,\n",
      "        -1.1002e-04, -4.1252e-04,  9.6526e-04, -1.7652e-03, -1.4288e-04,\n",
      "        -9.8161e-04, -7.0629e-03,  2.3556e-03,  1.3915e-03,  7.2769e-03,\n",
      "         1.5059e-06,  1.7972e-03, -7.3169e-03, -1.4757e-03, -1.5340e-04,\n",
      "         6.3189e-06, -3.7969e-03,  4.8158e-04,  3.6518e-04, -3.3193e-04,\n",
      "        -4.8919e-03,  5.1755e-03,  1.0995e-05, -1.1915e-03, -5.8674e-03,\n",
      "         3.3648e-04, -3.0038e-03,  6.2126e-03,  4.8569e-03,  1.1234e-03,\n",
      "        -2.7505e-04,  3.3553e-06, -9.7986e-04,  2.2738e-04, -5.6992e-03,\n",
      "         1.3090e-04, -5.3205e-04,  1.7823e-03, -3.4676e-03, -3.7529e-03,\n",
      "        -3.9524e-03,  7.2199e-07, -2.7810e-03,  4.8021e-03,  2.4981e-05,\n",
      "        -4.8898e-03,  3.7816e-03, -5.2787e-03,  6.2450e-03, -4.9677e-04,\n",
      "        -6.5855e-03, -3.3994e-03, -1.3821e-04, -6.7184e-04,  3.3874e-04,\n",
      "        -5.9883e-03,  3.3610e-03, -6.4646e-03,  3.8980e-03,  2.2441e-03,\n",
      "         1.2237e-03,  1.2016e-05,  2.1719e-03, -8.7040e-04,  4.3637e-03,\n",
      "         8.0485e-05, -4.4226e-03,  1.8775e-03, -4.8098e-03, -2.2891e-03,\n",
      "        -6.6371e-05, -1.9011e-03, -1.1880e-04,  1.3475e-03,  4.7853e-03,\n",
      "        -4.0571e-04, -1.1520e-04,  5.3308e-03,  2.8327e-03, -7.0019e-03,\n",
      "        -3.2035e-03, -5.1375e-03,  3.4474e-03, -1.4450e-04, -5.1813e-04,\n",
      "         6.2251e-03, -4.5632e-03, -4.3894e-03,  3.0361e-03, -6.2446e-03,\n",
      "         1.6508e-03, -3.9369e-05, -2.5405e-03, -2.8106e-03, -7.4574e-03,\n",
      "        -4.2717e-03, -1.2495e-04,  1.8816e-03,  4.3818e-04,  4.4695e-03,\n",
      "        -1.3329e-04,  4.8548e-03, -7.3564e-05, -6.0121e-03, -4.4400e-03,\n",
      "         5.5448e-04,  3.9887e-04,  1.4056e-03, -2.2393e-03, -4.6501e-04,\n",
      "         3.3737e-03, -1.8701e-03,  6.2652e-03,  1.4940e-04, -1.0332e-03,\n",
      "        -7.6626e-04, -3.7345e-04, -1.3459e-05, -3.7906e-03,  2.1761e-04,\n",
      "        -2.6256e-03,  2.1368e-03,  2.9636e-03,  5.8417e-03, -1.1307e-03,\n",
      "        -2.7677e-03,  1.1800e-03,  5.3391e-04,  6.4391e-03, -8.4998e-05,\n",
      "        -6.5232e-03, -4.1963e-03, -4.2929e-03, -1.3876e-03,  4.7079e-03,\n",
      "        -6.4031e-03, -2.6400e-03,  1.0780e-04, -9.4046e-04, -4.3278e-03,\n",
      "         3.5114e-03,  1.1566e-03,  6.9704e-04,  4.2982e-03,  1.6789e-04,\n",
      "        -6.2528e-03,  5.3771e-05, -5.0221e-04, -8.3199e-04,  1.0977e-03,\n",
      "         3.3812e-04, -5.3957e-03], device='cuda:0')), ('clinical_rna_feedforward.feedforward.3.weight', tensor([[-1.0912e-02,  2.1468e-02,  2.3076e-02,  ...,  5.6996e-03,\n",
      "         -4.1000e-02,  1.8762e-02],\n",
      "        [ 2.7145e-02,  4.3858e-02, -1.9045e-02,  ...,  1.0877e-02,\n",
      "         -2.3111e-02, -1.2344e-02],\n",
      "        [ 3.3079e-02,  4.5824e-03,  2.4570e-02,  ..., -8.3504e-03,\n",
      "         -1.6026e-02,  3.1652e-02],\n",
      "        ...,\n",
      "        [ 3.4629e-02,  7.2020e-03,  1.7670e-02,  ..., -2.4225e-02,\n",
      "         -1.7394e-02, -2.2528e-02],\n",
      "        [-4.0443e-02,  4.1396e-02,  5.2389e-05,  ...,  2.4602e-02,\n",
      "          9.9485e-03, -9.8377e-03],\n",
      "        [ 1.8584e-02,  9.4520e-03,  2.3789e-02,  ..., -3.4956e-02,\n",
      "         -8.7369e-03,  1.5941e-02]], device='cuda:0')), ('clinical_rna_feedforward.feedforward.3.bias', tensor([ 0.0175,  0.0217, -0.0261,  0.0102,  0.0287,  0.0069, -0.0252,  0.0093,\n",
      "        -0.0221,  0.0265, -0.0020,  0.0399,  0.0302, -0.0392, -0.0205, -0.0120,\n",
      "        -0.0325, -0.0170,  0.0296, -0.0395,  0.0389,  0.0280, -0.0099,  0.0348,\n",
      "         0.0245, -0.0150,  0.0432,  0.0441,  0.0016,  0.0310, -0.0332, -0.0331,\n",
      "         0.0433,  0.0325, -0.0008,  0.0292, -0.0221,  0.0044,  0.0205, -0.0411,\n",
      "        -0.0304, -0.0048, -0.0295,  0.0151,  0.0204,  0.0404, -0.0149, -0.0028,\n",
      "        -0.0253, -0.0217,  0.0083,  0.0277,  0.0134,  0.0090,  0.0032,  0.0098,\n",
      "        -0.0253,  0.0293,  0.0214,  0.0005,  0.0351,  0.0067,  0.0043, -0.0329,\n",
      "         0.0102, -0.0294, -0.0225, -0.0184,  0.0204, -0.0101,  0.0018,  0.0410,\n",
      "        -0.0422, -0.0111, -0.0182,  0.0243, -0.0348,  0.0294,  0.0299,  0.0323,\n",
      "        -0.0336,  0.0352,  0.0159, -0.0062,  0.0343,  0.0045, -0.0103,  0.0018,\n",
      "         0.0211,  0.0022,  0.0160,  0.0309,  0.0133,  0.0382, -0.0090, -0.0317,\n",
      "        -0.0155, -0.0118, -0.0397, -0.0110,  0.0226,  0.0198,  0.0032, -0.0011,\n",
      "        -0.0358, -0.0077, -0.0247, -0.0066,  0.0302,  0.0159, -0.0238, -0.0169,\n",
      "         0.0393, -0.0060,  0.0004, -0.0299, -0.0123, -0.0405, -0.0157,  0.0410,\n",
      "        -0.0315, -0.0048,  0.0441, -0.0383, -0.0028, -0.0068, -0.0041,  0.0197,\n",
      "        -0.0154, -0.0022, -0.0420, -0.0050,  0.0171,  0.0141,  0.0368,  0.0127,\n",
      "         0.0291,  0.0389,  0.0192, -0.0024, -0.0146, -0.0073, -0.0068,  0.0223,\n",
      "         0.0407,  0.0335,  0.0018,  0.0174,  0.0071,  0.0318,  0.0059,  0.0287,\n",
      "         0.0237, -0.0406, -0.0316, -0.0402, -0.0382,  0.0115, -0.0059, -0.0038,\n",
      "         0.0081, -0.0203, -0.0033,  0.0026, -0.0094, -0.0383,  0.0275, -0.0321,\n",
      "        -0.0280,  0.0327,  0.0231,  0.0382,  0.0255,  0.0324,  0.0303,  0.0436,\n",
      "         0.0009, -0.0439,  0.0280, -0.0168,  0.0402, -0.0005,  0.0253, -0.0230,\n",
      "        -0.0336,  0.0109,  0.0236, -0.0143, -0.0029,  0.0336,  0.0140, -0.0116,\n",
      "        -0.0183, -0.0062, -0.0374, -0.0041,  0.0147, -0.0296,  0.0041, -0.0044,\n",
      "         0.0036,  0.0078, -0.0278,  0.0003, -0.0067, -0.0388, -0.0278, -0.0313,\n",
      "         0.0189, -0.0300, -0.0133,  0.0253, -0.0127,  0.0058,  0.0171, -0.0110,\n",
      "         0.0013,  0.0127, -0.0102,  0.0173, -0.0275,  0.0313,  0.0196, -0.0340,\n",
      "        -0.0226,  0.0122, -0.0391,  0.0340, -0.0343,  0.0278,  0.0119, -0.0398,\n",
      "         0.0023, -0.0340,  0.0118, -0.0054,  0.0100, -0.0378,  0.0042,  0.0316,\n",
      "         0.0387,  0.0292,  0.0319,  0.0279,  0.0154,  0.0151,  0.0304,  0.0314,\n",
      "         0.0242,  0.0282, -0.0229,  0.0251,  0.0241, -0.0145, -0.0259, -0.0416],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.6.weight', tensor([[ 0.0077, -0.0612,  0.0231,  ...,  0.0249, -0.0596, -0.0119],\n",
      "        [-0.0046, -0.0135, -0.0039,  ...,  0.0317,  0.0151,  0.0514],\n",
      "        [-0.0517,  0.0214, -0.0561,  ..., -0.0100,  0.0359,  0.0225],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0114,  0.0186,  ..., -0.0452,  0.0461, -0.0053],\n",
      "        [ 0.0002, -0.0458,  0.0168,  ...,  0.0475,  0.0074,  0.0215],\n",
      "        [ 0.0430, -0.0413, -0.0537,  ...,  0.0095, -0.0299,  0.0339]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.6.bias', tensor([-0.0159,  0.0622, -0.0594, -0.0058, -0.0038,  0.0146,  0.0113,  0.0170,\n",
      "        -0.0176, -0.0066,  0.0204,  0.0179, -0.0357, -0.0416, -0.0475,  0.0177,\n",
      "         0.0424, -0.0398,  0.0224,  0.0280,  0.0308,  0.0286, -0.0018,  0.0108,\n",
      "        -0.0310,  0.0060, -0.0235, -0.0225,  0.0126, -0.0016,  0.0304,  0.0236,\n",
      "        -0.0526, -0.0091, -0.0081, -0.0602,  0.0471, -0.0564,  0.0367,  0.0556,\n",
      "         0.0354, -0.0344, -0.0595,  0.0421,  0.0616,  0.0341,  0.0263, -0.0140,\n",
      "        -0.0330,  0.0331, -0.0515,  0.0403, -0.0451, -0.0532, -0.0535,  0.0566,\n",
      "         0.0008,  0.0202, -0.0183,  0.0289, -0.0069, -0.0400,  0.0466,  0.0340,\n",
      "        -0.0139, -0.0294,  0.0106,  0.0487, -0.0359,  0.0026,  0.0327,  0.0068,\n",
      "         0.0467,  0.0246,  0.0538,  0.0212,  0.0099, -0.0256,  0.0390,  0.0585,\n",
      "        -0.0140, -0.0022,  0.0234,  0.0075,  0.0412,  0.0271,  0.0017, -0.0004,\n",
      "         0.0287, -0.0062, -0.0586, -0.0125,  0.0108, -0.0080,  0.0537, -0.0423,\n",
      "         0.0536, -0.0013,  0.0464, -0.0602,  0.0060, -0.0151, -0.0435,  0.0564,\n",
      "        -0.0005,  0.0366,  0.0116,  0.0592, -0.0054,  0.0087, -0.0623, -0.0553,\n",
      "        -0.0530,  0.0518,  0.0463,  0.0437,  0.0593,  0.0515,  0.0005,  0.0142,\n",
      "         0.0275, -0.0357, -0.0402, -0.0247, -0.0214, -0.0134,  0.0252, -0.0563,\n",
      "        -0.0087, -0.0344, -0.0232, -0.0580,  0.0386,  0.0474, -0.0055,  0.0241,\n",
      "         0.0558, -0.0272, -0.0300, -0.0601, -0.0212, -0.0152,  0.0498, -0.0271,\n",
      "        -0.0617, -0.0117,  0.0434, -0.0014,  0.0425, -0.0292,  0.0202, -0.0509,\n",
      "         0.0082,  0.0195,  0.0124,  0.0531, -0.0262, -0.0292, -0.0497,  0.0570,\n",
      "        -0.0411,  0.0581, -0.0055, -0.0387, -0.0160, -0.0369,  0.0208,  0.0225,\n",
      "         0.0308, -0.0007,  0.0355, -0.0183, -0.0289, -0.0417,  0.0350, -0.0169,\n",
      "         0.0295, -0.0548, -0.0116,  0.0171, -0.0118, -0.0574, -0.0315,  0.0256,\n",
      "        -0.0006,  0.0159, -0.0411,  0.0054, -0.0186, -0.0502,  0.0348, -0.0619,\n",
      "        -0.0012,  0.0397,  0.0059, -0.0222, -0.0316, -0.0031, -0.0406,  0.0025,\n",
      "        -0.0166, -0.0217,  0.0576, -0.0019,  0.0453, -0.0229,  0.0563, -0.0549,\n",
      "        -0.0376,  0.0510,  0.0314, -0.0541, -0.0195,  0.0558, -0.0518, -0.0071,\n",
      "         0.0262, -0.0331,  0.0215, -0.0282,  0.0174,  0.0510,  0.0577,  0.0030,\n",
      "         0.0237,  0.0092,  0.0439,  0.0261, -0.0144,  0.0107, -0.0200,  0.0489,\n",
      "        -0.0172, -0.0195,  0.0500, -0.0509,  0.0474, -0.0278,  0.0059,  0.0546,\n",
      "        -0.0246, -0.0406,  0.0209, -0.0533, -0.0545, -0.0447,  0.0582,  0.0437,\n",
      "         0.0593,  0.0439, -0.0288,  0.0123, -0.0024, -0.0306,  0.0574, -0.0276],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.9.weight', tensor([[ 0.0617, -0.0589,  0.0492,  ...,  0.0404,  0.0064,  0.0523],\n",
      "        [ 0.0044,  0.0177,  0.0190,  ...,  0.0647, -0.0473,  0.0605],\n",
      "        [ 0.0187, -0.0475,  0.0189,  ..., -0.0311,  0.0148,  0.0041],\n",
      "        ...,\n",
      "        [-0.0026,  0.0351, -0.0011,  ..., -0.0602,  0.0404,  0.0352],\n",
      "        [ 0.0570,  0.0149,  0.0595,  ..., -0.0601,  0.0592,  0.0614],\n",
      "        [ 0.0159, -0.0174,  0.0437,  ...,  0.0502,  0.0078,  0.0200]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.9.bias', tensor([-0.0125,  0.0591, -0.0546,  0.0501,  0.0150,  0.0461, -0.0124, -0.0346,\n",
      "         0.0416, -0.0279, -0.0160,  0.0281, -0.0517,  0.0106, -0.0005,  0.0348,\n",
      "         0.0159, -0.0424, -0.0394, -0.0325, -0.0086,  0.0253, -0.0580, -0.0092,\n",
      "         0.0543, -0.0108,  0.0496, -0.0260, -0.0031,  0.0320,  0.0037, -0.0094,\n",
      "         0.0570, -0.0536,  0.0437,  0.0079,  0.0082, -0.0542, -0.0333, -0.0023,\n",
      "        -0.0105,  0.0458,  0.0598,  0.0282, -0.0247,  0.0116, -0.0347, -0.0523,\n",
      "         0.0322,  0.0117,  0.0259, -0.0129, -0.0374, -0.0116, -0.0080, -0.0199,\n",
      "        -0.0462,  0.0487,  0.0449, -0.0492, -0.0499, -0.0519, -0.0123, -0.0266],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.12.weight', tensor([[ 0.0710,  0.0705, -0.0186,  ...,  0.0503, -0.1062, -0.0727],\n",
      "        [-0.1046,  0.0653, -0.0205,  ...,  0.0919,  0.0807, -0.0393],\n",
      "        [ 0.0919, -0.0938, -0.0982,  ..., -0.0196, -0.0343, -0.0949],\n",
      "        ...,\n",
      "        [-0.1161, -0.0159,  0.0172,  ...,  0.0548, -0.0739,  0.0389],\n",
      "        [ 0.1138,  0.0238, -0.0672,  ...,  0.0368, -0.0970,  0.0715],\n",
      "        [-0.1173,  0.0355,  0.0004,  ..., -0.1039,  0.1027, -0.0377]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.12.bias', tensor([ 0.1165,  0.0446, -0.0211, -0.1029, -0.0559,  0.1145, -0.0998,  0.0377,\n",
      "        -0.0834, -0.0261, -0.0205, -0.0888,  0.0730, -0.0316, -0.0898,  0.0912,\n",
      "        -0.0024, -0.0507,  0.0871,  0.0138, -0.0072,  0.0114,  0.1090,  0.0765,\n",
      "         0.0047,  0.0678,  0.0880, -0.1019, -0.0868,  0.0788, -0.0105, -0.0058,\n",
      "        -0.0168, -0.0419, -0.0784, -0.1045, -0.0767,  0.0436, -0.1060,  0.1152,\n",
      "        -0.0469, -0.0672, -0.0853, -0.0675,  0.0137, -0.0403, -0.0991, -0.0653,\n",
      "         0.0552, -0.0735, -0.1173, -0.0245, -0.0258, -0.1075, -0.0169,  0.0042,\n",
      "        -0.0681,  0.0681,  0.0937, -0.0158, -0.0584, -0.0156,  0.0065, -0.0806],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.15.weight', tensor([[ 0.0656, -0.0073, -0.1240,  ...,  0.1129, -0.0624, -0.1159],\n",
      "        [-0.1239, -0.0242, -0.0311,  ...,  0.1022,  0.0308,  0.0413],\n",
      "        [ 0.0395, -0.0098, -0.0144,  ..., -0.0831, -0.1071,  0.0836],\n",
      "        ...,\n",
      "        [-0.0576,  0.0815, -0.0950,  ...,  0.0763,  0.0386,  0.0661],\n",
      "        [-0.0142, -0.0108, -0.0050,  ...,  0.0172,  0.0106,  0.0714],\n",
      "        [ 0.0129,  0.0081, -0.0990,  ..., -0.0371,  0.0879,  0.0165]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.15.bias', tensor([ 0.0900,  0.0942,  0.0736,  0.0035,  0.1214,  0.0707,  0.0292, -0.0856,\n",
      "         0.1199, -0.0043, -0.0964, -0.0269,  0.0081,  0.0117,  0.0449, -0.1227,\n",
      "        -0.0449,  0.0451, -0.0585, -0.0545, -0.1128, -0.0064,  0.0278, -0.1147,\n",
      "        -0.0163,  0.1238,  0.0528, -0.1036,  0.0603, -0.0421,  0.1182,  0.0289],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.18.weight', tensor([[ 0.0095, -0.1136, -0.0818,  ...,  0.0884, -0.1389, -0.1433],\n",
      "        [-0.0793, -0.0712,  0.0106,  ...,  0.1089, -0.1486,  0.0600],\n",
      "        [-0.0466,  0.0950,  0.0857,  ..., -0.0173, -0.1564,  0.0665],\n",
      "        ...,\n",
      "        [ 0.1284, -0.0889, -0.1218,  ...,  0.1318,  0.0824,  0.0849],\n",
      "        [-0.0647, -0.1748, -0.0287,  ...,  0.0169,  0.1586, -0.0882],\n",
      "        [ 0.0003,  0.0995,  0.0327,  ...,  0.1458, -0.1549, -0.0213]],\n",
      "       device='cuda:0')), ('clinical_rna_feedforward.feedforward.18.bias', tensor([-0.0142, -0.1429, -0.0669,  0.1627,  0.1141, -0.0686, -0.1454,  0.0474,\n",
      "        -0.1276,  0.1043,  0.0046, -0.1604, -0.0515, -0.1385,  0.0963,  0.0506,\n",
      "         0.0708, -0.0693,  0.1077, -0.1676, -0.0900,  0.1207, -0.1287, -0.0980,\n",
      "        -0.1638,  0.1487, -0.0353, -0.1005, -0.0485, -0.0877, -0.1477,  0.0006],\n",
      "       device='cuda:0')), ('wsi_fcn.conv.weight', tensor([[[-0.0054],\n",
      "         [-0.0240],\n",
      "         [ 0.0155],\n",
      "         ...,\n",
      "         [ 0.0111],\n",
      "         [ 0.0098],\n",
      "         [ 0.0384]],\n",
      "\n",
      "        [[-0.0165],\n",
      "         [-0.0369],\n",
      "         [ 0.0232],\n",
      "         ...,\n",
      "         [ 0.0052],\n",
      "         [ 0.0201],\n",
      "         [-0.0444]],\n",
      "\n",
      "        [[ 0.0242],\n",
      "         [ 0.0066],\n",
      "         [-0.0029],\n",
      "         ...,\n",
      "         [ 0.0075],\n",
      "         [ 0.0342],\n",
      "         [ 0.0016]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0293],\n",
      "         [ 0.0310],\n",
      "         [ 0.0267],\n",
      "         ...,\n",
      "         [ 0.0101],\n",
      "         [-0.0388],\n",
      "         [-0.0343]],\n",
      "\n",
      "        [[ 0.0101],\n",
      "         [-0.0305],\n",
      "         [-0.0168],\n",
      "         ...,\n",
      "         [-0.0018],\n",
      "         [-0.0252],\n",
      "         [ 0.0312]],\n",
      "\n",
      "        [[ 0.0029],\n",
      "         [ 0.0285],\n",
      "         [-0.0133],\n",
      "         ...,\n",
      "         [ 0.0253],\n",
      "         [-0.0010],\n",
      "         [ 0.0244]]], device='cuda:0')), ('wsi_fcn.conv.bias', tensor([ 0.0183,  0.0274, -0.0253,  0.0295,  0.0008,  0.0330,  0.0008,  0.0159,\n",
      "         0.0134,  0.0367,  0.0368, -0.0215,  0.0190,  0.0420, -0.0378, -0.0109,\n",
      "        -0.0318,  0.0178, -0.0254,  0.0224,  0.0171,  0.0305, -0.0246,  0.0263,\n",
      "        -0.0302,  0.0396, -0.0324, -0.0317, -0.0299, -0.0396,  0.0008, -0.0124,\n",
      "         0.0277, -0.0394, -0.0326, -0.0121,  0.0353,  0.0375, -0.0069,  0.0254,\n",
      "         0.0175,  0.0419, -0.0030,  0.0411, -0.0217,  0.0049,  0.0300,  0.0270,\n",
      "        -0.0382,  0.0086,  0.0141, -0.0448,  0.0079, -0.0185,  0.0138,  0.0365,\n",
      "        -0.0016,  0.0190, -0.0011,  0.0265,  0.0252,  0.0290, -0.0146, -0.0104],\n",
      "       device='cuda:0')), ('attention.attention.0.weight', tensor([[-0.0855,  0.1237, -0.0585,  ...,  0.0891,  0.0429,  0.1126],\n",
      "        [ 0.0828, -0.0636,  0.0985,  ...,  0.0314, -0.0128, -0.0431],\n",
      "        [-0.1128,  0.1230,  0.0018,  ..., -0.0641, -0.0452, -0.0883],\n",
      "        ...,\n",
      "        [-0.0687,  0.0310,  0.0345,  ...,  0.0940, -0.1008,  0.1094],\n",
      "        [ 0.0524,  0.0529,  0.1003,  ..., -0.0443, -0.0813,  0.0506],\n",
      "        [-0.0373,  0.1027, -0.0240,  ...,  0.0987,  0.0344,  0.0568]],\n",
      "       device='cuda:0')), ('attention.attention.0.bias', tensor([-0.0573,  0.1089, -0.0950,  0.0464, -0.0514,  0.0877, -0.0769,  0.0445,\n",
      "        -0.0570,  0.1034,  0.0168, -0.0378,  0.0466, -0.0750,  0.0865,  0.0613,\n",
      "        -0.0721, -0.0222, -0.0496,  0.0190, -0.0802, -0.0958,  0.0606, -0.0162,\n",
      "        -0.0058, -0.0253,  0.1170, -0.1189, -0.0683, -0.0909,  0.1198, -0.0744,\n",
      "        -0.1064, -0.0753,  0.0116, -0.0958, -0.0385, -0.0063, -0.0066, -0.1209,\n",
      "        -0.0220, -0.0380,  0.0006, -0.0988, -0.0898, -0.0303,  0.1149, -0.1028,\n",
      "         0.0802,  0.0992, -0.0074,  0.0636, -0.0143,  0.1147,  0.0087, -0.0474,\n",
      "         0.1030,  0.1150, -0.0784, -0.0580,  0.0500, -0.0117, -0.0895, -0.1140],\n",
      "       device='cuda:0')), ('attention.attention.2.weight', tensor([[ 0.0875,  0.1021, -0.0982, -0.0302, -0.0282, -0.0706,  0.0253, -0.0148,\n",
      "          0.0045,  0.0809,  0.0004,  0.0685, -0.0145, -0.1220, -0.0034, -0.0476,\n",
      "         -0.0297,  0.0095,  0.1218,  0.0446, -0.0705,  0.0155, -0.0415, -0.0552,\n",
      "         -0.0836, -0.0726,  0.1182,  0.0850, -0.0405,  0.0382, -0.0690,  0.0170,\n",
      "         -0.0034,  0.0622, -0.0633,  0.1026, -0.0227, -0.0349,  0.0531,  0.0281,\n",
      "         -0.1217, -0.0287,  0.0761, -0.0080, -0.0276, -0.1219,  0.0294,  0.0893,\n",
      "          0.0002, -0.1072,  0.0720,  0.0763,  0.0971,  0.0366,  0.0794,  0.1059,\n",
      "          0.0614,  0.0201, -0.0678,  0.0175,  0.0745, -0.1029, -0.0624,  0.0508]],\n",
      "       device='cuda:0')), ('attention.attention.2.bias', tensor([-0.0552], device='cuda:0')), ('baby_feed_forward.0.weight', tensor([[-0.0528, -0.0581,  0.0932,  ..., -0.0108,  0.0603, -0.0985],\n",
      "        [ 0.0523, -0.0661,  0.0291,  ..., -0.0620,  0.0576,  0.0251],\n",
      "        [-0.0569, -0.0615,  0.0534,  ..., -0.0137,  0.0365,  0.1024],\n",
      "        ...,\n",
      "        [-0.0819,  0.0433, -0.0643,  ...,  0.0316, -0.0342, -0.0824],\n",
      "        [-0.0169, -0.0778, -0.0075,  ...,  0.0139,  0.0450,  0.0862],\n",
      "        [-0.0083, -0.0540, -0.0701,  ...,  0.0935,  0.1009,  0.0776]],\n",
      "       device='cuda:0')), ('baby_feed_forward.0.bias', tensor([-0.0895,  0.0457,  0.0080, -0.0382,  0.0431,  0.0843,  0.0334,  0.0213,\n",
      "         0.0700, -0.0485, -0.0140, -0.1028, -0.0940, -0.0851,  0.0063,  0.0473,\n",
      "         0.0055,  0.0359, -0.0459, -0.0076, -0.0361, -0.0257, -0.0468, -0.0422,\n",
      "        -0.0486, -0.0074, -0.0461, -0.0969, -0.0772,  0.0569,  0.0046,  0.0857,\n",
      "         0.0679, -0.0287,  0.0415, -0.0898, -0.0446,  0.0335,  0.0633,  0.0218,\n",
      "        -0.0120, -0.0642, -0.0871, -0.0124, -0.0094, -0.0896, -0.0573, -0.0088,\n",
      "        -0.0087, -0.0730,  0.0579,  0.0350, -0.0945,  0.0687,  0.0685,  0.0357,\n",
      "        -0.0290,  0.0820,  0.0816,  0.0800,  0.0075,  0.0233, -0.0324,  0.0605],\n",
      "       device='cuda:0')), ('baby_feed_forward.2.weight', tensor([[-0.0121, -0.0708, -0.1021,  ...,  0.0037,  0.0182,  0.1237],\n",
      "        [ 0.0522, -0.0743,  0.0725,  ..., -0.0396, -0.0635, -0.0566],\n",
      "        [-0.1010, -0.0618,  0.1251,  ...,  0.0309, -0.0686,  0.0998],\n",
      "        ...,\n",
      "        [ 0.0385, -0.1060,  0.0193,  ..., -0.0056, -0.0432, -0.0232],\n",
      "        [-0.0420, -0.0254, -0.0270,  ..., -0.0230,  0.0716, -0.0692],\n",
      "        [ 0.0713,  0.0653,  0.0032,  ...,  0.0915,  0.0951, -0.1030]],\n",
      "       device='cuda:0')), ('baby_feed_forward.2.bias', tensor([-0.0019, -0.1193, -0.1087, -0.0823, -0.0181,  0.0671, -0.0442,  0.1056,\n",
      "         0.1123, -0.0682, -0.1010,  0.0129, -0.0408, -0.0628,  0.0825,  0.0754,\n",
      "         0.0370, -0.0949,  0.1053,  0.1247,  0.0259, -0.0853, -0.1152,  0.0573,\n",
      "        -0.1207,  0.0108,  0.0734,  0.0297,  0.0152,  0.0081, -0.0496, -0.0401],\n",
      "       device='cuda:0')), ('baby_feed_forward.4.weight', tensor([[-1.4726e-01,  2.4631e-03, -5.6435e-02,  1.1140e-01,  1.6672e-01,\n",
      "         -1.4602e-01,  1.4268e-02, -6.1462e-02,  7.3468e-02, -4.9076e-02,\n",
      "          9.4807e-03, -3.7458e-02,  5.9095e-02, -1.1341e-01,  9.8433e-02,\n",
      "          9.9985e-02,  8.6931e-02, -4.0701e-02, -1.3057e-01,  1.6696e-01,\n",
      "          2.3232e-02, -1.4521e-01,  7.6511e-02, -1.2076e-01,  1.4376e-01,\n",
      "          9.1447e-02,  1.7309e-01, -3.9360e-02,  1.7669e-01,  6.7435e-02,\n",
      "          7.1931e-02,  1.2879e-01],\n",
      "        [-1.1408e-01,  6.1430e-03, -1.3250e-01, -1.0851e-01, -1.3777e-02,\n",
      "         -3.4715e-02, -6.4154e-02,  1.6018e-01, -1.3978e-01,  8.1990e-02,\n",
      "          1.6886e-01,  2.5112e-02, -3.9208e-02, -2.6525e-02, -5.3967e-02,\n",
      "          6.0326e-02, -9.7748e-02,  7.7467e-02,  1.1030e-01,  9.3941e-02,\n",
      "         -6.6450e-02, -1.5945e-02, -1.2780e-01,  7.1679e-02,  3.2485e-02,\n",
      "          2.9503e-02,  3.6164e-02, -1.4173e-01,  9.0665e-02, -1.2597e-01,\n",
      "         -1.5801e-01,  1.5066e-01],\n",
      "        [ 4.6358e-02,  9.7121e-02, -1.4429e-01,  9.0761e-02,  2.7723e-02,\n",
      "          3.2493e-02,  1.3488e-01, -9.4426e-02, -1.6143e-01, -8.1084e-02,\n",
      "          5.4901e-02, -8.8779e-02, -8.6739e-02,  1.0654e-01,  1.0858e-04,\n",
      "         -1.3732e-01, -1.1598e-01,  1.5745e-02, -8.7524e-02, -4.2491e-02,\n",
      "         -1.3874e-01,  1.0308e-01,  1.6406e-01, -1.5042e-01,  7.7436e-02,\n",
      "         -1.3954e-01, -1.4630e-01,  1.2813e-02, -1.6350e-01,  8.0402e-02,\n",
      "         -2.9271e-02, -1.1832e-01],\n",
      "        [ 1.1962e-01, -1.4305e-02,  4.9445e-02, -5.7673e-02,  2.8304e-02,\n",
      "         -1.8347e-02, -4.0273e-03,  8.1650e-02, -3.2421e-02, -1.6534e-01,\n",
      "         -7.6115e-02,  1.0543e-01, -1.4489e-02,  1.4820e-01, -1.3486e-02,\n",
      "         -6.0866e-02,  8.1966e-02,  9.8169e-02,  1.2285e-01,  6.6539e-02,\n",
      "         -1.0114e-01, -1.4002e-01,  6.1068e-02,  8.7477e-02,  1.3400e-01,\n",
      "          1.3965e-03,  1.5560e-01,  3.1433e-02,  1.5070e-01,  1.7215e-01,\n",
      "         -1.5314e-01, -7.6219e-02],\n",
      "        [-8.7444e-02,  1.1105e-01, -8.6566e-02, -1.2404e-01,  1.6844e-01,\n",
      "          4.9820e-03, -5.1593e-02,  1.4978e-01,  9.2228e-02,  4.9671e-02,\n",
      "         -1.6773e-01, -2.9694e-02, -1.5790e-02,  1.2630e-01,  2.6702e-02,\n",
      "          9.4173e-02, -1.4980e-02, -7.7723e-02,  1.7444e-01,  1.6758e-01,\n",
      "         -4.6357e-02,  1.7067e-01,  2.5737e-02,  6.2494e-02,  7.7217e-02,\n",
      "         -1.1095e-01,  3.2627e-02, -4.8085e-02, -1.3179e-01,  1.0225e-01,\n",
      "         -6.7031e-02, -6.5297e-02],\n",
      "        [ 1.4040e-01,  9.2580e-02,  1.5799e-01,  8.9725e-02, -1.4508e-01,\n",
      "         -1.2235e-01, -6.3061e-02,  3.3343e-03, -4.0623e-02, -1.2153e-01,\n",
      "          2.4127e-02, -1.2955e-01, -2.3647e-02,  8.1786e-02,  3.9430e-02,\n",
      "          1.2739e-01, -1.7262e-01, -7.4705e-02,  1.3784e-01,  8.3340e-02,\n",
      "          1.4844e-01,  3.9631e-02,  2.1334e-02, -5.0152e-02, -2.1583e-04,\n",
      "          5.8351e-02,  2.3577e-02, -9.5631e-02, -8.1182e-02,  1.0068e-01,\n",
      "          1.0043e-01,  1.6233e-01],\n",
      "        [ 7.0122e-02,  1.6259e-01, -2.7766e-03,  1.1662e-01, -1.4958e-01,\n",
      "         -4.7031e-02,  4.8181e-02, -1.2014e-01, -1.3913e-01,  7.3997e-02,\n",
      "         -1.4532e-01, -2.4479e-02, -4.2361e-03,  1.3760e-01, -7.7745e-02,\n",
      "          1.0696e-01, -1.3640e-01,  7.8522e-02, -1.0076e-01,  1.2965e-01,\n",
      "          1.4892e-01, -1.2764e-01,  1.5702e-01, -7.2277e-02, -1.6141e-02,\n",
      "         -1.5983e-01,  1.8711e-02,  1.2216e-01, -1.3914e-01, -1.1181e-01,\n",
      "         -3.5989e-02, -3.9656e-02],\n",
      "        [ 5.5279e-02, -2.3566e-02, -1.2775e-02,  5.5331e-02,  1.1919e-01,\n",
      "         -2.2528e-02,  4.4318e-02, -3.5859e-02, -1.2980e-01,  8.9109e-02,\n",
      "          6.7667e-03, -7.4797e-03,  6.4824e-02,  2.5794e-02,  1.4071e-01,\n",
      "         -5.2404e-02,  1.5282e-01, -1.1504e-01, -3.3954e-02,  1.6028e-01,\n",
      "         -5.3838e-02, -4.8695e-03,  1.4711e-01, -2.3736e-03,  1.4920e-01,\n",
      "         -1.6785e-01,  1.0632e-01,  7.8669e-02,  1.5375e-02, -8.2057e-02,\n",
      "         -1.2943e-01,  1.5582e-01],\n",
      "        [ 1.1683e-01,  3.0049e-02, -4.4926e-02,  1.4916e-01,  1.7905e-02,\n",
      "          1.0383e-01, -4.0286e-03,  2.3246e-02,  6.5012e-02,  1.2808e-01,\n",
      "          7.5060e-02,  1.1902e-01,  1.7630e-01, -2.7735e-02, -7.5357e-03,\n",
      "         -6.4025e-02,  1.2126e-01, -3.5676e-02, -8.5831e-02,  9.5062e-02,\n",
      "         -1.9849e-02,  1.6833e-01, -1.0239e-01,  1.6816e-01,  2.5433e-02,\n",
      "          1.1230e-02,  1.3153e-01,  3.5736e-02, -5.3903e-02,  5.4891e-02,\n",
      "         -9.8879e-02,  1.2163e-01],\n",
      "        [-1.1947e-01,  1.7050e-01,  4.0653e-02, -3.8649e-02, -8.1748e-02,\n",
      "          8.2915e-02,  1.4553e-01,  1.7471e-01,  1.0430e-01,  1.1030e-01,\n",
      "          2.5381e-03, -1.3669e-01, -8.1286e-02,  2.0729e-02, -1.3355e-01,\n",
      "          4.7335e-02, -9.8453e-02,  8.1112e-02, -6.4213e-02, -1.9903e-02,\n",
      "          1.1709e-01, -4.8573e-02, -2.5045e-03,  1.2019e-01,  7.7514e-02,\n",
      "          5.8542e-02,  1.2186e-01,  8.6602e-02,  1.8386e-02, -2.4500e-02,\n",
      "         -1.3617e-01,  6.8511e-02],\n",
      "        [-1.3685e-01,  1.4180e-01,  1.0305e-01,  8.2326e-02, -1.6261e-01,\n",
      "          4.7321e-02,  8.1185e-02, -1.2944e-01, -1.9839e-02, -1.6327e-01,\n",
      "          2.1071e-06, -1.0884e-01, -9.5058e-02,  2.3615e-02, -9.7578e-02,\n",
      "         -4.1634e-03,  8.6237e-02, -1.2229e-01,  4.1858e-02, -1.6052e-01,\n",
      "          1.5562e-01, -4.6699e-02,  5.9593e-02,  9.3618e-02, -1.0973e-02,\n",
      "         -3.0270e-02, -1.2509e-01,  7.4041e-02,  3.6071e-02, -1.0369e-01,\n",
      "          5.3073e-02, -1.1190e-01],\n",
      "        [ 1.5940e-01,  8.0736e-02, -8.1448e-02, -6.7214e-02,  1.8909e-02,\n",
      "          8.1920e-02,  3.8910e-02,  7.6218e-02, -5.7239e-02,  1.0309e-01,\n",
      "         -1.0398e-01,  1.4635e-01, -9.6511e-02, -8.1768e-03, -1.7449e-01,\n",
      "         -1.0851e-01, -8.3186e-02,  7.6147e-02, -1.6858e-01,  2.3963e-02,\n",
      "         -1.2819e-01, -1.8593e-02, -5.3888e-02,  8.9362e-02, -6.8946e-02,\n",
      "         -1.5971e-01, -4.5527e-02,  1.3262e-01, -3.6437e-02,  6.1541e-02,\n",
      "         -1.2217e-02,  2.7456e-02],\n",
      "        [-1.2541e-01,  4.7545e-02,  6.5874e-02, -3.2042e-02,  1.5304e-01,\n",
      "         -1.4657e-01,  1.5949e-01,  1.3852e-01, -1.4162e-01,  1.2144e-01,\n",
      "         -6.3083e-02, -4.3462e-02, -2.5907e-02,  4.4016e-03, -1.1773e-01,\n",
      "         -9.6978e-02, -1.5829e-01, -9.6966e-02,  8.3473e-02, -1.3423e-03,\n",
      "          5.1549e-02, -1.9087e-03,  9.8652e-02,  4.4344e-02, -1.0702e-01,\n",
      "         -8.1115e-02,  1.0241e-01, -9.5992e-02, -1.6287e-01, -1.0206e-01,\n",
      "         -1.0210e-01,  5.4324e-02],\n",
      "        [-1.1347e-01,  3.6185e-03, -8.6730e-02,  1.1605e-01, -9.0481e-02,\n",
      "          9.1385e-02,  6.5726e-02, -5.3509e-02,  1.2397e-01, -7.8324e-02,\n",
      "          1.3597e-01, -1.1672e-01, -1.6875e-01, -6.7043e-02, -1.9277e-06,\n",
      "          7.5192e-02,  3.3146e-02, -6.5687e-02, -5.2916e-03, -1.2037e-01,\n",
      "          1.5401e-01,  3.8306e-02, -1.6657e-01, -6.3826e-02, -5.7784e-02,\n",
      "         -1.1321e-02,  8.1548e-02, -1.3327e-01,  1.5715e-01,  1.2584e-02,\n",
      "         -1.1593e-01, -8.8935e-02],\n",
      "        [ 1.2104e-01,  3.0594e-02, -4.9297e-02,  1.4943e-01, -1.5159e-01,\n",
      "          4.4454e-02, -1.2909e-01,  4.7386e-02,  1.2367e-01, -1.4753e-01,\n",
      "          1.1865e-01,  1.9453e-02,  1.2475e-02,  1.6150e-01, -3.3531e-02,\n",
      "         -1.5833e-01, -8.2155e-02, -1.6837e-01, -1.1031e-01, -1.3913e-01,\n",
      "          8.8185e-02, -1.6677e-01,  2.9819e-02, -1.9644e-02, -2.8448e-02,\n",
      "         -8.6814e-02, -4.2390e-02, -2.8211e-02,  1.5279e-01,  1.4222e-02,\n",
      "         -1.4040e-01, -1.0513e-01],\n",
      "        [ 8.9219e-02, -3.9237e-02,  1.3179e-01,  1.5040e-02, -1.0191e-01,\n",
      "         -1.5867e-01, -3.4164e-02,  2.7070e-02, -1.6472e-01, -1.1291e-01,\n",
      "          6.5347e-02, -1.5014e-01, -8.2958e-02,  1.2578e-01,  4.6898e-02,\n",
      "          1.6892e-02,  6.1206e-02,  1.0557e-01, -6.9512e-02,  1.1970e-01,\n",
      "         -1.3762e-02, -3.4224e-02, -9.8610e-02, -7.1942e-02,  3.5890e-02,\n",
      "         -2.3796e-02,  5.6119e-02,  3.7835e-04,  7.9707e-02,  6.5602e-02,\n",
      "          4.1274e-03, -1.0750e-01]], device='cuda:0')), ('baby_feed_forward.4.bias', tensor([-0.1553,  0.0469, -0.0848,  0.1184, -0.1283, -0.1425, -0.1730,  0.0051,\n",
      "        -0.1296,  0.1234,  0.0015,  0.0632, -0.1164, -0.0482,  0.1677, -0.1324],\n",
      "       device='cuda:0')), ('baby_feed_forward.6.weight', tensor([[-0.0215, -0.0084,  0.0408,  0.0503, -0.1808, -0.0642,  0.0573,  0.1500,\n",
      "         -0.0991, -0.1813, -0.2057, -0.0140, -0.0808,  0.1349, -0.2009, -0.2408],\n",
      "        [ 0.1383,  0.2057,  0.0932,  0.1747, -0.2331, -0.0336, -0.1024, -0.0805,\n",
      "         -0.0426, -0.0039, -0.0158,  0.1819, -0.2204,  0.1453, -0.1150, -0.1278],\n",
      "        [-0.0203, -0.0892, -0.1209,  0.1953,  0.0503, -0.0395,  0.0834,  0.0166,\n",
      "         -0.0473,  0.0610,  0.2216,  0.0916, -0.1833, -0.2302, -0.0565,  0.0742],\n",
      "        [ 0.0638,  0.1354, -0.1510, -0.1966,  0.1476, -0.0810,  0.2181, -0.1204,\n",
      "         -0.2166,  0.0016, -0.0846,  0.1292,  0.0320, -0.0256, -0.1323,  0.1515],\n",
      "        [ 0.0159, -0.1987,  0.1026, -0.2214,  0.2191, -0.0757,  0.0722, -0.1480,\n",
      "         -0.1867,  0.1688, -0.1189, -0.0624, -0.1850, -0.2141, -0.2071,  0.0610],\n",
      "        [-0.0231, -0.1445,  0.0132,  0.0740,  0.0344, -0.1400,  0.1259,  0.1398,\n",
      "          0.1562,  0.0564,  0.0867,  0.0423, -0.1981, -0.0787,  0.2158,  0.0773],\n",
      "        [ 0.1707,  0.0344,  0.0549, -0.2162,  0.2108,  0.0477,  0.0713, -0.0027,\n",
      "         -0.1049,  0.2442, -0.0185, -0.0069, -0.0455, -0.1482,  0.2245, -0.0191],\n",
      "        [ 0.0652,  0.0942, -0.0153,  0.1612,  0.2232, -0.0640, -0.0326, -0.0076,\n",
      "          0.2159, -0.2392, -0.0362, -0.0832,  0.0346,  0.1098,  0.0221, -0.1802]],\n",
      "       device='cuda:0')), ('baby_feed_forward.6.bias', tensor([-0.1524,  0.0199, -0.1761, -0.0842, -0.1548,  0.1545,  0.0651,  0.2081],\n",
      "       device='cuda:0')), ('baby_feed_forward.8.weight', tensor([[ 0.0817, -0.2300, -0.0720, -0.2994,  0.3366, -0.2366,  0.2120, -0.2038]],\n",
      "       device='cuda:0')), ('baby_feed_forward.8.bias', tensor([0.0921], device='cuda:0'))])\n",
      "optimizer_state_dict {'state': {0: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 1.6196e-05, -2.8164e-05,  1.8168e-05,  ..., -2.1256e-07,\n",
      "          8.6835e-06,  1.4154e-07],\n",
      "        [-2.8931e-07, -4.0866e-07, -2.0759e-07,  ..., -3.0703e-09,\n",
      "          3.4542e-07,  5.4775e-07],\n",
      "        [ 2.3514e-10,  8.9206e-09,  1.8674e-10,  ..., -7.2316e-08,\n",
      "          7.0296e-12, -5.1888e-08],\n",
      "        ...,\n",
      "        [-4.0977e-07, -1.4084e-07, -3.1920e-09,  ...,  6.3094e-10,\n",
      "          7.3070e-08,  6.4562e-07],\n",
      "        [ 2.3177e-07, -1.9769e-07,  2.5844e-07,  ..., -1.1063e-08,\n",
      "          1.3232e-07, -2.2894e-07],\n",
      "        [ 5.7189e-05,  6.8797e-05,  4.8160e-06,  ..., -5.9400e-08,\n",
      "          7.5903e-06, -6.4061e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[1.2361e-08, 1.0251e-08, 2.4690e-10,  ..., 9.2378e-15, 4.1755e-10,\n",
      "         2.2097e-09],\n",
      "        [1.5020e-09, 4.1322e-11, 8.9955e-15,  ..., 7.2126e-17, 1.5645e-11,\n",
      "         1.4279e-10],\n",
      "        [6.8557e-18, 3.5754e-16, 9.0940e-19,  ..., 3.4576e-15, 1.7600e-20,\n",
      "         2.7851e-15],\n",
      "        ...,\n",
      "        [1.1148e-10, 2.5609e-09, 8.6827e-17,  ..., 1.7227e-16, 7.1229e-11,\n",
      "         4.7242e-09],\n",
      "        [3.3807e-10, 1.1360e-09, 4.1225e-12,  ..., 5.0444e-16, 4.1939e-11,\n",
      "         9.2160e-11],\n",
      "        [6.7753e-09, 1.3531e-08, 2.6253e-11,  ..., 3.0283e-15, 5.1480e-10,\n",
      "         3.1010e-09]], device='cuda:0')}, 1: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 3.3321e-05, -3.7686e-07, -4.4373e-08, -3.4664e-06,  4.5441e-05,\n",
      "        -3.8885e-07,  8.0297e-07,  1.2779e-07,  2.8688e-06, -2.0813e-07,\n",
      "         1.7806e-04, -4.3752e-06, -2.5660e-07,  1.2406e-09,  2.3268e-07,\n",
      "         2.6687e-05, -2.6707e-05,  7.5395e-09,  3.3874e-07, -8.2683e-05,\n",
      "         4.7681e-07, -7.1014e-05, -1.8144e-04,  2.8821e-08,  4.4353e-07,\n",
      "         8.3646e-08,  1.8929e-07,  7.3010e-07,  6.5293e-07, -5.8900e-08,\n",
      "         4.3301e-07, -1.1897e-08,  8.3989e-08,  5.4934e-07,  2.4298e-06,\n",
      "         6.8121e-08, -5.5883e-05, -9.6443e-05,  1.3619e-06, -1.2012e-04,\n",
      "        -1.1679e-05,  5.1384e-07, -1.8851e-07, -1.0983e-08,  1.7718e-08,\n",
      "         4.1511e-08,  2.0403e-07, -1.1097e-08,  6.3924e-05,  1.0777e-06,\n",
      "        -2.0717e-07, -4.0201e-07,  8.7930e-09, -3.0834e-07, -3.5149e-07,\n",
      "        -1.1527e-05, -6.1946e-09,  8.7585e-05,  8.6975e-09,  2.1095e-07,\n",
      "        -9.3524e-09,  3.1180e-06,  1.7258e-09, -8.0542e-08,  4.5667e-07,\n",
      "        -7.8069e-05, -5.9693e-08,  3.5692e-05,  9.7116e-07, -2.2476e-07,\n",
      "         4.0919e-07,  1.7541e-05,  3.9148e-06, -8.1276e-05,  6.0135e-09,\n",
      "        -4.8887e-08,  1.5463e-06,  3.9405e-05, -6.8522e-05,  6.0386e-07,\n",
      "         1.4012e-07,  5.6584e-07,  6.7778e-09, -3.2327e-08, -2.6673e-07,\n",
      "         1.1046e-08, -3.2213e-05, -2.7311e-08,  1.6728e-06,  2.2659e-07,\n",
      "         1.0154e-07,  3.0327e-05,  6.8826e-07,  5.6472e-07, -3.2576e-07,\n",
      "        -6.4298e-05,  1.2798e-07,  6.9616e-07,  6.2160e-07,  1.6670e-06,\n",
      "         2.8169e-06, -4.8235e-07,  1.0778e-08,  8.7662e-07,  4.0724e-06,\n",
      "         3.9226e-07,  5.9024e-05,  1.5516e-04, -6.9783e-05,  9.8563e-06,\n",
      "         2.0400e-06,  9.0485e-06, -4.6483e-07, -7.9404e-05,  3.1303e-07,\n",
      "        -8.4563e-09, -2.2626e-07, -6.3239e-07,  6.6404e-08,  5.8701e-07,\n",
      "        -1.5228e-05,  7.0196e-07,  1.4666e-08,  9.5502e-07,  1.6268e-06,\n",
      "        -1.0796e-04,  3.2706e-08, -3.7901e-09,  3.7694e-07, -2.0768e-05,\n",
      "         8.5556e-05,  1.8006e-06,  3.4940e-07,  6.2016e-07, -2.6298e-07,\n",
      "        -2.1055e-07,  7.4304e-07,  4.2648e-08,  4.6212e-05,  1.1474e-07,\n",
      "         2.3133e-05,  1.2360e-04, -6.9018e-05,  1.4144e-07, -4.4749e-06,\n",
      "        -2.6027e-07,  9.4345e-09,  8.1640e-07, -3.8020e-05, -2.5756e-08,\n",
      "         3.5081e-05,  6.9417e-07,  8.4401e-06,  1.1514e-07,  1.1650e-07,\n",
      "         6.9562e-07, -1.5588e-07, -2.6259e-10,  3.6482e-07,  4.6570e-07,\n",
      "        -4.3202e-09, -7.4694e-05,  4.7944e-06, -8.7265e-05,  3.5550e-08,\n",
      "         6.1222e-06,  1.1357e-06,  4.0578e-05, -8.0270e-05,  6.1548e-08,\n",
      "        -1.5210e-07, -1.8295e-07,  6.3481e-07,  3.2487e-07, -3.2087e-05,\n",
      "         1.4670e-06, -1.5563e-07, -3.0391e-08,  6.8116e-07, -1.5967e-07,\n",
      "         6.4751e-07,  4.1017e-07,  5.1073e-07,  1.3945e-06,  2.3146e-07,\n",
      "         6.0673e-07,  8.5083e-08,  1.0495e-07, -2.2922e-07,  2.3666e-10,\n",
      "        -5.2472e-08,  7.5857e-07, -1.9711e-07, -7.8975e-07, -2.6727e-07,\n",
      "        -8.1676e-06, -1.1317e-07,  9.5889e-06,  6.1577e-07, -1.2513e-07,\n",
      "        -2.5452e-08, -1.2156e-05,  7.0962e-07, -2.4908e-07,  6.8371e-09,\n",
      "         5.8139e-07,  1.0332e-06,  1.1435e-09,  3.1261e-08, -5.0755e-07,\n",
      "         4.9418e-05,  8.4149e-07,  4.8352e-07,  8.8409e-05, -3.0400e-07,\n",
      "         2.3073e-07,  3.7649e-06,  5.4732e-07,  3.9784e-07,  2.2763e-07,\n",
      "        -1.8521e-08, -4.5442e-08,  1.3343e-06, -1.3642e-04,  1.0305e-07,\n",
      "         1.7524e-05,  8.5770e-07, -8.9978e-09,  7.0875e-07,  3.3781e-07,\n",
      "         1.0399e-08,  4.9155e-07,  2.8105e-07,  1.0120e-06, -8.3876e-05,\n",
      "         2.6826e-07,  6.2512e-07,  7.6428e-08,  8.5165e-07, -5.2868e-07,\n",
      "         5.4108e-07, -2.6036e-05,  2.6911e-07,  1.3358e-06, -9.1143e-06,\n",
      "        -6.2771e-07, -8.3469e-05,  8.3282e-07,  9.2928e-07,  8.8260e-08,\n",
      "        -2.2202e-08,  1.7065e-06,  1.0580e-07, -1.4661e-07,  1.0112e-06,\n",
      "         4.9893e-06,  9.9886e-07,  5.7381e-07, -9.5139e-08,  1.8917e-06,\n",
      "         9.9094e-06,  1.6730e-08, -9.1534e-09,  3.3471e-05, -6.5826e-08,\n",
      "         1.2091e-05, -4.4270e-07,  4.5963e-05,  1.4720e-04,  1.7346e-07,\n",
      "         9.8313e-05, -3.0037e-07,  1.0430e-06,  9.7616e-05,  5.2454e-07,\n",
      "        -1.2996e-05,  2.7623e-07, -1.2935e-07, -2.1155e-07, -9.1775e-10,\n",
      "        -1.0268e-09,  1.8969e-05,  1.2563e-06, -1.8788e-08, -1.4114e-07,\n",
      "        -2.6494e-07, -6.2267e-05,  5.5972e-07,  3.8533e-07,  2.9484e-05,\n",
      "        -9.7730e-08,  1.2517e-09, -7.8305e-06,  3.1072e-07,  6.3082e-09,\n",
      "         5.9823e-07, -1.2941e-04, -7.4375e-05, -1.0074e-07,  6.8803e-08,\n",
      "        -4.3191e-05,  3.1814e-07, -6.3167e-08,  6.6892e-06,  1.5282e-06,\n",
      "         1.0560e-04, -1.8959e-05, -4.8984e-05, -1.6568e-05, -3.1060e-05,\n",
      "        -3.6862e-05,  4.9166e-08, -1.0674e-04, -2.1761e-07, -5.3233e-09,\n",
      "         7.5153e-07,  1.8026e-07, -7.1572e-05, -2.6053e-07,  8.8231e-07,\n",
      "        -7.7998e-05,  4.0345e-07,  7.2980e-07, -3.8051e-08, -2.7355e-09,\n",
      "        -3.1093e-11,  7.8147e-07, -1.5313e-07,  2.1911e-05,  1.5049e-05,\n",
      "         1.4786e-06,  7.7872e-05, -3.6556e-07,  1.0598e-06,  5.0016e-08,\n",
      "        -3.6650e-05,  2.7756e-09,  1.0502e-06,  1.4082e-07, -1.0038e-07,\n",
      "         1.7739e-06,  2.5175e-07,  2.8988e-06, -5.8288e-05,  5.2044e-06,\n",
      "         1.4794e-07,  2.5957e-07,  6.2054e-07,  1.2726e-08,  5.0601e-05,\n",
      "         1.4328e-06, -4.2489e-07, -5.1986e-06,  5.8534e-07,  1.2494e-06,\n",
      "         7.9391e-08,  4.6694e-07, -1.9747e-09,  4.4631e-07,  9.8491e-07,\n",
      "        -1.6295e-05,  1.3421e-06,  1.5100e-07, -2.3951e-07, -1.4437e-09,\n",
      "        -1.5286e-07, -1.8684e-07,  3.7820e-07,  4.1274e-07, -4.2078e-05,\n",
      "         7.7462e-09,  6.7919e-07, -4.7490e-07, -1.7388e-07, -1.3046e-06,\n",
      "         3.0105e-08,  6.9255e-08,  9.4103e-08,  9.9716e-07, -7.5400e-08,\n",
      "        -2.6459e-05,  9.9563e-07, -2.1815e-09,  7.5968e-05,  2.1237e-05,\n",
      "         8.8452e-05, -1.9522e-07,  6.4485e-07,  2.0474e-05,  1.7416e-06,\n",
      "         8.4945e-05,  2.8144e-09, -1.5266e-07,  2.6107e-07,  2.2533e-05,\n",
      "         9.5825e-09, -1.0025e-07, -1.2012e-04, -3.2826e-07,  3.6471e-05,\n",
      "         3.2206e-07, -4.4077e-10, -8.8697e-08,  6.3946e-07,  3.2014e-09,\n",
      "         1.0458e-04,  4.7587e-07, -3.3772e-07,  6.5603e-07,  1.7896e-05,\n",
      "        -3.0930e-07,  2.8987e-07,  7.1686e-10, -1.1698e-07,  7.6268e-08,\n",
      "        -4.5254e-07, -1.8834e-06, -1.2876e-07,  7.3174e-07,  3.7283e-05,\n",
      "         3.1402e-07, -6.7151e-09,  4.9395e-07,  3.4434e-05,  6.1228e-07,\n",
      "         1.1081e-08,  1.1975e-06,  2.5164e-07, -4.6909e-07, -1.7111e-07,\n",
      "         1.9158e-05,  1.2452e-05, -1.0521e-08,  1.4079e-05,  3.0669e-05,\n",
      "        -8.4717e-08,  7.5131e-09,  7.5160e-07,  6.5493e-07,  1.7836e-05,\n",
      "        -2.4040e-07,  2.8422e-05,  4.9929e-07, -5.0507e-08, -9.8565e-08,\n",
      "         1.2599e-05, -2.6970e-07, -2.0959e-07, -2.4489e-05, -4.8193e-05,\n",
      "        -1.2029e-07, -3.0422e-09, -1.8885e-07,  5.3579e-07, -4.2100e-07,\n",
      "        -4.2677e-05,  7.0711e-06,  2.5208e-07,  2.2145e-06,  5.7487e-07,\n",
      "        -9.2974e-09,  5.0198e-07,  6.5186e-09, -3.3761e-05,  1.7613e-05,\n",
      "         3.0637e-07,  2.9120e-07,  8.7652e-05, -4.8691e-08, -2.6863e-09,\n",
      "         3.5689e-07,  2.4081e-08,  1.8033e-06,  5.1189e-08,  6.6674e-07,\n",
      "         9.2841e-08, -8.0668e-08,  1.2364e-07,  8.2466e-08,  6.0476e-08,\n",
      "         7.4044e-08,  1.2146e-06,  3.5572e-07,  7.2648e-05,  1.3587e-07,\n",
      "        -1.7397e-07, -2.3846e-05,  1.0048e-07,  2.1079e-06, -1.1123e-08,\n",
      "         3.3518e-07,  1.9407e-07,  1.2022e-07, -1.9829e-07,  9.3196e-07,\n",
      "         9.4446e-06,  5.8640e-07,  1.0942e-08, -1.4817e-07, -4.1776e-07,\n",
      "         5.0245e-05,  4.5253e-07,  3.7948e-07,  9.6245e-07,  9.3392e-06,\n",
      "         4.8068e-06,  2.1927e-09,  3.4752e-05,  8.6463e-05,  5.0337e-07,\n",
      "         2.3343e-07,  1.5917e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.9475e-08, 1.5444e-09, 2.5461e-15, 1.5437e-08, 1.1663e-08, 2.2066e-11,\n",
      "        1.6100e-09, 1.0523e-14, 6.7865e-09, 9.0216e-15, 2.5817e-08, 1.0017e-08,\n",
      "        1.1507e-14, 1.4595e-16, 4.6062e-09, 1.6514e-08, 9.5563e-09, 3.1278e-16,\n",
      "        7.2271e-09, 1.5015e-08, 7.8011e-10, 2.5518e-08, 1.4070e-08, 3.7058e-11,\n",
      "        2.4983e-09, 3.8470e-15, 7.4150e-10, 3.3649e-10, 1.3334e-08, 8.7580e-11,\n",
      "        1.0513e-09, 3.5261e-14, 1.3519e-13, 4.6618e-10, 1.5641e-08, 3.3165e-15,\n",
      "        1.5740e-08, 1.0340e-08, 1.3658e-08, 2.2860e-08, 1.1596e-08, 2.9830e-11,\n",
      "        8.0916e-15, 5.9936e-16, 1.7202e-15, 1.8204e-09, 1.9365e-09, 5.7269e-16,\n",
      "        2.0709e-08, 2.4915e-09, 8.9753e-15, 2.7790e-12, 1.4408e-15, 1.5865e-09,\n",
      "        2.0299e-10, 7.2952e-12, 2.7822e-16, 1.8164e-08, 3.4937e-16, 2.7427e-09,\n",
      "        3.7485e-16, 1.4580e-08, 1.2083e-15, 9.5646e-11, 5.7985e-11, 1.9974e-08,\n",
      "        1.5069e-09, 1.1407e-08, 1.2066e-09, 4.2619e-12, 4.9424e-09, 1.1846e-08,\n",
      "        2.4749e-08, 1.3892e-08, 1.3512e-15, 2.6892e-15, 7.8712e-09, 3.3011e-08,\n",
      "        1.6082e-08, 4.7573e-10, 4.8359e-11, 2.6541e-09, 2.9243e-16, 1.1496e-08,\n",
      "        1.2060e-14, 1.5122e-15, 2.7202e-08, 2.0156e-15, 1.3728e-08, 1.8455e-10,\n",
      "        6.1675e-10, 2.6572e-08, 4.4946e-09, 6.6839e-09, 1.4303e-09, 2.6895e-08,\n",
      "        1.3368e-08, 3.1387e-10, 7.6310e-09, 3.7014e-09, 8.7093e-09, 4.8671e-10,\n",
      "        5.5103e-10, 8.8599e-09, 6.6678e-10, 4.6541e-11, 9.9951e-09, 2.8534e-08,\n",
      "        1.8632e-08, 1.8635e-08, 2.3914e-08, 1.6865e-08, 5.2294e-10, 1.8993e-08,\n",
      "        6.3679e-11, 7.9949e-16, 9.9199e-15, 2.0777e-12, 3.2593e-15, 1.1633e-08,\n",
      "        1.8870e-08, 1.5513e-09, 4.5657e-10, 7.4309e-12, 3.9858e-09, 1.8785e-08,\n",
      "        5.8634e-11, 2.2788e-16, 1.2616e-09, 4.0963e-11, 2.0317e-08, 6.3675e-09,\n",
      "        1.9053e-09, 1.5965e-09, 5.9991e-13, 2.5336e-11, 1.7010e-09, 2.4918e-15,\n",
      "        1.5195e-08, 4.9817e-15, 1.7319e-08, 1.9246e-08, 1.1667e-08, 2.8502e-12,\n",
      "        1.4366e-08, 1.6715e-09, 7.4195e-16, 8.1486e-09, 1.8986e-08, 1.4790e-08,\n",
      "        1.9578e-08, 1.0103e-09, 1.1362e-08, 4.9968e-15, 5.0492e-15, 1.0245e-09,\n",
      "        1.5694e-10, 1.1576e-15, 1.0638e-09, 4.2903e-09, 1.2956e-15, 2.3271e-08,\n",
      "        1.0211e-08, 1.0268e-08, 2.2701e-15, 3.3100e-08, 6.8340e-09, 1.3778e-08,\n",
      "        3.8005e-09, 2.6352e-09, 6.4814e-15, 7.8362e-15, 1.3840e-09, 4.7125e-09,\n",
      "        1.4969e-08, 4.6106e-11, 6.6309e-15, 2.1105e-15, 6.4052e-09, 4.8520e-10,\n",
      "        2.0523e-08, 4.2209e-10, 1.9314e-09, 5.2551e-10, 4.4655e-10, 3.0017e-11,\n",
      "        3.8973e-15, 5.3625e-10, 1.0070e-14, 2.6732e-18, 1.0944e-11, 1.0757e-08,\n",
      "        8.4940e-15, 2.0825e-08, 1.2090e-14, 4.6520e-09, 1.6803e-11, 7.1546e-11,\n",
      "        4.6634e-09, 5.3837e-15, 1.9584e-15, 2.4337e-08, 2.4804e-09, 1.7217e-09,\n",
      "        2.9393e-16, 4.9763e-09, 6.7921e-09, 2.0969e-17, 7.7337e-11, 9.4017e-10,\n",
      "        1.2880e-08, 3.4053e-09, 1.3185e-09, 2.3486e-08, 2.1834e-12, 1.1830e-08,\n",
      "        1.3739e-08, 4.5324e-09, 9.2472e-10, 1.3635e-08, 1.8557e-15, 2.5799e-15,\n",
      "        2.0455e-08, 1.0022e-08, 9.3645e-10, 1.2273e-08, 4.0904e-08, 7.6887e-16,\n",
      "        5.3756e-09, 3.7801e-11, 6.6958e-16, 2.4873e-11, 1.0711e-11, 5.1663e-09,\n",
      "        1.5180e-08, 3.7461e-09, 3.0670e-10, 3.5975e-15, 1.2594e-09, 4.2194e-11,\n",
      "        9.3418e-10, 2.7805e-08, 1.4133e-09, 1.4596e-08, 1.8628e-08, 4.3708e-13,\n",
      "        9.2145e-10, 1.1071e-09, 4.4555e-09, 7.3047e-11, 1.8584e-15, 4.7457e-09,\n",
      "        4.9861e-09, 5.9955e-09, 7.7835e-10, 1.5675e-08, 3.8085e-09, 7.9329e-14,\n",
      "        4.2548e-15, 1.9600e-08, 3.3109e-09, 1.6896e-15, 7.5955e-16, 2.0240e-08,\n",
      "        3.2401e-15, 3.0390e-09, 9.6772e-10, 1.5214e-08, 1.9036e-08, 3.3061e-11,\n",
      "        8.8787e-09, 3.7799e-11, 5.8293e-09, 1.6694e-08, 1.4540e-11, 1.3082e-08,\n",
      "        2.7356e-12, 5.5501e-15, 8.9230e-15, 2.4464e-17, 2.3033e-17, 1.0540e-10,\n",
      "        1.3130e-08, 1.6110e-10, 1.7669e-09, 8.8849e-10, 1.0132e-08, 1.3336e-09,\n",
      "        3.6631e-11, 1.6878e-08, 4.3486e-15, 1.3966e-16, 1.6621e-08, 6.7766e-09,\n",
      "        1.3608e-15, 6.2257e-09, 1.9722e-08, 2.1855e-08, 4.4585e-15, 1.4763e-11,\n",
      "        1.7058e-08, 1.7095e-09, 1.6239e-10, 2.2614e-08, 6.9953e-09, 2.7440e-08,\n",
      "        9.2474e-09, 1.1506e-08, 1.6933e-08, 2.9051e-08, 1.3363e-08, 8.6113e-09,\n",
      "        2.1143e-08, 9.4865e-15, 9.4618e-16, 1.1147e-08, 1.3106e-11, 1.3589e-08,\n",
      "        1.1720e-14, 9.2400e-09, 1.4647e-08, 6.4153e-10, 8.8732e-09, 3.9829e-09,\n",
      "        6.2962e-17, 8.2015e-18, 1.6100e-08, 6.5250e-15, 3.5083e-08, 2.0629e-08,\n",
      "        1.8783e-10, 1.6481e-08, 8.8828e-10, 4.4038e-09, 5.7989e-12, 4.1211e-09,\n",
      "        1.0653e-16, 1.2972e-08, 9.3926e-09, 4.4452e-15, 1.8713e-08, 1.1246e-14,\n",
      "        1.6286e-08, 8.6430e-10, 1.3447e-08, 2.3555e-10, 1.1668e-14, 1.2209e-09,\n",
      "        1.5650e-15, 1.2745e-08, 6.1800e-09, 3.8746e-11, 1.0608e-08, 7.2984e-10,\n",
      "        1.5665e-08, 3.6993e-15, 2.0661e-08, 1.2571e-16, 2.0421e-09, 9.4393e-09,\n",
      "        1.0234e-08, 5.2327e-13, 6.4349e-15, 1.0600e-14, 1.0966e-15, 6.5134e-15,\n",
      "        6.3352e-09, 4.7846e-10, 1.7646e-09, 1.5652e-08, 3.1874e-16, 3.5793e-14,\n",
      "        1.5391e-09, 9.4087e-15, 9.8189e-09, 2.1017e-15, 4.7391e-09, 4.2175e-15,\n",
      "        2.1796e-08, 3.5624e-15, 2.0071e-08, 5.4104e-09, 5.4071e-17, 1.5090e-08,\n",
      "        1.2241e-08, 1.6667e-08, 2.5744e-10, 9.1663e-12, 1.2802e-08, 5.2757e-09,\n",
      "        1.9912e-08, 6.4653e-17, 6.5050e-15, 1.3364e-09, 3.4755e-08, 7.3223e-16,\n",
      "        4.4407e-15, 1.9378e-08, 7.5958e-12, 2.0903e-08, 1.2121e-08, 3.6953e-18,\n",
      "        7.5526e-10, 4.5151e-10, 8.2070e-17, 1.7203e-08, 3.3584e-10, 2.2024e-09,\n",
      "        2.0003e-11, 3.8373e-10, 2.8567e-09, 4.4751e-09, 1.1735e-15, 5.0675e-15,\n",
      "        3.5921e-15, 1.7427e-10, 1.8267e-08, 6.2948e-09, 2.2700e-09, 1.6566e-08,\n",
      "        8.6428e-10, 2.9085e-16, 1.3162e-09, 2.2712e-08, 4.3647e-10, 5.0906e-16,\n",
      "        3.5278e-08, 1.1240e-14, 5.2918e-12, 5.0998e-11, 1.6797e-10, 1.6786e-08,\n",
      "        6.5804e-16, 1.2359e-08, 1.0769e-08, 3.8844e-15, 1.3998e-15, 1.1301e-09,\n",
      "        1.4363e-09, 8.8132e-09, 1.4858e-10, 1.2518e-08, 5.6297e-10, 2.7409e-15,\n",
      "        4.3790e-15, 2.2596e-08, 3.1507e-09, 1.2305e-09, 2.0389e-08, 1.1506e-08,\n",
      "        1.6557e-08, 9.7057e-17, 9.8767e-11, 5.7406e-09, 2.2081e-09, 3.4689e-08,\n",
      "        7.7378e-12, 1.1264e-14, 9.9716e-09, 3.8555e-10, 7.5066e-16, 1.8291e-12,\n",
      "        4.1841e-12, 2.2889e-08, 1.0642e-08, 1.4824e-09, 1.4868e-09, 1.7325e-08,\n",
      "        1.1151e-10, 5.9030e-13, 5.0015e-12, 1.0450e-09, 1.6584e-08, 2.7627e-15,\n",
      "        4.2929e-09, 7.1499e-13, 3.7435e-15, 3.6572e-10, 5.0091e-09, 3.0635e-15,\n",
      "        5.9213e-13, 2.2776e-09, 8.1224e-11, 2.0224e-08, 1.4561e-09, 1.6159e-10,\n",
      "        1.7835e-08, 4.4490e-15, 6.5183e-09, 5.2317e-16, 8.2393e-09, 8.8661e-09,\n",
      "        7.1045e-09, 8.5496e-15, 5.0327e-09, 3.8049e-08, 1.3168e-08, 6.0651e-16,\n",
      "        6.3163e-15, 5.8267e-12, 1.7668e-08, 2.6764e-09, 2.2615e-09, 2.3443e-09,\n",
      "        2.3781e-08, 1.8787e-08, 1.2108e-16, 6.0656e-10, 1.5559e-08, 3.6550e-09,\n",
      "        6.9091e-10, 2.0533e-08], device='cuda:0')}, 2: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-7.0885e-05,  1.9382e-06,  2.3958e-06,  ...,  6.4877e-07,\n",
      "         -5.0527e-06, -5.1087e-03],\n",
      "        [-1.6259e-03,  3.9547e-06, -1.9923e-06,  ...,  1.1930e-06,\n",
      "         -3.9144e-06,  9.1979e-05],\n",
      "        [-2.3663e-03,  2.3255e-07,  2.5453e-06,  ..., -9.1580e-07,\n",
      "         -1.4278e-06, -1.3927e-04],\n",
      "        ...,\n",
      "        [-5.8886e-05, -3.4155e-07,  1.8545e-06,  ..., -2.2695e-06,\n",
      "         -2.4254e-06,  1.3068e-02],\n",
      "        [-2.8450e-04,  3.2381e-06, -2.0263e-09,  ...,  2.5485e-06,\n",
      "          7.1331e-06, -6.7890e-03],\n",
      "        [-5.0742e-04,  1.7111e-06,  2.4671e-06,  ..., -3.5836e-06,\n",
      "         -1.6462e-06, -1.2962e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.7347e-06, 9.9549e-10, 3.9410e-13,  ..., 4.1628e-14, 1.0690e-08,\n",
      "         1.2433e-04],\n",
      "        [5.1879e-06, 4.2363e-09, 2.8099e-13,  ..., 2.6273e-10, 1.3684e-07,\n",
      "         5.4846e-05],\n",
      "        [3.8722e-06, 1.1912e-09, 4.4084e-13,  ..., 1.5259e-10, 3.0898e-09,\n",
      "         1.1698e-04],\n",
      "        ...,\n",
      "        [6.4887e-06, 2.6558e-08, 2.4674e-13,  ..., 5.3097e-10, 2.3140e-08,\n",
      "         2.2644e-04],\n",
      "        [4.1514e-06, 1.8872e-08, 1.9606e-16,  ..., 4.4187e-13, 3.3500e-07,\n",
      "         1.6837e-05],\n",
      "        [1.4846e-06, 1.3911e-08, 4.1607e-13,  ..., 8.3757e-13, 1.2886e-07,\n",
      "         6.2204e-05]], device='cuda:0')}, 3: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-9.0630e-05, -8.2463e-05, -6.5935e-05, -3.7614e-06, -1.5848e-04,\n",
      "         3.7759e-05, -2.8120e-05, -6.5349e-05,  1.3497e-04,  7.0591e-05,\n",
      "        -1.4777e-05,  6.8557e-05, -6.3710e-05, -1.5780e-04, -3.6659e-04,\n",
      "         1.5892e-04,  5.8129e-05,  2.8079e-04,  1.6937e-05, -1.4957e-05,\n",
      "         1.4009e-04,  1.5493e-04, -1.6039e-04, -1.3312e-04,  1.1718e-04,\n",
      "         4.6659e-05, -1.0301e-04, -1.7242e-04,  1.0807e-04,  2.7684e-05,\n",
      "        -7.8761e-07,  9.1820e-06,  6.9551e-05,  4.6182e-05,  4.2810e-05,\n",
      "         2.5587e-04,  2.6596e-04,  6.4407e-05, -2.1044e-05,  1.6913e-04,\n",
      "        -1.8660e-04, -1.4859e-05, -1.1078e-05,  4.6134e-05, -6.2789e-05,\n",
      "         1.2649e-04, -3.6814e-05, -1.5560e-04, -2.3235e-04, -1.2560e-04,\n",
      "        -1.2338e-04,  4.0399e-05,  1.6267e-05, -1.0499e-04,  1.4573e-04,\n",
      "        -7.6611e-06, -1.7175e-04, -1.1729e-04, -1.5458e-04, -1.4304e-04,\n",
      "         1.1065e-05,  7.9151e-05,  9.5645e-05, -7.2828e-06,  1.3659e-05,\n",
      "         7.7614e-06,  3.6450e-05, -5.1003e-05, -4.7341e-05,  1.8705e-04,\n",
      "        -1.9411e-04,  4.0068e-05, -2.2976e-04,  1.1586e-04,  6.0447e-05,\n",
      "        -7.6478e-06,  1.3718e-05,  1.4064e-05, -4.4013e-05,  3.9908e-05,\n",
      "         1.3334e-04,  3.7597e-06, -5.5593e-05,  5.2709e-05, -1.2615e-04,\n",
      "        -1.9487e-05,  3.7585e-05, -4.4460e-05, -1.6561e-04, -1.6286e-05,\n",
      "         3.3757e-04, -1.7329e-04, -1.4216e-04,  6.2494e-06, -7.8969e-05,\n",
      "         9.1260e-05, -7.6186e-06,  3.9955e-05, -7.0968e-05, -5.7366e-05,\n",
      "         5.2100e-05, -8.2981e-05,  2.6111e-06, -3.1266e-07,  5.6870e-05,\n",
      "        -1.1335e-04, -4.5924e-05,  1.4068e-04, -7.9769e-05, -1.6669e-04,\n",
      "        -4.4812e-05, -1.1286e-04,  1.6943e-05, -1.7139e-04, -1.8690e-05,\n",
      "         3.0187e-05, -4.8903e-05,  1.8303e-04,  1.4580e-05, -2.7081e-05,\n",
      "        -4.7781e-05, -4.7728e-05, -9.2357e-05,  3.4352e-05, -2.9365e-04,\n",
      "        -2.3690e-05,  6.5317e-06, -2.3155e-05,  4.7645e-05,  1.1186e-04,\n",
      "         1.0927e-05, -1.2869e-04,  1.9157e-04, -2.8289e-05, -1.6248e-04,\n",
      "         1.2489e-04,  1.4723e-05,  2.4633e-05,  1.3178e-05, -2.8078e-05,\n",
      "        -4.0029e-07, -5.4249e-05,  7.7503e-05,  1.7782e-04, -5.3480e-05,\n",
      "         4.9426e-05, -6.2166e-05, -1.3158e-04, -4.4809e-06,  7.8319e-05,\n",
      "        -1.0435e-04, -7.7949e-05, -1.0718e-05,  9.5387e-05,  2.7870e-05,\n",
      "         2.1754e-04, -1.4540e-04, -3.6189e-05,  4.8091e-05,  4.2456e-05,\n",
      "         1.6728e-04, -4.1773e-05,  1.1414e-04, -1.1599e-04, -1.7556e-05,\n",
      "        -8.3762e-05,  5.2520e-05,  5.8491e-05, -5.6069e-05, -4.3788e-05,\n",
      "         6.7750e-05, -1.0410e-04,  6.6774e-06, -1.6015e-04, -8.1964e-06,\n",
      "        -4.8983e-05,  1.7092e-04, -8.6622e-05,  1.7754e-05,  1.9363e-05,\n",
      "         2.0118e-04, -1.6577e-04, -1.5569e-04,  5.9818e-06, -8.2363e-05,\n",
      "         1.1726e-04,  2.8181e-04, -1.0500e-05,  5.4702e-05,  2.5023e-07,\n",
      "         2.3844e-04,  1.0276e-06, -1.7557e-04, -4.9033e-05, -2.7095e-04,\n",
      "        -2.1244e-05,  7.9905e-06, -2.7262e-04,  6.4500e-05, -1.3465e-04,\n",
      "        -3.8423e-05,  6.6617e-05, -9.3965e-05, -1.3500e-05, -2.2715e-04,\n",
      "         5.4323e-05, -6.7484e-05, -1.1394e-05,  7.5758e-05,  1.2195e-04,\n",
      "         1.3827e-04, -2.0201e-04,  3.3044e-05,  1.0815e-04,  2.8460e-05,\n",
      "        -2.5741e-05, -3.1971e-05, -7.8876e-05, -3.7883e-05, -6.0731e-06,\n",
      "        -1.6393e-04,  2.9525e-04,  6.1074e-06,  1.8213e-04,  3.3876e-04,\n",
      "         4.7168e-05,  2.7326e-05,  3.0875e-05,  1.7125e-04,  3.3064e-06,\n",
      "        -1.2067e-04, -8.8793e-05, -7.2999e-05,  1.0922e-05,  1.7527e-04,\n",
      "         4.7624e-05,  1.0496e-04, -9.2417e-05,  1.0708e-04,  1.3662e-05,\n",
      "         1.4051e-05, -2.9181e-05, -6.1628e-05,  2.2159e-05,  2.6971e-04,\n",
      "        -4.4387e-05,  7.1700e-05,  1.6286e-05, -6.9442e-05,  1.2081e-04,\n",
      "         5.7337e-05,  1.0167e-05,  6.8306e-06,  9.8018e-05, -8.6945e-05,\n",
      "        -5.0196e-06], device='cuda:0'), 'exp_avg_sq': tensor([2.7836e-08, 3.2104e-08, 1.5171e-08, 3.0025e-08, 1.0082e-07, 7.7607e-08,\n",
      "        4.1920e-08, 1.6620e-08, 2.8190e-08, 3.7252e-08, 6.6185e-08, 8.6212e-08,\n",
      "        7.2737e-08, 7.4530e-08, 8.0713e-08, 2.0471e-07, 1.1847e-07, 6.8023e-08,\n",
      "        2.3372e-08, 6.9765e-08, 5.0552e-08, 7.8758e-08, 3.9807e-08, 1.9086e-07,\n",
      "        7.9373e-08, 3.9325e-08, 7.4603e-08, 6.6980e-08, 3.7786e-08, 2.2862e-09,\n",
      "        4.0456e-08, 7.3508e-10, 1.0371e-07, 5.9242e-08, 6.2553e-08, 1.0716e-07,\n",
      "        9.4552e-08, 1.0758e-08, 1.6098e-08, 5.4571e-08, 5.4793e-08, 3.4193e-08,\n",
      "        3.3565e-09, 3.2277e-08, 3.8290e-08, 1.1999e-08, 2.7061e-09, 4.9140e-08,\n",
      "        5.0647e-08, 3.6058e-08, 5.4060e-08, 5.0093e-08, 7.5133e-08, 1.1196e-07,\n",
      "        8.7409e-08, 7.4184e-09, 1.5871e-07, 1.2684e-08, 4.9893e-08, 1.1905e-07,\n",
      "        2.0370e-08, 9.9625e-08, 3.6952e-08, 2.1898e-08, 7.4136e-08, 1.5703e-08,\n",
      "        2.2510e-07, 7.0069e-08, 1.3185e-07, 8.9492e-08, 1.1021e-07, 7.3799e-09,\n",
      "        7.3307e-08, 4.7488e-08, 2.5926e-08, 2.6305e-08, 3.1554e-08, 2.6528e-08,\n",
      "        1.1654e-07, 7.6284e-09, 8.0913e-08, 3.7095e-08, 3.7230e-08, 2.8752e-08,\n",
      "        6.6846e-08, 2.6851e-08, 5.9667e-08, 9.8686e-08, 4.6565e-08, 9.1315e-08,\n",
      "        2.2441e-07, 1.1867e-07, 4.9317e-08, 7.2483e-08, 5.2235e-08, 3.8584e-08,\n",
      "        7.3374e-08, 1.7164e-07, 1.1922e-08, 6.5387e-08, 4.4074e-08, 6.4455e-08,\n",
      "        5.8647e-08, 1.1113e-08, 1.2940e-08, 7.9528e-08, 2.1508e-08, 7.0611e-08,\n",
      "        7.7881e-08, 5.6419e-08, 6.7540e-08, 2.4683e-08, 2.9451e-08, 4.7426e-08,\n",
      "        8.2816e-08, 6.6763e-08, 7.3337e-08, 7.9901e-08, 2.2595e-08, 1.0049e-07,\n",
      "        1.7338e-08, 3.4265e-08, 2.5077e-08, 1.8169e-08, 1.3652e-07, 7.1800e-08,\n",
      "        5.6176e-08, 3.4647e-08, 3.8287e-08, 1.0287e-07, 5.0772e-08, 8.7005e-08,\n",
      "        3.3986e-08, 9.9843e-08, 5.9042e-08, 9.5202e-08, 1.1066e-08, 7.5467e-08,\n",
      "        1.8559e-08, 2.5736e-10, 1.7218e-08, 4.5786e-08, 5.7184e-08, 6.9608e-08,\n",
      "        9.1053e-08, 1.3803e-08, 7.3853e-08, 2.4137e-07, 7.2754e-12, 2.5176e-08,\n",
      "        5.0293e-08, 5.5706e-08, 5.2334e-08, 5.0077e-08, 2.6533e-08, 3.5747e-08,\n",
      "        6.3107e-08, 3.5502e-08, 1.6449e-08, 1.3920e-08, 2.6082e-08, 1.3415e-08,\n",
      "        2.2090e-08, 8.6883e-08, 3.1002e-08, 3.6898e-08, 6.9702e-08, 9.0589e-08,\n",
      "        2.4676e-08, 7.2374e-08, 1.2142e-07, 5.5506e-08, 3.2536e-08, 2.6914e-08,\n",
      "        5.8630e-08, 1.4724e-08, 6.7849e-08, 5.8386e-08, 5.2033e-08, 3.1742e-08,\n",
      "        9.2121e-08, 8.3734e-08, 8.0559e-08, 1.3549e-08, 4.5673e-08, 6.8090e-08,\n",
      "        1.4963e-07, 2.3207e-08, 1.4916e-08, 2.4791e-09, 4.7374e-08, 2.9253e-08,\n",
      "        1.5707e-07, 1.0215e-07, 8.4001e-08, 2.6950e-08, 2.3250e-08, 6.6744e-08,\n",
      "        1.8215e-08, 7.9516e-08, 4.4257e-08, 8.0384e-08, 2.6981e-08, 7.2700e-09,\n",
      "        7.0404e-08, 3.2937e-08, 2.0021e-08, 1.1928e-07, 5.3126e-08, 2.5639e-08,\n",
      "        6.0383e-08, 5.2236e-08, 7.0382e-08, 2.6918e-08, 7.2446e-08, 6.8658e-08,\n",
      "        1.8522e-08, 4.4455e-08, 4.7025e-08, 1.1004e-07, 1.7168e-08, 5.8546e-08,\n",
      "        1.5236e-07, 6.4070e-08, 1.2457e-07, 2.8889e-08, 7.5063e-09, 1.3424e-07,\n",
      "        5.5190e-08, 4.5479e-09, 1.0412e-07, 5.8982e-08, 8.3826e-08, 4.4569e-09,\n",
      "        2.1893e-07, 5.6042e-08, 4.8177e-08, 3.7077e-08, 6.3057e-08, 3.9061e-08,\n",
      "        7.9683e-08, 6.2009e-08, 3.3328e-08, 1.0802e-07, 6.7066e-08, 1.4420e-07,\n",
      "        2.9261e-08, 4.4613e-09, 4.5210e-08, 2.6472e-08, 1.9376e-08, 1.2229e-08,\n",
      "        3.0263e-08, 6.8087e-08, 4.8069e-08, 4.2416e-08], device='cuda:0')}, 4: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 2.6819e-04,  3.3653e-04,  2.7517e-04,  ..., -6.3895e-04,\n",
      "          1.4300e-05,  5.3362e-04],\n",
      "        [-7.2848e-04,  3.5699e-05, -1.5064e-03,  ..., -1.5402e-03,\n",
      "          1.0514e-03, -1.5118e-03],\n",
      "        [ 5.9928e-05,  6.5540e-04,  9.1780e-04,  ...,  1.9640e-04,\n",
      "         -8.5244e-04,  6.8605e-04],\n",
      "        ...,\n",
      "        [-5.9897e-04,  1.0121e-03,  1.3802e-03,  ...,  3.7632e-04,\n",
      "          1.1002e-05, -4.8673e-04],\n",
      "        [ 1.2226e-03,  5.2945e-06, -1.6198e-03,  ..., -2.1844e-03,\n",
      "          2.2888e-04,  1.5717e-04],\n",
      "        [-3.0159e-04, -4.8449e-04,  7.6874e-04,  ..., -4.0860e-04,\n",
      "          3.3017e-04,  1.4730e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[4.3101e-06, 8.0263e-07, 1.1224e-06,  ..., 6.1028e-06, 4.3170e-08,\n",
      "         1.2582e-07],\n",
      "        [1.5830e-06, 5.6718e-06, 4.6721e-06,  ..., 2.0525e-05, 2.3085e-07,\n",
      "         1.6736e-06],\n",
      "        [1.1937e-05, 1.9165e-06, 5.1593e-06,  ..., 1.4309e-05, 2.4244e-07,\n",
      "         1.4555e-06],\n",
      "        ...,\n",
      "        [2.5625e-06, 3.6881e-06, 4.6249e-06,  ..., 1.8867e-05, 4.1853e-08,\n",
      "         1.4484e-06],\n",
      "        [1.8671e-06, 1.5636e-07, 1.1377e-06,  ..., 1.5382e-05, 9.6649e-08,\n",
      "         5.8961e-07],\n",
      "        [3.8197e-06, 5.9316e-07, 1.1496e-06,  ..., 9.1228e-06, 5.9496e-07,\n",
      "         4.0961e-07]], device='cuda:0')}, 5: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-3.5364e-05, -2.1701e-04,  4.6371e-05, -6.4855e-06,  6.2433e-05,\n",
      "        -2.2000e-04, -3.5384e-04,  1.7044e-04,  4.0032e-06,  2.7686e-04,\n",
      "        -2.9467e-05, -4.5638e-04,  9.6819e-05,  7.9988e-04, -1.4377e-04,\n",
      "        -3.3496e-05, -3.4637e-05,  3.5943e-05,  9.0447e-05, -2.0898e-04,\n",
      "         3.9487e-04, -2.4132e-05,  2.4526e-04, -2.2065e-04,  1.4865e-04,\n",
      "        -7.2423e-05, -2.2745e-04,  1.4530e-04,  5.1264e-05, -1.8716e-04,\n",
      "         1.6976e-04, -1.5010e-04, -3.6285e-04,  1.4471e-05,  2.0509e-04,\n",
      "        -3.3236e-04, -2.3848e-04, -2.2486e-04,  3.1088e-04,  3.8301e-04,\n",
      "        -2.9006e-05,  1.3296e-04,  3.2908e-04,  1.0571e-04, -1.8148e-05,\n",
      "        -1.4037e-04,  1.9340e-05,  8.3154e-07, -1.5448e-05, -7.0474e-06,\n",
      "         1.3394e-04,  9.5125e-05,  8.4088e-05, -1.6368e-04, -2.7607e-04,\n",
      "         9.0531e-05, -3.2059e-04, -3.9297e-05,  5.9297e-05, -2.0004e-05,\n",
      "         1.1437e-04,  2.0444e-04, -1.2819e-04,  1.8975e-04, -2.6775e-04,\n",
      "         8.6959e-05, -2.6291e-05,  3.4842e-05,  9.1032e-05,  1.8297e-04,\n",
      "        -2.8652e-04,  2.0749e-05, -1.2496e-04, -1.7893e-04,  1.1843e-04,\n",
      "         1.0530e-04,  2.5920e-05,  9.3638e-05,  3.9743e-04, -1.6861e-05,\n",
      "         3.9959e-05, -2.7007e-04, -2.9063e-05, -5.3750e-05,  1.1719e-04,\n",
      "         1.5114e-04,  2.5390e-04, -7.6898e-05,  4.5444e-05, -4.4221e-04,\n",
      "        -3.6477e-04, -1.7792e-04, -1.4604e-05, -2.4181e-04,  2.0607e-05,\n",
      "         2.6909e-05,  5.1049e-05,  2.1821e-05,  2.5516e-05,  2.0404e-04,\n",
      "         1.9807e-04, -8.7685e-06,  1.0355e-04,  5.9208e-05,  4.0236e-04,\n",
      "         1.4866e-04,  2.1209e-04,  5.6568e-05,  1.0790e-04, -1.9180e-04,\n",
      "         1.8568e-04,  3.4416e-05, -1.6822e-04, -2.7071e-05,  3.9064e-05,\n",
      "         1.8207e-04,  1.2305e-04,  3.1949e-04, -5.4992e-05, -1.2138e-04,\n",
      "         1.1821e-05, -1.8757e-04, -9.0649e-05,  4.5202e-04,  4.0644e-05,\n",
      "         3.2327e-05, -4.5888e-04,  2.1928e-04,  1.6951e-05,  2.1267e-04,\n",
      "        -2.7878e-04, -1.4929e-04, -1.9387e-04, -3.5037e-05,  5.0496e-05,\n",
      "         9.0704e-05, -1.0553e-04,  1.4006e-05, -1.1091e-04,  1.2007e-04,\n",
      "        -1.1994e-04,  1.4270e-05,  1.2451e-04,  5.0767e-05,  2.4671e-04,\n",
      "        -3.3278e-04,  5.6631e-05,  2.4980e-04, -1.0528e-05,  1.5659e-04,\n",
      "         3.0223e-04, -1.9576e-04, -3.9024e-04,  4.6215e-05, -9.7864e-05,\n",
      "        -2.6786e-04,  1.5306e-04, -1.2189e-04,  4.8387e-05,  4.9100e-06,\n",
      "        -2.8613e-05, -9.1024e-05,  7.4068e-05, -2.0504e-04, -3.1291e-05,\n",
      "         1.3484e-04,  2.7030e-04, -1.4649e-04, -2.0876e-04, -2.0567e-04,\n",
      "         8.1067e-05,  4.5813e-04, -9.1429e-05, -4.6303e-04,  2.8864e-05,\n",
      "        -8.8509e-05, -4.2629e-06,  2.6915e-05,  1.0171e-04, -1.6965e-04,\n",
      "         2.9336e-04, -1.0201e-04, -5.6071e-05, -1.0280e-04, -2.6957e-04,\n",
      "        -2.1011e-04, -4.9432e-04,  2.9478e-04, -1.6217e-04, -1.0510e-04,\n",
      "        -3.5005e-04,  1.4634e-04,  1.6187e-04,  6.8610e-05,  2.0790e-04,\n",
      "        -1.6805e-04,  3.8894e-05,  4.0846e-05, -3.7228e-04,  1.4277e-04,\n",
      "         1.1328e-04,  2.7417e-04,  2.3159e-04,  2.3030e-04,  4.0929e-05,\n",
      "         7.5582e-05,  1.6786e-04, -2.4534e-05,  2.7146e-04,  1.8607e-04,\n",
      "        -2.1930e-04, -5.4461e-04, -9.4046e-05, -1.0815e-04,  1.9208e-04,\n",
      "        -1.7648e-04, -2.8489e-04, -1.3203e-04, -6.4738e-06, -1.4136e-04,\n",
      "        -2.0734e-04,  7.9415e-05,  6.7714e-05, -2.2975e-04, -2.9401e-04,\n",
      "        -2.5770e-04, -2.0166e-04, -1.4095e-04,  4.1741e-05,  9.5609e-05,\n",
      "         1.2639e-04, -6.3138e-05,  1.9400e-05, -1.5103e-04,  8.7333e-06,\n",
      "        -5.0419e-04, -1.8546e-05, -1.9185e-04,  6.4941e-05,  1.3883e-04,\n",
      "         1.1817e-04, -1.2150e-04, -1.0770e-04,  8.0935e-05, -1.8614e-04,\n",
      "        -2.5566e-04,  4.9484e-05,  9.1406e-05,  1.3132e-04,  2.0594e-04,\n",
      "         6.3662e-05, -7.7937e-05,  2.4093e-04,  3.7155e-05,  3.4442e-06,\n",
      "         1.5283e-04], device='cuda:0'), 'exp_avg_sq': tensor([6.2378e-08, 1.6769e-07, 9.5971e-08, 4.4029e-08, 3.9030e-07, 9.3997e-08,\n",
      "        1.1686e-07, 1.6596e-07, 7.6376e-08, 2.1939e-07, 1.1763e-07, 2.2323e-07,\n",
      "        8.8605e-08, 2.1446e-07, 2.8095e-07, 1.1165e-07, 1.3112e-07, 4.8587e-08,\n",
      "        5.1182e-08, 1.6930e-07, 2.5734e-07, 2.4195e-07, 3.4464e-07, 3.3416e-07,\n",
      "        1.3818e-07, 1.7623e-07, 1.0023e-07, 1.7569e-07, 1.9969e-07, 2.8849e-07,\n",
      "        3.3790e-07, 3.8082e-07, 2.6424e-07, 1.0550e-07, 2.2495e-07, 1.7839e-07,\n",
      "        3.1403e-07, 1.4052e-07, 2.4020e-07, 1.2316e-07, 2.2939e-07, 3.3013e-08,\n",
      "        1.8363e-07, 1.4689e-07, 1.2160e-07, 2.8819e-07, 4.3904e-08, 5.7717e-08,\n",
      "        1.6225e-07, 4.4906e-08, 1.1213e-07, 3.9142e-07, 2.4229e-07, 6.7837e-08,\n",
      "        3.0618e-07, 1.7644e-07, 2.5390e-07, 1.0505e-07, 7.4437e-08, 8.2033e-08,\n",
      "        2.2133e-07, 3.5475e-07, 1.7347e-07, 1.7336e-07, 6.9790e-08, 2.0031e-08,\n",
      "        1.2958e-07, 1.0251e-07, 5.9828e-08, 1.3794e-07, 1.7631e-07, 3.6007e-08,\n",
      "        9.8802e-08, 1.0667e-07, 2.1898e-07, 6.5071e-08, 1.3023e-07, 3.3744e-08,\n",
      "        6.7046e-07, 1.5142e-07, 1.9216e-07, 6.4353e-08, 1.9419e-07, 3.9285e-07,\n",
      "        2.0161e-07, 3.2690e-07, 1.2579e-07, 6.3979e-08, 2.4677e-07, 2.6139e-07,\n",
      "        1.0500e-07, 1.3860e-07, 1.0808e-07, 1.4154e-07, 1.6775e-07, 1.3229e-07,\n",
      "        1.8182e-07, 5.0009e-08, 2.8256e-08, 1.2190e-07, 9.6212e-08, 1.0032e-07,\n",
      "        1.4391e-07, 1.9370e-07, 2.4800e-07, 5.6241e-08, 2.1799e-07, 1.3930e-07,\n",
      "        5.1798e-08, 1.3756e-07, 1.4815e-07, 4.9901e-08, 1.4045e-07, 3.6779e-08,\n",
      "        1.3145e-07, 2.0461e-07, 1.1974e-07, 1.9733e-07, 3.3949e-08, 8.8541e-08,\n",
      "        1.6310e-07, 2.7026e-07, 7.7653e-08, 2.6812e-07, 3.1179e-07, 1.7296e-08,\n",
      "        1.3326e-07, 2.1960e-07, 1.4762e-07, 6.1644e-08, 8.6513e-08, 7.1863e-08,\n",
      "        2.3503e-07, 2.7755e-07, 1.1913e-07, 2.1306e-07, 1.0448e-07, 2.4968e-07,\n",
      "        1.1973e-07, 5.7256e-08, 2.5890e-07, 9.1979e-08, 2.4306e-07, 1.1822e-07,\n",
      "        9.2205e-08, 2.6551e-07, 3.0931e-08, 1.2158e-07, 9.2446e-09, 4.6729e-07,\n",
      "        1.2635e-07, 1.1515e-07, 7.5001e-08, 1.8549e-07, 2.3112e-07, 1.4074e-07,\n",
      "        1.3309e-07, 1.8972e-07, 1.4653e-07, 7.8514e-08, 3.9777e-08, 6.3633e-08,\n",
      "        2.1284e-07, 2.4021e-07, 9.7134e-08, 2.7368e-07, 1.8823e-07, 2.4056e-07,\n",
      "        1.5031e-07, 1.1141e-07, 2.6078e-07, 1.4823e-07, 2.8615e-08, 2.7039e-07,\n",
      "        7.9845e-08, 2.4362e-07, 1.9010e-07, 1.0283e-07, 1.9775e-07, 2.4360e-07,\n",
      "        2.0137e-07, 4.0701e-07, 1.3205e-07, 1.5753e-07, 2.4464e-07, 3.3916e-07,\n",
      "        1.5893e-07, 1.7515e-07, 1.0924e-07, 1.1246e-07, 1.6635e-07, 1.7738e-07,\n",
      "        1.5252e-07, 8.9902e-08, 1.6092e-07, 1.6430e-07, 4.2484e-07, 7.1168e-08,\n",
      "        1.7397e-07, 1.2926e-07, 1.2915e-07, 4.6798e-08, 1.2036e-07, 1.8943e-07,\n",
      "        8.5200e-09, 1.3789e-07, 1.3012e-07, 2.3438e-07, 7.7490e-08, 3.0812e-07,\n",
      "        1.9139e-07, 4.8601e-07, 2.6182e-07, 1.5306e-07, 1.1895e-07, 2.2099e-07,\n",
      "        1.1071e-07, 5.5264e-07, 1.4878e-07, 2.6564e-07, 2.9628e-07, 4.0321e-07,\n",
      "        1.1311e-07, 2.9969e-07, 1.1265e-07, 8.5412e-08, 1.8604e-07, 3.3819e-07,\n",
      "        1.0113e-07, 1.4504e-07, 4.8145e-07, 3.4256e-07, 1.1935e-07, 1.8909e-07,\n",
      "        6.4251e-08, 2.2262e-07, 1.3126e-07, 2.2351e-07, 7.2924e-08, 7.6561e-08,\n",
      "        3.9476e-08, 1.0385e-07, 1.7161e-07, 1.0305e-07, 1.2579e-07, 1.6269e-07,\n",
      "        7.5738e-08, 1.7077e-07, 7.5277e-08, 5.4988e-07, 2.8097e-07, 1.6067e-07,\n",
      "        2.3776e-07, 1.3147e-07, 2.0927e-07, 7.0528e-08], device='cuda:0')}, 6: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-0.0003, -0.0012, -0.0036,  ...,  0.0061,  0.0011,  0.0003],\n",
      "        [ 0.0016, -0.0079, -0.0010,  ..., -0.0063,  0.0008,  0.0004],\n",
      "        [ 0.0012, -0.0039, -0.0006,  ...,  0.0013, -0.0005, -0.0011],\n",
      "        ...,\n",
      "        [-0.0007, -0.0046,  0.0074,  ...,  0.0088, -0.0039, -0.0006],\n",
      "        [-0.0006,  0.0085,  0.0013,  ..., -0.0042, -0.0041, -0.0033],\n",
      "        [ 0.0015,  0.0039, -0.0034,  ..., -0.0007, -0.0004,  0.0002]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[3.2504e-06, 6.4548e-06, 1.4868e-05,  ..., 4.5104e-05, 9.2011e-07,\n",
      "         1.8170e-06],\n",
      "        [7.7476e-06, 1.4261e-04, 1.6673e-05,  ..., 4.5199e-04, 6.4095e-05,\n",
      "         5.6421e-06],\n",
      "        [3.9870e-06, 1.2693e-05, 1.2497e-05,  ..., 3.7751e-05, 1.5373e-06,\n",
      "         3.7749e-06],\n",
      "        ...,\n",
      "        [1.6897e-06, 1.0160e-05, 1.4665e-05,  ..., 4.6379e-05, 1.0172e-05,\n",
      "         1.4271e-06],\n",
      "        [8.0487e-06, 1.0983e-04, 2.2993e-05,  ..., 1.7507e-04, 2.5053e-05,\n",
      "         5.4009e-06],\n",
      "        [6.5306e-06, 1.2477e-05, 1.1895e-05,  ..., 4.0435e-05, 1.7431e-06,\n",
      "         1.2987e-06]], device='cuda:0')}, 7: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-2.6442e-04, -7.5459e-04,  1.0512e-04,  7.0600e-04,  9.4020e-04,\n",
      "         5.9184e-04, -4.8354e-04,  5.8312e-04, -6.1133e-04, -4.2958e-04,\n",
      "         3.3716e-04, -9.3848e-04,  6.2133e-04, -8.0447e-04,  4.7254e-04,\n",
      "        -5.1334e-04, -6.2395e-04,  4.7245e-04,  1.3141e-03,  2.2811e-04,\n",
      "        -7.4097e-04, -6.9498e-04, -5.9210e-04,  2.2636e-04,  2.4695e-04,\n",
      "        -6.0090e-04, -4.2947e-04, -8.6988e-05,  5.0244e-04,  3.4238e-04,\n",
      "        -4.7457e-04,  4.4232e-04,  5.9560e-04,  2.4807e-04, -5.0771e-04,\n",
      "        -7.6893e-04, -3.1888e-04, -5.4231e-04,  1.1390e-03, -1.5676e-04,\n",
      "        -1.1948e-03,  2.4171e-04, -1.1110e-04,  2.5843e-04, -4.6004e-05,\n",
      "        -2.3079e-04,  2.1541e-04, -2.8414e-04, -6.0022e-04,  1.5030e-03,\n",
      "         1.0618e-03,  4.0981e-04, -7.0086e-04,  1.4855e-03, -2.4727e-04,\n",
      "         1.3441e-03,  8.0683e-05, -2.3712e-05,  1.0676e-03,  1.9405e-04,\n",
      "         5.6954e-05,  5.9889e-04, -6.8509e-05,  8.1204e-06], device='cuda:0'), 'exp_avg_sq': tensor([4.6769e-07, 3.4715e-06, 6.4086e-07, 1.1229e-06, 2.4650e-06, 9.8711e-07,\n",
      "        1.0121e-06, 1.3657e-06, 2.1257e-06, 1.5985e-06, 7.4626e-07, 2.7418e-06,\n",
      "        4.9419e-06, 2.5062e-06, 3.4682e-06, 4.0726e-06, 7.3815e-07, 1.3564e-06,\n",
      "        2.7962e-06, 2.0812e-06, 2.3344e-06, 8.0202e-07, 1.1070e-06, 8.1645e-07,\n",
      "        6.4986e-07, 1.5263e-06, 1.2171e-06, 1.5933e-06, 4.8590e-07, 3.4971e-06,\n",
      "        2.7123e-06, 9.5633e-07, 1.0372e-06, 1.0272e-06, 5.4622e-06, 2.9240e-06,\n",
      "        2.1263e-06, 3.4461e-06, 2.0130e-06, 2.9054e-06, 1.4583e-06, 1.9111e-06,\n",
      "        1.2645e-06, 1.4591e-06, 4.2699e-06, 2.8688e-06, 1.2415e-06, 3.2400e-06,\n",
      "        9.4330e-07, 2.7615e-06, 3.7666e-06, 1.1842e-06, 1.6039e-06, 1.3309e-06,\n",
      "        1.7421e-06, 1.5287e-06, 1.9018e-06, 2.3241e-06, 1.3441e-06, 1.7441e-07,\n",
      "        1.2995e-06, 8.7593e-07, 1.2889e-06, 1.0619e-06], device='cuda:0')}, 8: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-7.1174e-04,  7.2123e-04, -7.3162e-05,  ..., -2.4679e-03,\n",
      "          3.4754e-03, -1.0647e-03],\n",
      "        [ 4.0397e-04,  2.4023e-02, -3.0689e-03,  ...,  5.9578e-03,\n",
      "          4.7273e-03,  2.2783e-03],\n",
      "        [ 3.0886e-03,  4.4198e-03, -4.6706e-04,  ...,  1.1049e-03,\n",
      "         -4.1390e-03,  1.1746e-03],\n",
      "        ...,\n",
      "        [-2.6728e-03, -1.7891e-02, -3.4279e-03,  ...,  5.8098e-03,\n",
      "         -4.3984e-03, -8.2910e-03],\n",
      "        [-7.7937e-04,  7.0478e-03, -9.8843e-04,  ...,  1.6623e-03,\n",
      "          9.7732e-03,  2.6076e-04],\n",
      "        [ 1.3288e-03, -3.7775e-03,  1.3508e-03,  ..., -2.7779e-03,\n",
      "         -4.4309e-03,  1.0047e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.1694e-05, 5.8300e-05, 2.2406e-06,  ..., 3.7505e-06, 2.0092e-05,\n",
      "         1.4365e-05],\n",
      "        [2.9383e-05, 2.8493e-04, 3.1410e-05,  ..., 1.1518e-05, 1.6731e-04,\n",
      "         9.4452e-06],\n",
      "        [3.1398e-05, 1.6327e-04, 1.1479e-05,  ..., 1.7723e-05, 1.4538e-04,\n",
      "         2.4003e-05],\n",
      "        ...,\n",
      "        [3.0229e-05, 1.4531e-04, 1.2963e-05,  ..., 5.6566e-06, 8.1298e-05,\n",
      "         3.6048e-05],\n",
      "        [1.5517e-05, 7.0314e-05, 4.6687e-06,  ..., 6.0864e-06, 2.6372e-05,\n",
      "         8.7674e-06],\n",
      "        [2.2484e-05, 1.1981e-04, 3.1877e-05,  ..., 4.5790e-06, 1.7651e-04,\n",
      "         1.8568e-05]], device='cuda:0')}, 9: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 4.2475e-04,  6.5168e-04, -1.3725e-03,  4.3809e-04,  1.4419e-03,\n",
      "        -1.6819e-03,  2.1708e-04, -7.1564e-04, -7.7725e-04, -1.0481e-03,\n",
      "        -2.3884e-03, -1.6075e-03,  1.0201e-03, -1.1486e-03,  1.6200e-04,\n",
      "         1.1045e-03,  1.6530e-03,  8.9472e-04,  2.8168e-04,  1.6240e-03,\n",
      "         3.5194e-04, -7.1380e-04, -7.3182e-05,  9.6115e-04,  1.1903e-03,\n",
      "        -1.0103e-03, -8.3657e-04,  2.3341e-04,  8.2783e-04,  1.0965e-03,\n",
      "         1.4688e-03,  5.3350e-04,  3.6693e-03,  1.2666e-04, -1.9662e-04,\n",
      "         1.0227e-03, -9.2857e-05,  6.0212e-04,  3.2424e-04,  6.6736e-04,\n",
      "        -7.9866e-04,  1.2829e-03,  1.1217e-04, -6.8597e-05,  2.5483e-04,\n",
      "        -5.5735e-04,  2.4603e-04,  1.7837e-03, -1.3042e-04, -1.8571e-04,\n",
      "        -1.2148e-04, -1.4364e-03, -2.8703e-03,  9.5949e-04,  8.9408e-04,\n",
      "         2.9032e-04,  6.4781e-04,  6.0092e-05, -1.0687e-03, -5.4466e-04,\n",
      "         3.9397e-04, -8.0678e-04,  5.0366e-04, -1.4425e-03], device='cuda:0'), 'exp_avg_sq': tensor([6.5422e-06, 7.3644e-06, 1.0258e-05, 2.2340e-06, 4.6647e-06, 5.8545e-06,\n",
      "        3.1415e-06, 9.2636e-06, 4.6681e-06, 3.2654e-06, 5.0806e-06, 2.3771e-06,\n",
      "        4.8301e-06, 6.1500e-06, 3.7274e-06, 1.8868e-06, 7.9507e-06, 3.6181e-06,\n",
      "        3.9373e-06, 7.5713e-06, 6.3186e-06, 3.1775e-06, 1.2245e-05, 2.0675e-06,\n",
      "        3.1207e-06, 7.7072e-06, 5.5936e-06, 7.0796e-06, 8.7437e-06, 4.8139e-06,\n",
      "        1.0883e-05, 6.3055e-06, 1.2438e-05, 7.9617e-06, 3.1992e-06, 2.8525e-06,\n",
      "        1.8883e-06, 6.8301e-06, 3.2061e-06, 4.6860e-06, 2.5403e-06, 3.4072e-06,\n",
      "        7.5699e-06, 3.5197e-06, 3.3579e-06, 2.6192e-06, 1.1370e-06, 5.9079e-06,\n",
      "        2.8934e-06, 4.6916e-06, 5.9203e-06, 3.2054e-06, 1.1530e-05, 2.6140e-06,\n",
      "        4.0537e-06, 5.4461e-06, 1.0230e-05, 5.3972e-06, 3.5172e-06, 5.9270e-06,\n",
      "        1.5196e-06, 5.7454e-06, 3.9383e-06, 3.0240e-06], device='cuda:0')}, 10: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 0.0014,  0.0157,  0.0071,  ...,  0.0095,  0.0035, -0.0020],\n",
      "        [ 0.0005, -0.0010, -0.0031,  ..., -0.0051, -0.0015, -0.0135],\n",
      "        [ 0.0011,  0.0032,  0.0002,  ...,  0.0002,  0.0020, -0.0018],\n",
      "        ...,\n",
      "        [-0.0013, -0.0012,  0.0013,  ...,  0.0039,  0.0005, -0.0063],\n",
      "        [ 0.0046,  0.0166, -0.0183,  ...,  0.0075,  0.0057, -0.0082],\n",
      "        [-0.0048,  0.0109, -0.0007,  ...,  0.0031, -0.0038,  0.0064]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[3.0052e-05, 3.4367e-04, 1.3513e-04,  ..., 2.2426e-04, 1.7059e-05,\n",
      "         1.3588e-04],\n",
      "        [1.8418e-05, 1.4495e-04, 6.3923e-05,  ..., 2.3020e-04, 1.2075e-05,\n",
      "         2.5697e-04],\n",
      "        [1.6996e-05, 1.0792e-04, 5.8241e-05,  ..., 3.5268e-05, 1.8442e-06,\n",
      "         2.1441e-04],\n",
      "        ...,\n",
      "        [1.5936e-05, 2.0981e-04, 1.8656e-04,  ..., 2.3466e-04, 1.2799e-05,\n",
      "         5.1361e-04],\n",
      "        [2.9097e-05, 3.7156e-04, 4.6122e-04,  ..., 3.4306e-04, 2.0480e-05,\n",
      "         5.1975e-04],\n",
      "        [1.9942e-05, 1.1164e-04, 6.0649e-05,  ..., 4.9474e-05, 1.2801e-05,\n",
      "         5.7045e-05]], device='cuda:0')}, 11: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 0.0012, -0.0004,  0.0016, -0.0037, -0.0006,  0.0011,  0.0004,  0.0017,\n",
      "         0.0015,  0.0005,  0.0032, -0.0014, -0.0024, -0.0032,  0.0046, -0.0020,\n",
      "        -0.0025,  0.0008,  0.0009, -0.0001,  0.0040,  0.0028, -0.0025,  0.0042,\n",
      "        -0.0031, -0.0042, -0.0009,  0.0002,  0.0023, -0.0002,  0.0023, -0.0001],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([3.6078e-05, 4.3379e-05, 1.5882e-05, 1.2423e-05, 3.9161e-05, 3.2228e-05,\n",
      "        1.1956e-05, 1.6863e-05, 2.8551e-05, 7.2619e-05, 1.6737e-05, 1.2164e-05,\n",
      "        4.7892e-05, 6.1168e-05, 1.5445e-05, 3.6881e-05, 2.4465e-05, 4.8280e-05,\n",
      "        1.1296e-05, 1.3070e-05, 2.3183e-05, 3.7512e-05, 3.3716e-05, 4.5776e-05,\n",
      "        1.7060e-05, 2.6700e-05, 2.3407e-05, 1.0906e-05, 2.8537e-05, 3.4552e-05,\n",
      "        6.5498e-05, 1.5077e-05], device='cuda:0')}, 12: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 5.5867e-03,  6.4343e-03,  3.5592e-03,  ...,  1.4023e-03,\n",
      "          1.1587e-02, -3.5292e-03],\n",
      "        [-7.0586e-03, -8.2417e-04,  1.8652e-03,  ..., -1.2375e-04,\n",
      "          1.6585e-03,  1.7177e-03],\n",
      "        [-4.1020e-03, -5.8897e-03, -2.3451e-04,  ..., -4.8800e-03,\n",
      "          4.7283e-03, -6.6054e-04],\n",
      "        ...,\n",
      "        [ 5.2394e-03,  5.0035e-03,  1.4857e-02,  ...,  3.7123e-02,\n",
      "          1.7495e-02,  2.6912e-04],\n",
      "        [-2.5002e-04,  2.0217e-03,  1.7813e-03,  ...,  8.7852e-03,\n",
      "          1.3489e-02,  4.3133e-03],\n",
      "        [ 1.5282e-02,  1.1173e-02,  5.9831e-03,  ...,  2.5282e-03,\n",
      "          8.0669e-05,  1.1202e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[3.0213e-04, 2.4354e-05, 2.6690e-05,  ..., 2.5865e-04, 3.3627e-05,\n",
      "         1.1224e-05],\n",
      "        [4.2960e-05, 3.5264e-06, 1.1841e-06,  ..., 2.9234e-05, 2.9311e-06,\n",
      "         6.0167e-06],\n",
      "        [8.8614e-05, 4.4392e-05, 2.7976e-05,  ..., 6.9691e-05, 3.7510e-05,\n",
      "         1.1641e-05],\n",
      "        ...,\n",
      "        [8.3155e-04, 1.1012e-04, 5.7052e-05,  ..., 2.7489e-04, 2.3547e-04,\n",
      "         3.7142e-05],\n",
      "        [1.8898e-04, 3.3292e-05, 2.3364e-05,  ..., 7.1343e-05, 6.3013e-05,\n",
      "         1.0090e-05],\n",
      "        [2.5998e-04, 8.1081e-05, 1.9227e-04,  ..., 1.8948e-04, 8.1440e-05,\n",
      "         5.2163e-05]], device='cuda:0')}, 13: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 4.1985e-03,  1.8585e-04, -1.8944e-04, -1.9678e-03, -2.2914e-03,\n",
      "         1.2855e-03,  7.5970e-03,  3.2352e-04,  3.4648e-03,  3.8172e-03,\n",
      "         5.4379e-04,  1.2686e-03, -7.6197e-03,  3.9908e-03,  2.3333e-03,\n",
      "        -6.3945e-05, -2.9904e-03, -1.5594e-03, -1.4197e-04, -2.8398e-03,\n",
      "         5.6048e-03, -2.0680e-03,  2.3115e-03,  1.6873e-03, -1.4992e-03,\n",
      "         6.5483e-03, -1.5969e-04, -1.9271e-03, -4.0922e-03,  1.4260e-02,\n",
      "         1.2620e-02, -5.9467e-06], device='cuda:0'), 'exp_avg_sq': tensor([4.5207e-05, 8.9930e-06, 3.8341e-05, 4.2269e-05, 6.1425e-05, 1.7678e-05,\n",
      "        9.0728e-05, 8.6697e-05, 1.1482e-05, 5.5756e-05, 6.6596e-05, 1.2650e-05,\n",
      "        8.6471e-05, 4.9600e-05, 3.1730e-05, 7.2270e-05, 2.8551e-04, 4.6787e-05,\n",
      "        8.8116e-05, 1.7081e-05, 1.9831e-04, 9.6861e-05, 3.5338e-05, 8.4707e-05,\n",
      "        9.6558e-06, 4.4523e-05, 2.0165e-05, 6.5198e-05, 1.7714e-05, 1.9830e-04,\n",
      "        3.7399e-05, 2.0310e-04], device='cuda:0')}, 14: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[[ 2.1962e-03],\n",
      "         [ 8.1478e-04],\n",
      "         [ 9.9806e-04],\n",
      "         ...,\n",
      "         [ 1.9013e-03],\n",
      "         [ 5.9584e-04],\n",
      "         [ 1.1855e-03]],\n",
      "\n",
      "        [[-5.0935e-05],\n",
      "         [-2.4955e-04],\n",
      "         [-2.9881e-04],\n",
      "         ...,\n",
      "         [-1.6408e-04],\n",
      "         [ 6.6579e-04],\n",
      "         [ 8.7679e-05]],\n",
      "\n",
      "        [[ 4.3487e-03],\n",
      "         [ 8.3144e-04],\n",
      "         [ 1.1937e-03],\n",
      "         ...,\n",
      "         [ 2.9281e-03],\n",
      "         [ 1.0543e-03],\n",
      "         [ 7.5090e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4569e-03],\n",
      "         [ 6.2571e-04],\n",
      "         [ 7.2322e-04],\n",
      "         ...,\n",
      "         [ 2.0349e-03],\n",
      "         [ 5.8567e-04],\n",
      "         [ 1.5110e-03]],\n",
      "\n",
      "        [[ 4.4913e-03],\n",
      "         [ 6.4389e-04],\n",
      "         [ 4.3598e-04],\n",
      "         ...,\n",
      "         [ 3.1676e-03],\n",
      "         [ 5.7814e-04],\n",
      "         [ 6.3784e-04]],\n",
      "\n",
      "        [[ 1.8344e-04],\n",
      "         [ 2.5326e-05],\n",
      "         [ 7.8968e-06],\n",
      "         ...,\n",
      "         [ 2.1144e-04],\n",
      "         [ 1.5368e-05],\n",
      "         [ 1.6286e-04]]], device='cuda:0'), 'exp_avg_sq': tensor([[[3.4308e-05],\n",
      "         [3.9470e-06],\n",
      "         [3.3494e-06],\n",
      "         ...,\n",
      "         [2.4107e-05],\n",
      "         [1.5080e-06],\n",
      "         [3.6027e-06]],\n",
      "\n",
      "        [[4.5383e-05],\n",
      "         [1.1967e-06],\n",
      "         [2.3025e-06],\n",
      "         ...,\n",
      "         [2.5635e-05],\n",
      "         [1.4530e-06],\n",
      "         [4.3164e-06]],\n",
      "\n",
      "        [[3.4726e-05],\n",
      "         [2.8053e-06],\n",
      "         [3.2035e-06],\n",
      "         ...,\n",
      "         [2.0245e-05],\n",
      "         [1.5836e-06],\n",
      "         [3.1901e-06]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.1930e-05],\n",
      "         [7.1891e-07],\n",
      "         [9.0873e-07],\n",
      "         ...,\n",
      "         [8.2645e-06],\n",
      "         [3.2456e-07],\n",
      "         [1.0023e-06]],\n",
      "\n",
      "        [[2.0359e-05],\n",
      "         [2.8830e-06],\n",
      "         [2.2731e-06],\n",
      "         ...,\n",
      "         [1.6779e-05],\n",
      "         [1.3294e-06],\n",
      "         [2.6468e-06]],\n",
      "\n",
      "        [[2.7919e-08],\n",
      "         [1.7263e-09],\n",
      "         [5.7685e-09],\n",
      "         ...,\n",
      "         [2.5127e-08],\n",
      "         [3.1860e-09],\n",
      "         [8.9279e-09]]], device='cuda:0')}, 15: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 2.1698e-03,  7.4045e-04,  2.4069e-03,  1.2350e-03,  2.9425e-04,\n",
      "         7.7621e-03, -6.1585e-03, -2.5590e-03,  1.0889e-03,  4.7105e-03,\n",
      "         2.4996e-03,  8.3691e-03,  2.4621e-04,  2.4451e-04,  1.0842e-04,\n",
      "         2.7473e-03, -1.7003e-03, -4.1019e-03,  7.2297e-04, -5.0023e-04,\n",
      "         1.0665e-03, -1.3737e-03, -4.6980e-03, -3.0183e-04, -1.0425e-03,\n",
      "        -5.0449e-03,  2.0173e-03,  3.9916e-03,  5.4867e-05,  5.5241e-03,\n",
      "        -1.0200e-03, -3.0649e-05,  4.5350e-03,  1.9505e-04,  1.6883e-03,\n",
      "        -4.7290e-03,  6.5594e-04, -1.5280e-04, -3.1646e-03, -3.2076e-03,\n",
      "        -2.3007e-04,  3.3467e-03, -2.1686e-04,  1.4442e-03, -1.1559e-03,\n",
      "        -4.4977e-04, -4.4913e-03,  2.3112e-03,  1.1539e-06, -6.2945e-06,\n",
      "        -4.3294e-04, -8.3558e-04, -1.4630e-03, -2.8704e-03, -4.6728e-04,\n",
      "        -4.1159e-05,  2.0023e-03, -1.1342e-03, -4.3897e-03, -2.8450e-03,\n",
      "        -3.5912e-04,  3.7683e-03,  1.4429e-03,  5.0280e-04], device='cuda:0'), 'exp_avg_sq': tensor([3.2732e-05, 3.5095e-05, 2.0446e-05, 7.7330e-06, 6.4777e-06, 5.9128e-05,\n",
      "        5.4839e-05, 1.8194e-05, 3.0075e-06, 3.0532e-05, 4.9779e-05, 4.3087e-05,\n",
      "        6.4621e-06, 2.7749e-06, 3.3686e-08, 1.5095e-05, 2.0595e-06, 1.6314e-05,\n",
      "        4.6711e-06, 3.6835e-05, 4.9720e-06, 5.8129e-05, 1.2387e-05, 4.3473e-07,\n",
      "        1.2511e-05, 2.5703e-05, 1.6613e-05, 3.5462e-05, 7.4658e-08, 6.9890e-05,\n",
      "        1.1501e-05, 3.8732e-07, 5.0610e-05, 8.9431e-06, 1.7797e-05, 1.3626e-05,\n",
      "        5.8275e-05, 8.1859e-06, 3.2353e-05, 1.9206e-05, 3.9217e-06, 1.8761e-05,\n",
      "        2.3226e-05, 2.2441e-05, 1.9469e-05, 4.5309e-07, 2.2891e-05, 2.9935e-05,\n",
      "        3.1931e-09, 1.1104e-10, 2.3845e-05, 9.4539e-06, 1.4717e-05, 5.1821e-05,\n",
      "        6.5520e-07, 3.9713e-08, 3.2438e-06, 5.0758e-05, 3.9570e-05, 3.2587e-05,\n",
      "        9.5207e-06, 1.0542e-05, 2.1618e-05, 1.4112e-07], device='cuda:0')}, 16: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 2.5588e-06, -8.0213e-06,  2.3678e-05,  ..., -4.2923e-05,\n",
      "         -7.1870e-05,  2.2106e-05],\n",
      "        [ 3.4148e-05, -1.4609e-05,  6.2637e-05,  ..., -5.7242e-05,\n",
      "         -9.6146e-05,  1.0038e-05],\n",
      "        [-3.8128e-05,  1.3694e-05, -5.4260e-05,  ...,  5.8743e-05,\n",
      "          8.9143e-05, -2.2399e-05],\n",
      "        ...,\n",
      "        [-3.6983e-05,  3.9920e-06, -5.4765e-05,  ...,  7.3900e-05,\n",
      "          8.3138e-05, -3.6052e-06],\n",
      "        [-1.0500e-05,  9.8834e-06, -1.9578e-05,  ...,  3.5346e-05,\n",
      "          4.8091e-05, -3.0575e-06],\n",
      "        [ 2.3178e-06,  5.7743e-07,  1.4818e-05,  ..., -2.0998e-05,\n",
      "         -3.8931e-05,  1.2364e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[2.4157e-09, 2.2995e-09, 4.6249e-09,  ..., 1.0553e-09, 1.6827e-09,\n",
      "         1.4140e-10],\n",
      "        [5.5135e-09, 4.9647e-09, 1.0454e-08,  ..., 1.8700e-09, 2.7727e-09,\n",
      "         1.7921e-10],\n",
      "        [5.0486e-09, 4.4422e-09, 9.6284e-09,  ..., 1.7264e-09, 2.4298e-09,\n",
      "         1.8021e-10],\n",
      "        ...,\n",
      "        [5.4384e-09, 4.9882e-09, 1.0693e-08,  ..., 1.9291e-09, 2.4154e-09,\n",
      "         1.6761e-10],\n",
      "        [1.8642e-09, 1.8891e-09, 3.6620e-09,  ..., 6.7413e-10, 9.2390e-10,\n",
      "         5.9988e-11],\n",
      "        [5.6842e-10, 6.3675e-10, 1.3170e-09,  ..., 3.3615e-10, 5.5151e-10,\n",
      "         4.5601e-11]], device='cuda:0')}, 17: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-5.0081e-05,  1.4926e-05, -6.7031e-06,  1.1261e-05, -2.9497e-06,\n",
      "         1.5241e-07, -1.0650e-05,  5.6751e-06, -5.1275e-06,  9.7981e-06,\n",
      "         1.7086e-06, -1.2205e-05,  6.2396e-06, -7.7994e-06,  8.3721e-06,\n",
      "         1.0940e-05, -7.4733e-06, -2.3900e-06, -3.7487e-05,  6.5354e-06,\n",
      "        -1.8212e-05, -1.3768e-05,  7.5895e-06, -4.4438e-07, -2.5056e-05,\n",
      "         1.1047e-05, -9.4996e-06, -3.7627e-05, -1.9941e-05, -1.9526e-05,\n",
      "         1.5068e-05, -1.8235e-05, -9.1455e-06, -1.6288e-05, -1.4994e-05,\n",
      "        -1.7895e-05, -1.1632e-06, -1.0876e-05, -2.0075e-08, -1.1586e-05,\n",
      "        -6.1340e-06, -3.5608e-06,  1.3445e-06, -1.0365e-05, -8.0026e-06,\n",
      "        -1.0019e-05,  1.0892e-05, -7.3912e-06,  8.0868e-06, -7.4825e-06,\n",
      "        -2.1809e-06,  1.1613e-05, -9.9335e-05,  1.4345e-05,  1.0726e-05,\n",
      "        -5.7498e-06,  1.6080e-05,  1.6100e-05, -1.0539e-05, -6.2054e-06,\n",
      "        -7.5932e-06, -1.1616e-05,  4.2893e-06, -3.5526e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.8301e-09, 1.8331e-10, 1.7184e-10, 4.8864e-11, 4.0710e-11, 6.3852e-11,\n",
      "        1.7368e-11, 7.2943e-11, 1.8729e-12, 2.1153e-09, 2.3482e-13, 1.7403e-11,\n",
      "        2.9764e-12, 7.3777e-11, 5.2187e-12, 7.8654e-11, 5.2663e-12, 7.2376e-13,\n",
      "        2.7798e-09, 3.2167e-10, 7.3947e-11, 4.7338e-11, 4.4755e-11, 1.3662e-11,\n",
      "        7.7109e-10, 8.9752e-11, 4.1917e-10, 9.2374e-10, 1.6084e-10, 8.8446e-11,\n",
      "        1.0501e-10, 2.2235e-10, 5.9506e-12, 5.1671e-11, 1.2793e-09, 5.3413e-10,\n",
      "        2.6896e-12, 3.5194e-11, 3.2780e-12, 3.0880e-11, 3.7674e-10, 2.4815e-12,\n",
      "        3.1006e-11, 7.1966e-12, 8.7216e-12, 1.1015e-10, 2.2193e-11, 1.1429e-10,\n",
      "        4.0444e-12, 3.5224e-10, 1.0326e-10, 8.3919e-10, 9.3850e-09, 2.3373e-10,\n",
      "        1.3756e-10, 3.7807e-11, 6.1168e-11, 6.4190e-11, 5.4674e-10, 3.2997e-12,\n",
      "        1.4517e-10, 1.1668e-10, 9.3613e-11, 8.2920e-10], device='cuda:0')}, 18: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 5.3092e-04,  8.6477e-05, -2.3864e-05,  5.4166e-04,  4.5614e-04,\n",
      "         -2.4574e-04, -2.3370e-04,  8.7990e-05,  6.3983e-04, -1.6659e-05,\n",
      "         -4.0829e-04,  5.9922e-04, -7.6050e-04, -1.9525e-04, -7.3392e-05,\n",
      "         -2.6838e-04, -8.5122e-05, -4.9381e-04,  6.4896e-04, -1.1098e-04,\n",
      "          3.1147e-04, -2.8519e-04,  2.4384e-05, -6.1772e-05, -5.8446e-04,\n",
      "          6.5431e-04, -6.0028e-04, -6.8497e-04, -5.0420e-04,  5.2809e-04,\n",
      "         -4.1938e-04, -6.8916e-04, -7.1769e-04,  6.9273e-04,  2.5804e-04,\n",
      "          2.9443e-04,  5.2084e-04, -8.3064e-04, -3.7023e-06,  1.0299e-04,\n",
      "          3.1854e-04,  1.7324e-04, -5.5090e-04,  1.3149e-04, -3.9080e-04,\n",
      "          1.5651e-04,  1.2510e-04,  1.8652e-04,  3.3487e-04,  2.0938e-07,\n",
      "          4.1866e-05, -1.0865e-04, -9.4901e-04, -5.9112e-05, -2.3058e-04,\n",
      "         -2.9796e-04, -3.1943e-04, -2.5909e-04, -2.3262e-04,  5.1259e-04,\n",
      "         -3.7127e-04,  7.5352e-04, -3.6493e-04, -4.5810e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[4.0654e-07, 4.0741e-07, 4.1881e-07, 2.8630e-07, 8.1457e-07, 7.2042e-08,\n",
      "         6.0423e-08, 5.7726e-07, 1.7379e-07, 5.6093e-07, 3.2125e-07, 8.3094e-08,\n",
      "         2.5110e-07, 2.5892e-07, 1.6328e-07, 1.9930e-07, 1.2597e-07, 4.6790e-07,\n",
      "         6.7294e-07, 7.1817e-07, 1.0227e-07, 1.8271e-07, 6.5900e-08, 3.8942e-07,\n",
      "         6.9224e-07, 2.8801e-07, 5.7880e-07, 6.6750e-07, 3.1555e-07, 1.8387e-07,\n",
      "         2.8319e-07, 8.6284e-07, 2.5580e-07, 1.7512e-07, 4.5656e-07, 6.9114e-07,\n",
      "         1.5157e-07, 1.3679e-06, 1.1709e-07, 1.0014e-07, 4.3479e-07, 1.1113e-07,\n",
      "         5.7249e-07, 1.8516e-07, 9.8684e-08, 7.4335e-07, 3.1730e-07, 3.1843e-07,\n",
      "         2.9287e-07, 2.3319e-07, 1.3878e-07, 1.3005e-06, 1.3360e-06, 5.5719e-07,\n",
      "         1.9676e-07, 1.1281e-07, 7.3116e-08, 4.3259e-07, 6.7138e-07, 1.7906e-07,\n",
      "         2.3709e-07, 5.5291e-07, 1.3236e-07, 4.0198e-07]], device='cuda:0')}, 19: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-5.6096e-06], device='cuda:0'), 'exp_avg_sq': tensor([1.9756e-12], device='cuda:0')}, 20: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 3.6237e-03,  1.6270e-04,  1.7079e-02,  ..., -5.7400e-04,\n",
      "          8.2586e-04, -2.4806e-05],\n",
      "        [-2.1220e-02,  1.3760e-03, -9.4988e-04,  ..., -2.1404e-03,\n",
      "          5.6277e-04, -9.2687e-05],\n",
      "        [ 8.5389e-03,  1.0569e-03, -1.1574e-02,  ...,  1.0654e-03,\n",
      "         -4.2141e-04,  1.2360e-04],\n",
      "        ...,\n",
      "        [ 7.6999e-04, -4.3681e-04,  1.8176e-04,  ...,  9.4464e-04,\n",
      "          1.9068e-04,  9.5006e-05],\n",
      "        [ 7.4580e-03,  6.2540e-03,  2.1937e-02,  ...,  4.4547e-03,\n",
      "          3.5250e-03,  1.5512e-04],\n",
      "        [ 5.5407e-03, -1.1239e-03, -1.8830e-02,  ...,  2.6345e-03,\n",
      "         -5.4063e-04,  3.4554e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[9.2998e-05, 3.9168e-06, 4.9404e-04,  ..., 9.4804e-06, 2.9715e-05,\n",
      "         1.6629e-08],\n",
      "        [2.1367e-04, 7.9488e-06, 6.0878e-04,  ..., 1.1609e-05, 2.2076e-05,\n",
      "         4.1081e-08],\n",
      "        [4.8216e-05, 7.3526e-06, 3.2965e-04,  ..., 3.2723e-06, 5.6029e-06,\n",
      "         1.3261e-08],\n",
      "        ...,\n",
      "        [1.4722e-06, 5.7345e-07, 1.2828e-06,  ..., 1.0492e-06, 8.2383e-07,\n",
      "         1.6083e-09],\n",
      "        [2.2393e-04, 1.0078e-05, 3.0397e-04,  ..., 1.2694e-05, 8.7898e-06,\n",
      "         2.2992e-08],\n",
      "        [2.3100e-04, 6.5034e-06, 2.5151e-04,  ..., 9.9705e-06, 1.6627e-05,\n",
      "         2.2748e-08]], device='cuda:0')}, 21: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-0.0014, -0.0059,  0.0035,  0.0002,  0.0065, -0.0005, -0.0083, -0.0070,\n",
      "        -0.0030,  0.0144, -0.0050,  0.0086,  0.0020,  0.0043,  0.0033,  0.0086,\n",
      "         0.0015, -0.0089,  0.0024,  0.0139, -0.0070, -0.0083,  0.0019, -0.0011,\n",
      "        -0.0039, -0.0114, -0.0033,  0.0007,  0.0055,  0.0005, -0.0088,  0.0091,\n",
      "         0.0032,  0.0081,  0.0010,  0.0232,  0.0032,  0.0056,  0.0036, -0.0113,\n",
      "        -0.0013, -0.0021, -0.0018, -0.0030,  0.0031, -0.0044, -0.0010,  0.0020,\n",
      "         0.0024, -0.0052,  0.0103,  0.0016, -0.0019, -0.0079, -0.0005,  0.0055,\n",
      "         0.0185,  0.0041, -0.0021,  0.0073,  0.0195,  0.0038,  0.0149,  0.0071],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([6.2731e-04, 5.3194e-04, 8.0491e-05, 1.1077e-06, 1.3338e-04, 6.6268e-05,\n",
      "        9.1708e-05, 5.6400e-05, 1.8491e-04, 3.0442e-04, 8.5595e-05, 9.1036e-05,\n",
      "        7.2798e-05, 1.3635e-04, 2.7807e-05, 1.3860e-04, 1.5460e-04, 1.1687e-04,\n",
      "        1.0888e-04, 6.3538e-04, 4.3780e-05, 1.4223e-03, 2.4945e-04, 2.2684e-04,\n",
      "        7.5860e-05, 8.4017e-05, 2.9586e-04, 3.6358e-04, 1.3707e-04, 7.1821e-05,\n",
      "        1.3120e-04, 4.3426e-04, 1.7788e-04, 5.4106e-04, 3.4852e-05, 4.3031e-04,\n",
      "        3.8740e-04, 2.2255e-04, 3.7314e-05, 1.3608e-04, 7.4123e-06, 5.2100e-06,\n",
      "        1.5892e-04, 6.0044e-06, 1.1131e-04, 6.9277e-05, 8.3063e-05, 5.8017e-05,\n",
      "        4.8226e-06, 3.5914e-05, 1.5275e-04, 1.0306e-04, 1.6886e-04, 1.4649e-04,\n",
      "        6.1120e-05, 2.0883e-04, 2.6919e-04, 1.0540e-04, 6.0230e-05, 2.8456e-05,\n",
      "        4.2320e-04, 1.9854e-05, 2.7584e-04, 3.2744e-04], device='cuda:0')}, 22: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-6.1049e-03, -6.6066e-04, -8.8669e-04,  ..., -2.9342e-03,\n",
      "         -2.5106e-03,  1.0670e-02],\n",
      "        [-3.2123e-03, -3.8389e-03, -7.2085e-03,  ..., -1.0071e-03,\n",
      "         -1.8399e-03, -4.4993e-03],\n",
      "        [-2.5072e-03, -5.0593e-04, -2.9998e-03,  ..., -3.9254e-04,\n",
      "          6.8676e-05, -1.7632e-03],\n",
      "        ...,\n",
      "        [-2.1548e-04,  4.6592e-03, -6.9282e-04,  ...,  3.5017e-04,\n",
      "          5.0070e-04, -7.9736e-03],\n",
      "        [-3.5906e-03,  1.9572e-05, -4.5617e-05,  ..., -8.5597e-04,\n",
      "          7.2417e-06,  6.1231e-03],\n",
      "        [-1.7263e-03,  5.7296e-04, -4.3641e-04,  ..., -1.7018e-04,\n",
      "         -3.9567e-05,  4.0307e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[6.3413e-05, 2.0080e-05, 2.9456e-05,  ..., 6.5880e-06, 1.1467e-05,\n",
      "         1.4391e-04],\n",
      "        [7.9653e-05, 2.5276e-05, 6.4117e-05,  ..., 8.6946e-07, 9.0010e-06,\n",
      "         6.1987e-05],\n",
      "        [3.1917e-06, 1.0899e-06, 3.7625e-06,  ..., 4.1557e-08, 8.4161e-07,\n",
      "         1.1167e-05],\n",
      "        ...,\n",
      "        [9.1408e-05, 4.0041e-05, 7.6467e-05,  ..., 5.2718e-06, 3.2459e-06,\n",
      "         1.4211e-04],\n",
      "        [6.3250e-06, 1.1379e-08, 2.2275e-06,  ..., 4.5723e-07, 3.2399e-12,\n",
      "         1.1108e-05],\n",
      "        [4.5926e-06, 1.3314e-06, 8.1281e-07,  ..., 1.2015e-07, 4.1060e-07,\n",
      "         2.1462e-05]], device='cuda:0')}, 23: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([ 4.5282e-02, -2.5882e-02, -7.8645e-03, -8.3118e-06,  1.8346e-02,\n",
      "         1.4177e-02,  6.7042e-03,  9.3513e-03,  6.3676e-03, -2.0429e-03,\n",
      "        -3.0163e-03,  3.2151e-02,  2.8600e-02, -1.5601e-02, -6.4315e-03,\n",
      "         8.6440e-04,  3.9262e-02,  1.9528e-03, -2.7442e-03,  2.1561e-02,\n",
      "        -8.1947e-03,  6.0862e-03, -1.0623e-02,  4.3047e-02, -3.4747e-03,\n",
      "         1.6646e-02,  2.3619e-02,  4.4814e-03, -2.1212e-02,  1.2266e-02,\n",
      "         2.1929e-03, -1.6951e-03], device='cuda:0'), 'exp_avg_sq': tensor([1.3957e-03, 8.1683e-04, 5.6594e-05, 4.2384e-12, 7.9640e-04, 5.3765e-04,\n",
      "        3.1491e-04, 5.3917e-04, 6.8418e-04, 2.9982e-05, 1.0997e-04, 9.0084e-04,\n",
      "        1.4179e-03, 2.5119e-04, 1.9294e-04, 7.4251e-04, 1.1128e-03, 3.7745e-04,\n",
      "        7.9820e-04, 1.1342e-03, 2.2600e-04, 4.4057e-03, 3.7837e-04, 1.5682e-03,\n",
      "        3.8121e-05, 2.0591e-04, 1.0050e-03, 1.7847e-04, 1.1772e-03, 2.6137e-03,\n",
      "        6.0284e-05, 4.6103e-05], device='cuda:0')}, 24: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-5.8440e-05, -1.6243e-04, -2.1123e-04,  1.1218e-05, -1.7842e-04,\n",
      "         -6.8056e-04, -2.6775e-05, -1.1828e-03, -8.7630e-04, -4.2691e-06,\n",
      "         -2.4892e-03, -6.6339e-03, -3.2996e-03,  7.4141e-05, -3.4871e-03,\n",
      "         -2.1431e-04, -2.8488e-03, -2.3680e-03, -1.6812e-03, -6.5061e-03,\n",
      "         -3.2361e-03, -4.5930e-04, -2.5083e-03, -1.4350e-03,  4.6633e-05,\n",
      "         -5.6981e-05, -4.0974e-03, -2.4640e-03,  1.5973e-05,  2.2330e-05,\n",
      "          3.6733e-05, -2.2785e-04],\n",
      "        [ 1.4175e-03,  3.6876e-03, -1.3322e-03, -1.0929e-05,  2.6031e-03,\n",
      "          4.4008e-04, -9.0656e-04,  8.2412e-03,  7.1254e-03,  1.0270e-04,\n",
      "         -1.7516e-03,  4.2838e-03, -1.1757e-03, -1.1685e-03,  2.5355e-03,\n",
      "         -4.4031e-04,  1.5291e-02,  8.4527e-05, -1.8179e-03,  4.4373e-03,\n",
      "         -1.2765e-03, -2.8607e-03, -1.9063e-03, -9.6198e-04, -5.5654e-04,\n",
      "          2.0640e-03, -2.0109e-03,  5.0145e-03,  7.1935e-04, -5.3259e-04,\n",
      "         -2.7909e-04,  2.5608e-05],\n",
      "        [ 4.7228e-06,  9.7917e-06, -1.4501e-05,  9.1567e-06,  2.8606e-06,\n",
      "          3.3375e-06,  1.3562e-05, -9.5227e-06, -1.6212e-05, -8.1906e-06,\n",
      "          5.5760e-06, -8.9589e-06, -8.7552e-06,  1.0732e-05,  1.0921e-08,\n",
      "         -1.3805e-05, -1.1674e-05,  1.6616e-06, -8.8335e-06, -4.3365e-06,\n",
      "         -1.3947e-05,  1.0387e-05,  1.6474e-05, -1.5112e-05,  7.8263e-06,\n",
      "         -1.4027e-05, -1.4701e-05,  1.3673e-06, -1.6418e-05,  8.1224e-06,\n",
      "         -3.0154e-06, -1.1908e-05],\n",
      "        [ 1.0023e-03,  1.6806e-02, -3.1231e-02, -5.8529e-06,  3.8573e-02,\n",
      "         -2.4442e-02, -6.3045e-03,  3.7917e-02, -1.2047e-02,  7.4999e-04,\n",
      "         -3.8255e-03, -1.5818e-02, -1.7737e-02, -1.6598e-02, -8.9027e-03,\n",
      "         -9.6768e-03,  4.6012e-02, -2.4348e-02,  4.3028e-03, -5.3201e-02,\n",
      "         -6.4423e-02, -2.1744e-02, -1.0329e-02,  9.9034e-03,  1.5772e-03,\n",
      "          1.1530e-02, -9.2430e-03,  1.0923e-02,  1.1420e-03, -9.8877e-03,\n",
      "         -1.4498e-03,  7.6978e-04],\n",
      "        [-4.0904e-04, -8.4676e-03,  4.3422e-03, -1.2479e-05, -6.2326e-03,\n",
      "          7.4624e-03,  3.3000e-03, -6.2930e-03,  4.8094e-03, -2.1563e-04,\n",
      "          1.5444e-03,  2.0548e-03,  5.2698e-03, -1.0175e-03,  7.8900e-03,\n",
      "          3.3466e-03, -1.1037e-02,  6.1459e-03, -8.5610e-03,  1.4768e-02,\n",
      "          7.9954e-03,  8.3193e-03,  8.0992e-04, -1.7465e-03,  3.1564e-05,\n",
      "         -6.0735e-04,  1.1827e-02, -3.7011e-03, -9.1641e-04,  9.5734e-04,\n",
      "          4.6546e-04, -1.4483e-03],\n",
      "        [ 1.3345e-03, -6.2708e-04,  3.9028e-03,  9.0533e-06, -1.4579e-05,\n",
      "         -3.7155e-03,  2.4258e-03, -1.1196e-03,  8.7108e-03, -1.2229e-05,\n",
      "         -1.6696e-03, -2.9552e-03,  8.6319e-03,  5.2592e-03,  4.2778e-03,\n",
      "          4.4749e-03, -1.5747e-04,  1.8809e-03,  2.1229e-03, -9.6983e-04,\n",
      "          4.3448e-03,  3.9454e-03,  2.5962e-04,  6.0490e-04,  5.3067e-04,\n",
      "          1.6805e-03,  8.0457e-03, -8.2399e-04, -5.6419e-04,  4.2035e-03,\n",
      "          7.3809e-04, -1.9301e-03],\n",
      "        [ 2.8432e-04,  1.5697e-04,  4.7613e-04,  1.1738e-05, -4.1093e-06,\n",
      "          1.2067e-03,  7.0026e-05,  2.9490e-05,  5.0162e-04,  7.4829e-06,\n",
      "          1.3038e-04,  3.1261e-03,  1.2012e-03,  4.1453e-04,  2.1337e-03,\n",
      "          3.5389e-04,  1.0749e-03,  1.6968e-03,  4.4689e-05,  4.4375e-03,\n",
      "          3.6202e-03,  5.1315e-05,  1.3845e-04,  4.7504e-05, -1.7013e-06,\n",
      "          2.7689e-05,  7.2561e-04,  2.2478e-03,  3.0767e-04,  7.1503e-05,\n",
      "         -4.4684e-06,  8.0120e-05],\n",
      "        [ 3.2388e-04,  1.7012e-03, -3.1046e-03,  5.6190e-06,  3.6680e-03,\n",
      "         -3.1365e-03, -6.1616e-04,  3.1457e-03, -3.4964e-03,  8.5709e-05,\n",
      "         -6.3534e-04, -4.6890e-03, -3.3929e-03, -2.1133e-03, -1.5598e-03,\n",
      "         -1.4670e-03,  3.1353e-03, -4.2100e-03,  3.5601e-04, -8.0221e-03,\n",
      "         -8.1567e-03, -2.2996e-03, -1.9565e-03,  9.0178e-05,  1.9958e-04,\n",
      "          3.8541e-04, -1.4271e-03,  6.8024e-04, -9.0556e-05, -7.9125e-04,\n",
      "         -1.5457e-04,  5.6350e-05],\n",
      "        [ 3.8263e-03,  2.0733e-02, -9.1062e-03,  1.4987e-05,  2.2481e-02,\n",
      "          1.3012e-02, -3.3957e-03,  3.6308e-02,  1.7341e-02,  5.3660e-04,\n",
      "         -7.0952e-03,  2.6446e-02,  2.9512e-04, -5.6230e-03,  1.1024e-03,\n",
      "          7.8224e-03,  4.3026e-02, -7.9867e-03,  3.3255e-02,  9.0179e-03,\n",
      "         -1.3563e-02, -1.5253e-02, -2.2395e-03,  8.8931e-03,  7.5328e-04,\n",
      "          1.2854e-02,  1.6510e-02,  2.2494e-02,  5.2746e-05, -2.3106e-04,\n",
      "         -6.1188e-04, -1.6764e-03],\n",
      "        [-7.2736e-04, -1.0836e-02,  2.0177e-02, -3.9526e-06, -2.5524e-02,\n",
      "          1.5063e-02,  4.0980e-03, -2.5238e-02,  7.3061e-03, -4.8215e-04,\n",
      "          9.4402e-04,  5.4578e-03,  9.0711e-03,  1.0429e-02,  3.8586e-03,\n",
      "          6.1422e-03, -3.1512e-02,  1.4353e-02, -4.1526e-03,  3.0471e-02,\n",
      "          3.9673e-02,  1.3889e-02,  4.6869e-03, -7.6846e-03, -1.0010e-03,\n",
      "         -7.4595e-03,  3.1241e-03, -8.7026e-03, -7.2522e-04,  6.3964e-03,\n",
      "          9.1525e-04, -7.0516e-04],\n",
      "        [-9.9148e-05, -9.4734e-04, -1.4095e-03,  8.3146e-06, -8.1929e-05,\n",
      "         -1.8461e-03, -2.2078e-06, -3.5645e-04, -5.7862e-04, -1.6396e-05,\n",
      "          2.7723e-09, -1.3976e-03, -1.1136e-03,  5.3189e-05, -1.0539e-03,\n",
      "         -4.6312e-04, -3.7240e-04, -8.9386e-06, -1.4356e-03, -1.8270e-03,\n",
      "         -2.1981e-03, -4.7569e-06, -6.9808e-04,  9.4420e-06,  8.5278e-05,\n",
      "         -5.8721e-05, -1.5614e-03, -1.3464e-03, -4.1975e-04, -1.2094e-04,\n",
      "          5.3934e-06, -1.1267e-05],\n",
      "        [-1.6113e-03,  2.5957e-03, -4.8099e-03, -6.8056e-06,  7.1767e-03,\n",
      "         -4.8819e-03, -9.2392e-05,  6.7706e-03,  5.2296e-05,  3.0372e-04,\n",
      "          6.3604e-04, -2.8175e-03,  4.2151e-05, -3.0469e-03,  3.1484e-03,\n",
      "         -1.9233e-03,  7.8706e-03, -7.0913e-03,  3.9976e-03, -5.2154e-03,\n",
      "         -4.0425e-03, -3.1710e-03, -1.6918e-03,  2.5404e-03, -6.9205e-05,\n",
      "          6.6639e-04,  2.3251e-03,  4.5119e-03,  1.9541e-03, -4.3276e-04,\n",
      "         -2.3543e-04, -3.5811e-05],\n",
      "        [-1.2616e-05,  4.8414e-06,  6.6718e-06, -3.2924e-06,  1.5374e-05,\n",
      "         -1.4729e-05,  1.6018e-05,  1.3924e-05, -1.4234e-05,  1.2219e-05,\n",
      "         -6.3932e-06, -4.4335e-06, -2.6790e-06,  5.1587e-07, -1.1850e-05,\n",
      "         -9.7774e-06, -1.5898e-05, -9.7762e-06,  8.4290e-06, -1.9328e-07,\n",
      "          5.2413e-06, -2.5499e-07,  9.9445e-06,  4.5216e-06, -1.0780e-05,\n",
      "         -8.1937e-06,  1.0320e-05, -9.6790e-06, -1.6355e-05, -1.0285e-05,\n",
      "         -1.0289e-05,  5.5184e-06],\n",
      "        [-1.1424e-05,  4.3500e-07, -8.7543e-06,  1.1682e-05, -9.1288e-06,\n",
      "          9.2190e-06,  6.6571e-06, -5.4371e-06,  1.2472e-05, -7.9150e-06,\n",
      "          1.3670e-05, -1.1748e-05, -1.6942e-05, -6.7885e-06,  7.4992e-09,\n",
      "          7.6023e-06,  3.4027e-06, -6.6531e-06, -6.0711e-07, -1.2112e-05,\n",
      "          1.5471e-05,  3.9184e-06, -1.6725e-05, -6.4673e-06, -5.8640e-06,\n",
      "         -1.2173e-06,  8.2369e-06, -1.3400e-05,  1.5784e-05,  1.3442e-06,\n",
      "         -1.1670e-05, -8.9744e-06],\n",
      "        [-2.7007e-04, -1.9506e-03,  5.0527e-03,  1.5014e-05, -4.6543e-03,\n",
      "          2.6493e-03,  7.0062e-04, -5.1895e-03,  3.4260e-04, -1.4617e-04,\n",
      "         -1.0955e-03, -6.2723e-03, -4.9344e-04,  1.9114e-03,  9.5334e-05,\n",
      "          1.5649e-03, -9.8817e-03,  5.0952e-04,  4.3666e-04,  8.6689e-04,\n",
      "          6.0933e-03,  3.1831e-03, -4.3810e-04, -2.1985e-03,  3.8968e-05,\n",
      "         -1.4940e-03, -2.0292e-03, -3.6151e-03,  5.0754e-05,  1.9442e-03,\n",
      "          3.5307e-04, -8.4382e-04],\n",
      "        [-1.5719e-04, -9.7790e-06, -9.9546e-04,  1.5909e-06, -1.0270e-05,\n",
      "         -5.4007e-04, -3.4121e-05, -3.2497e-04, -2.8123e-04, -1.1368e-05,\n",
      "         -1.2450e-03, -1.1085e-03, -3.3772e-04, -1.0346e-03, -1.8435e-03,\n",
      "          1.7766e-06, -6.8717e-04, -1.2674e-03, -1.2860e-03, -2.6537e-03,\n",
      "         -2.2441e-03, -3.5104e-06, -9.9403e-06, -7.2778e-06,  3.6770e-06,\n",
      "         -7.9508e-05, -5.5799e-04, -2.0964e-04, -3.0644e-04, -8.3096e-04,\n",
      "          4.8763e-07, -9.7919e-05]], device='cuda:0'), 'exp_avg_sq': tensor([[3.6453e-08, 2.8779e-08, 2.1961e-08, 7.6258e-12, 1.3174e-07, 4.0728e-07,\n",
      "         8.6370e-09, 4.4713e-07, 1.5203e-06, 2.8129e-11, 1.0007e-06, 8.7703e-06,\n",
      "         2.0978e-06, 1.2692e-09, 2.0030e-06, 9.4998e-08, 2.3621e-06, 1.2242e-06,\n",
      "         9.3768e-07, 8.9931e-06, 1.6390e-06, 1.2685e-07, 1.3194e-06, 4.9092e-07,\n",
      "         2.6684e-09, 6.1500e-09, 3.2891e-06, 1.0463e-06, 1.9805e-08, 1.0456e-08,\n",
      "         8.0807e-10, 1.4021e-07],\n",
      "        [7.5396e-07, 1.7364e-05, 9.0043e-06, 7.2449e-12, 3.6215e-05, 1.0215e-04,\n",
      "         9.9903e-06, 8.2608e-05, 8.9547e-05, 1.2930e-07, 1.4679e-05, 9.0342e-05,\n",
      "         5.4145e-05, 1.2857e-05, 8.6861e-05, 4.3321e-05, 5.1927e-05, 1.9319e-05,\n",
      "         1.2870e-04, 2.2526e-04, 1.1891e-04, 3.9111e-05, 5.7899e-05, 1.6300e-05,\n",
      "         4.8996e-07, 1.1307e-05, 1.1266e-04, 6.9352e-05, 1.8484e-05, 9.9382e-06,\n",
      "         3.2691e-07, 3.7595e-06],\n",
      "        [1.4179e-12, 5.8401e-12, 1.2641e-11, 5.1215e-12, 5.4799e-13, 7.3212e-13,\n",
      "         1.1078e-11, 5.5299e-12, 1.5754e-11, 4.1186e-12, 1.9519e-12, 4.9073e-12,\n",
      "         4.6915e-12, 6.9907e-12, 6.0986e-16, 1.1473e-11, 8.2480e-12, 2.0250e-13,\n",
      "         4.7739e-12, 1.2041e-12, 1.1706e-11, 6.5562e-12, 1.6261e-11, 1.3714e-11,\n",
      "         3.7688e-12, 1.1839e-11, 1.2988e-11, 1.4339e-13, 1.6153e-11, 4.0520e-12,\n",
      "         6.0484e-13, 8.5758e-12],\n",
      "        [8.2582e-05, 6.0979e-04, 5.5945e-04, 2.1436e-12, 1.0215e-03, 1.9617e-03,\n",
      "         3.6602e-04, 1.9272e-03, 1.6909e-03, 4.2838e-06, 4.0926e-04, 1.3482e-03,\n",
      "         1.1945e-03, 8.8887e-04, 2.0943e-03, 1.1924e-03, 1.2960e-03, 8.3886e-04,\n",
      "         2.3089e-03, 2.6974e-03, 7.3280e-03, 1.0722e-03, 1.5819e-03, 6.1666e-04,\n",
      "         1.7292e-05, 4.2048e-04, 2.0557e-03, 2.4264e-03, 4.7950e-04, 3.4781e-04,\n",
      "         9.5823e-06, 1.1628e-04],\n",
      "        [1.3688e-06, 4.5590e-05, 2.9977e-05, 9.4035e-12, 1.5940e-04, 5.4978e-04,\n",
      "         6.7262e-05, 4.6188e-04, 8.5393e-04, 4.2684e-07, 3.6841e-05, 6.6536e-04,\n",
      "         5.9243e-04, 3.8446e-05, 6.4958e-04, 1.7917e-04, 2.0031e-04, 3.5488e-05,\n",
      "         7.8459e-04, 2.5394e-03, 5.1126e-04, 1.3859e-04, 1.9738e-04, 5.9021e-05,\n",
      "         1.6874e-06, 2.6640e-05, 1.1504e-03, 3.8930e-04, 9.8108e-05, 3.6046e-05,\n",
      "         1.0442e-06, 1.2094e-05],\n",
      "        [5.6619e-07, 7.6241e-07, 4.0577e-05, 5.0090e-12, 1.2776e-11, 1.3345e-05,\n",
      "         5.3336e-06, 1.8450e-05, 1.0837e-04, 9.0356e-12, 2.5007e-05, 2.8975e-05,\n",
      "         7.2949e-05, 1.3408e-04, 1.4267e-04, 5.1401e-06, 4.4668e-06, 4.7163e-05,\n",
      "         8.9106e-05, 6.4706e-04, 6.5212e-04, 1.1776e-05, 1.4081e-08, 4.3635e-07,\n",
      "         1.0113e-07, 2.1936e-05, 4.3215e-05, 8.2960e-06, 3.1663e-06, 2.2843e-05,\n",
      "         2.1987e-07, 8.4921e-06],\n",
      "        [2.1343e-08, 2.0384e-07, 5.0456e-08, 8.3367e-12, 5.2590e-10, 1.1814e-06,\n",
      "         1.7181e-07, 2.3419e-09, 1.3359e-07, 3.4533e-12, 5.2227e-08, 5.6216e-06,\n",
      "         1.0040e-06, 8.3979e-07, 2.1626e-06, 2.3400e-07, 5.9227e-07, 6.4609e-07,\n",
      "         9.0019e-07, 1.0814e-05, 6.8604e-06, 9.6070e-10, 1.0177e-07, 1.2563e-07,\n",
      "         2.1124e-13, 1.8963e-10, 1.3642e-06, 3.6746e-06, 1.9940e-07, 3.5237e-08,\n",
      "         5.1028e-11, 6.4494e-09],\n",
      "        [7.4476e-07, 5.3815e-06, 5.1797e-06, 1.9811e-12, 8.9220e-06, 1.6844e-05,\n",
      "         3.1426e-06, 1.7091e-05, 1.5480e-05, 4.0977e-08, 3.8061e-06, 1.1441e-05,\n",
      "         1.1202e-05, 8.0732e-06, 1.9110e-05, 1.0355e-05, 1.1277e-05, 7.9280e-06,\n",
      "         2.0169e-05, 2.7004e-05, 6.7841e-05, 9.2343e-06, 1.3571e-05, 5.3497e-06,\n",
      "         1.6724e-07, 3.6599e-06, 1.8046e-05, 2.0863e-05, 4.1185e-06, 3.0199e-06,\n",
      "         9.0679e-08, 1.0274e-06],\n",
      "        [2.3565e-05, 2.2363e-04, 1.5617e-04, 1.3490e-11, 5.3762e-04, 1.2214e-03,\n",
      "         2.5032e-04, 8.8894e-04, 2.7114e-03, 1.9876e-06, 1.4467e-04, 2.0334e-03,\n",
      "         1.7058e-03, 1.3068e-04, 1.8272e-03, 6.4312e-04, 7.9691e-04, 2.3940e-04,\n",
      "         1.2531e-03, 6.3452e-03, 2.6259e-03, 4.0675e-04, 6.0265e-04, 4.0140e-04,\n",
      "         8.1025e-06, 1.0371e-04, 2.8617e-03, 1.6205e-03, 2.3166e-04, 1.0174e-04,\n",
      "         3.9572e-06, 1.9337e-05],\n",
      "        [3.5191e-05, 2.6038e-04, 2.3698e-04, 1.0089e-12, 4.3551e-04, 8.4148e-04,\n",
      "         1.5649e-04, 8.2383e-04, 7.1564e-04, 1.8043e-06, 1.7158e-04, 5.8870e-04,\n",
      "         5.0715e-04, 3.7971e-04, 8.8696e-04, 5.1057e-04, 5.5388e-04, 3.5465e-04,\n",
      "         9.8310e-04, 1.1632e-03, 3.1103e-03, 4.5907e-04, 6.7243e-04, 2.6294e-04,\n",
      "         7.2732e-06, 1.7889e-04, 8.7553e-04, 1.0372e-03, 2.0480e-04, 1.4808e-04,\n",
      "         4.0521e-06, 4.9326e-05],\n",
      "        [3.9716e-09, 3.4171e-07, 3.0901e-07, 4.2412e-12, 3.7319e-08, 8.0181e-07,\n",
      "         5.7361e-11, 6.3437e-08, 2.6900e-07, 1.6109e-11, 6.3730e-17, 3.9524e-07,\n",
      "         6.2530e-07, 1.2199e-07, 3.9865e-07, 2.1034e-07, 3.5756e-07, 2.5682e-08,\n",
      "         7.8141e-07, 5.9607e-07, 1.5820e-06, 1.4376e-12, 3.5914e-07, 5.4385e-12,\n",
      "         2.5191e-09, 2.4220e-08, 5.3688e-07, 7.1841e-07, 8.9651e-08, 3.9289e-08,\n",
      "         1.8305e-12, 7.6919e-12],\n",
      "        [3.6848e-06, 2.4999e-05, 7.6206e-06, 2.8713e-12, 4.6520e-05, 2.1860e-04,\n",
      "         1.5204e-05, 1.5680e-04, 3.3831e-04, 2.1605e-07, 8.5428e-06, 3.6952e-04,\n",
      "         1.7293e-04, 1.5734e-05, 1.4552e-04, 1.1659e-04, 1.4986e-04, 2.7959e-05,\n",
      "         2.8131e-04, 8.8789e-04, 1.8244e-04, 3.0556e-05, 9.1095e-05, 3.2199e-05,\n",
      "         5.1982e-08, 1.0663e-05, 3.2267e-04, 1.5815e-04, 2.4842e-05, 9.6542e-06,\n",
      "         4.1326e-07, 3.3081e-06],\n",
      "        [9.6081e-12, 1.4870e-12, 2.7627e-12, 7.1360e-13, 1.4187e-11, 1.3036e-11,\n",
      "         1.5384e-11, 1.1669e-11, 1.2186e-11, 9.0220e-12, 2.5432e-12, 1.2561e-12,\n",
      "         4.8487e-13, 2.9415e-14, 8.4930e-12, 5.8235e-12, 1.5157e-11, 5.8220e-12,\n",
      "         4.3560e-12, 8.3138e-15, 1.7323e-12, 1.1420e-14, 6.0201e-12, 1.3044e-12,\n",
      "         7.0527e-12, 4.1217e-12, 6.4735e-12, 5.7091e-12, 1.6031e-11, 6.4313e-12,\n",
      "         6.4354e-12, 1.9132e-12],\n",
      "        [7.9039e-12, 2.2991e-14, 4.6905e-12, 8.2582e-12, 5.0910e-12, 5.1900e-12,\n",
      "         2.7509e-12, 1.8592e-12, 9.3935e-12, 3.8526e-12, 1.1252e-11, 8.3511e-12,\n",
      "         1.7188e-11, 2.8573e-12, 3.1165e-16, 3.5614e-12, 7.5938e-13, 2.7477e-12,\n",
      "         3.7578e-14, 8.8673e-12, 1.4365e-11, 9.9235e-13, 1.6755e-11, 2.6008e-12,\n",
      "         2.1514e-12, 1.1717e-13, 4.1642e-12, 1.0819e-11, 1.4944e-11, 1.3919e-13,\n",
      "         8.2414e-12, 4.9239e-12],\n",
      "        [1.5555e-06, 1.1381e-05, 1.0716e-05, 1.3539e-11, 1.9514e-05, 4.1353e-05,\n",
      "         7.0664e-06, 3.7626e-05, 3.2959e-05, 7.9371e-08, 7.9190e-06, 4.9460e-05,\n",
      "         2.9048e-05, 1.7594e-05, 4.1033e-05, 2.3399e-05, 2.6147e-05, 1.6441e-05,\n",
      "         4.6597e-05, 7.2361e-05, 1.3859e-04, 2.1383e-05, 3.2012e-05, 1.1844e-05,\n",
      "         2.5467e-07, 7.4935e-06, 4.3902e-05, 4.5273e-05, 8.7994e-06, 6.6233e-06,\n",
      "         1.6297e-07, 2.1130e-06],\n",
      "        [1.0209e-07, 1.2350e-10, 2.4986e-06, 1.8737e-13, 6.4126e-12, 9.8279e-07,\n",
      "         4.1175e-09, 3.7941e-07, 2.4830e-07, 7.8277e-12, 5.1160e-06, 3.7800e-06,\n",
      "         2.6299e-07, 2.2060e-06, 8.6085e-06, 2.2834e-13, 1.6985e-06, 3.8711e-06,\n",
      "         6.0196e-06, 2.3272e-05, 1.6940e-05, 8.0549e-13, 6.0151e-12, 3.2714e-12,\n",
      "         8.7944e-13, 2.1034e-08, 7.0987e-07, 1.9163e-07, 4.3057e-07, 1.3768e-06,\n",
      "         2.7085e-14, 2.6966e-08]], device='cuda:0')}, 25: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-7.9438e-03,  2.8339e-02, -8.5604e-06,  1.1182e-02,  4.9502e-03,\n",
      "         9.1165e-03,  4.9161e-03, -3.6068e-03,  1.5708e-01, -1.2591e-02,\n",
      "        -6.1936e-03,  1.7676e-02, -1.1712e-05, -4.9066e-06, -8.5304e-03,\n",
      "        -3.7456e-03], device='cuda:0'), 'exp_avg_sq': tensor([1.5581e-05, 1.3495e-03, 4.4896e-12, 2.4066e-05, 1.8606e-02, 1.8412e-03,\n",
      "        1.7124e-05, 6.4345e-06, 4.3906e-02, 3.0606e-05, 8.9101e-06, 5.8425e-03,\n",
      "        8.3004e-12, 1.5257e-12, 4.0731e-05, 3.4347e-05], device='cuda:0')}, 26: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[-2.2379e-06, -9.2561e-07,  4.1678e-06,  5.1120e-06, -1.8145e-05,\n",
      "         -6.5024e-06,  5.8177e-06,  1.5071e-05, -9.9897e-06, -1.8193e-05,\n",
      "         -2.0630e-05, -1.4822e-06, -8.1664e-06,  1.3567e-05, -2.0148e-05,\n",
      "         -2.4130e-05],\n",
      "        [-1.1422e-02,  5.2696e-04,  9.3968e-06,  5.1072e-03,  4.7007e-03,\n",
      "         -7.7942e-03, -2.9803e-03,  2.7340e-03, -2.0156e-02, -7.9650e-03,\n",
      "         -5.3788e-04,  7.1510e-04, -2.2096e-05,  1.4601e-05,  2.0833e-03,\n",
      "          5.4542e-04],\n",
      "        [-2.1181e-06, -9.0052e-06, -1.2170e-05,  1.9594e-05,  5.1183e-06,\n",
      "         -4.0362e-06,  8.4191e-06,  1.7467e-06, -4.8211e-06,  6.1841e-06,\n",
      "          2.2217e-05,  9.2381e-06, -1.8391e-05, -2.3072e-05, -5.7308e-06,\n",
      "          7.5077e-06],\n",
      "        [ 6.4676e-06,  1.3616e-05, -1.5175e-05, -1.9721e-05,  1.4831e-05,\n",
      "         -8.1838e-06,  2.1874e-05, -1.2118e-05, -2.1720e-05,  2.2180e-07,\n",
      "         -8.5429e-06,  1.2991e-05,  3.2919e-06, -2.6434e-06, -1.3303e-05,\n",
      "          1.5222e-05],\n",
      "        [ 1.6818e-06, -1.9934e-05,  1.0341e-05, -2.2195e-05,  2.1973e-05,\n",
      "         -7.6568e-06,  7.3082e-06, -1.4868e-05, -1.8734e-05,  1.6946e-05,\n",
      "         -1.1962e-05, -6.3243e-06, -1.8565e-05, -2.1474e-05, -2.0766e-05,\n",
      "          6.1842e-06],\n",
      "        [-1.1757e-02,  4.8850e-04,  1.4080e-06,  5.2443e-03,  4.8708e-03,\n",
      "         -8.0262e-03, -3.0419e-03,  2.8491e-03, -2.0690e-02, -8.1683e-03,\n",
      "         -5.4203e-04,  7.2244e-04, -1.9873e-05, -7.9561e-06,  2.1732e-03,\n",
      "          5.8142e-04],\n",
      "        [ 7.2505e-03, -5.1987e-03,  5.5805e-06, -2.6567e-02, -5.1421e-03,\n",
      "          7.1798e-03,  2.7469e-03, -1.9180e-02,  5.1021e-03,  3.1597e-03,\n",
      "          4.9437e-04, -3.3933e-03, -4.6398e-06, -1.4894e-05, -3.1441e-03,\n",
      "         -5.1774e-04],\n",
      "        [-1.0120e-02,  4.5211e-04, -1.6131e-06,  4.5269e-03,  4.2160e-03,\n",
      "         -6.9059e-03, -2.6340e-03,  2.4355e-03, -1.7818e-02, -7.0703e-03,\n",
      "         -4.7824e-04,  6.0953e-04,  3.5479e-06,  1.1056e-05,  1.8546e-03,\n",
      "          4.7624e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[3.4759e-13, 7.3693e-14, 1.1162e-12, 1.6509e-12, 1.9683e-11, 2.6282e-12,\n",
      "         2.1187e-12, 1.3641e-11, 6.0738e-12, 1.9788e-11, 2.5376e-11, 1.6527e-13,\n",
      "         4.0949e-12, 1.1087e-11, 2.4217e-11, 3.4620e-11],\n",
      "        [5.6728e-05, 9.3633e-04, 5.3876e-12, 2.5108e-04, 6.7739e-04, 6.0350e-05,\n",
      "         2.8378e-06, 6.9782e-04, 5.4869e-04, 4.5999e-04, 2.6241e-06, 2.1665e-04,\n",
      "         2.9073e-11, 1.2814e-11, 5.0507e-04, 8.7626e-07],\n",
      "        [3.1422e-13, 4.9570e-12, 8.9501e-12, 2.2915e-11, 1.6549e-12, 1.0500e-12,\n",
      "         4.3460e-12, 2.2147e-13, 1.4751e-12, 2.3845e-12, 2.9392e-11, 5.2110e-12,\n",
      "         2.0216e-11, 3.1675e-11, 2.0580e-12, 3.4756e-12],\n",
      "        [2.6010e-12, 1.1165e-11, 1.3827e-11, 2.3211e-11, 1.3216e-11, 4.1119e-12,\n",
      "         2.8497e-11, 8.8756e-12, 2.8102e-11, 9.6952e-15, 4.4717e-12, 1.0178e-11,\n",
      "         7.1340e-13, 4.7294e-13, 1.0666e-11, 1.3912e-11],\n",
      "        [2.0693e-13, 2.3709e-11, 6.4997e-12, 2.9334e-11, 2.8755e-11, 3.6113e-12,\n",
      "         3.2980e-12, 1.3281e-11, 2.0968e-11, 1.7196e-11, 8.6521e-12, 2.4904e-12,\n",
      "         2.0596e-11, 2.7474e-11, 2.5710e-11, 2.3846e-12],\n",
      "        [5.9363e-05, 9.7304e-04, 1.5097e-13, 2.6176e-04, 7.0743e-04, 6.3489e-05,\n",
      "         2.9941e-06, 7.2609e-04, 5.6990e-04, 4.7943e-04, 2.7664e-06, 2.2534e-04,\n",
      "         2.3567e-11, 3.8917e-12, 5.2332e-04, 9.2706e-07],\n",
      "        [4.6448e-05, 8.0067e-04, 1.9550e-12, 3.0973e-04, 5.7838e-04, 5.1401e-05,\n",
      "         2.4194e-06, 6.2702e-04, 4.9431e-04, 3.9875e-04, 2.2431e-06, 1.8138e-04,\n",
      "         1.3705e-12, 1.3326e-11, 4.2758e-04, 7.4909e-07],\n",
      "        [4.4242e-05, 7.2713e-04, 1.9205e-13, 1.9534e-04, 5.2754e-04, 4.7220e-05,\n",
      "         2.2244e-06, 5.4201e-04, 4.2589e-04, 3.5792e-04, 2.0553e-06, 1.6833e-04,\n",
      "         8.2185e-13, 7.4112e-12, 3.9124e-04, 6.8663e-07]], device='cuda:0')}, 27: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([-1.5308e-05,  1.9962e-06, -1.7681e-05, -8.5066e-06, -1.5551e-05,\n",
      "         1.5507e-05, -5.1584e-02,  2.0858e-05], device='cuda:0'), 'exp_avg_sq': tensor([1.4067e-11, 3.5179e-13, 1.8701e-11, 4.4346e-12, 1.4511e-11, 1.4755e-11,\n",
      "        5.1274e-04, 2.6268e-11], device='cuda:0')}, 28: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([[ 8.2560e-06,  2.5795e-03, -7.2830e-06, -2.9981e-05,  3.3703e-05,\n",
      "          5.4095e-03,  1.1957e-02,  3.2437e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[4.1830e-12, 1.4525e-03, 3.2759e-12, 5.3271e-11, 6.7219e-11, 1.7606e-03,\n",
      "         5.6927e-04, 1.1544e-03]], device='cuda:0')}, 29: {'step': tensor(60., device='cuda:0'), 'exp_avg': tensor([9.2782e-06], device='cuda:0'), 'exp_avg_sq': tensor([5.0308e-12], device='cuda:0')}}, 'param_groups': [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}]}\n",
      "batch_size 32\n",
      "dropout_ratio 0.3\n",
      "learning_rate 0.0001\n",
      "weight_decay 0.0001\n",
      "n_epochs 5\n",
      "random_seed 0\n",
      "val_c_index 0.6721470019342359\n"
     ]
    }
   ],
   "source": [
    "model_chkpt = FusionNetwork()\n",
    "model_chkpt.to(device)\n",
    "optimizer = optim.Adam(model_chkpt.parameters())\n",
    "\n",
    "# load from last check point\n",
    "checkpoint_path = _\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_chkpt.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "val_c_index = checkpoint['val_c_index']\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical_rna_feedforward.feedforward.0.weight: torch.Size([512, 19975])\n",
      "clinical_rna_feedforward.feedforward.0.bias: torch.Size([512])\n",
      "clinical_rna_feedforward.feedforward.3.weight: torch.Size([256, 512])\n",
      "clinical_rna_feedforward.feedforward.3.bias: torch.Size([256])\n",
      "clinical_rna_feedforward.feedforward.6.weight: torch.Size([256, 256])\n",
      "clinical_rna_feedforward.feedforward.6.bias: torch.Size([256])\n",
      "clinical_rna_feedforward.feedforward.9.weight: torch.Size([64, 256])\n",
      "clinical_rna_feedforward.feedforward.9.bias: torch.Size([64])\n",
      "clinical_rna_feedforward.feedforward.12.weight: torch.Size([64, 64])\n",
      "clinical_rna_feedforward.feedforward.12.bias: torch.Size([64])\n",
      "clinical_rna_feedforward.feedforward.15.weight: torch.Size([32, 64])\n",
      "clinical_rna_feedforward.feedforward.15.bias: torch.Size([32])\n",
      "clinical_rna_feedforward.feedforward.18.weight: torch.Size([32, 32])\n",
      "clinical_rna_feedforward.feedforward.18.bias: torch.Size([32])\n",
      "wsi_fcn.conv.weight: torch.Size([64, 512, 1])\n",
      "wsi_fcn.conv.bias: torch.Size([64])\n",
      "attention.attention.0.weight: torch.Size([64, 64])\n",
      "attention.attention.0.bias: torch.Size([64])\n",
      "attention.attention.2.weight: torch.Size([1, 64])\n",
      "attention.attention.2.bias: torch.Size([1])\n",
      "baby_feed_forward.0.weight: torch.Size([64, 96])\n",
      "baby_feed_forward.0.bias: torch.Size([64])\n",
      "baby_feed_forward.2.weight: torch.Size([32, 64])\n",
      "baby_feed_forward.2.bias: torch.Size([32])\n",
      "baby_feed_forward.4.weight: torch.Size([16, 32])\n",
      "baby_feed_forward.4.bias: torch.Size([16])\n",
      "baby_feed_forward.6.weight: torch.Size([8, 16])\n",
      "baby_feed_forward.6.bias: torch.Size([8])\n",
      "baby_feed_forward.8.weight: torch.Size([1, 8])\n",
      "baby_feed_forward.8.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in checkpoint['model_state_dict'].items():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from utils import display_km_curves_fusion\n",
    "\n",
    "# from models import *\n",
    "# from train import test_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############################# FINAL ###########################################\n",
    "\n",
    "checkpoint_path = _ # TODO: to choose\n",
    "\n",
    "model_chkpt = FusionNetwork()\n",
    "model_chkpt.to(device)\n",
    "optimizer = optim.Adam(model_chkpt.parameters())\n",
    "\n",
    "# load from last check point\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_chkpt.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "val_c_index = checkpoint['val_c_index']\n",
    "\n",
    "model_chkpt.eval()\n",
    "\n",
    "test_risks = []\n",
    "test_times = []\n",
    "test_events = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        # unpack the batch\n",
    "        batch_clinical_rna_features, batch_lists_phenotype_clusters, batch_times, batch_events = batch\n",
    "        \n",
    "        # move times and events to the device\n",
    "        batch_times = batch_times.to(device)\n",
    "        batch_events = batch_events.to(device)\n",
    "        \n",
    "        # tterate over each sample in the batch\n",
    "        for i, (clinical_rna_features, list_of_phenotype_tensors) in enumerate(zip(batch_clinical_rna_features, batch_lists_phenotype_clusters)):\n",
    "            \n",
    "            risk_score = model_chkpt(clinical_rna_features, list_of_phenotype_tensors)\n",
    "            \n",
    "            test_risks.append(risk_score.item())\n",
    "            test_times.append(batch_times[i].item())\n",
    "            test_events.append(batch_events[i].item())\n",
    "\n",
    "test_c_index = concordance_index(test_times, -np.array(test_risks), test_events)\n",
    "print(f\"test c-index: {test_c_index}\")\n",
    "display_km_curves_fusion(test_risks, test_times, test_events, \"test set\", save_figure=True)\n",
    "\n",
    "\n",
    "\n",
    "# append to csv file\n",
    "with open(\"../evaluation-results/c-index-results.csv\", \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"model\", \"c-index\"])\n",
    "    writer.writerow({\"model\": \"fusion_network\", \"c-index\": test_c_index})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_index_custom(hazards, times, events):\n",
    "    \"\"\"\n",
    "    computes the c-index for survival prediction\n",
    "    - hazards: predicted risk scores (higher means higher risk)\n",
    "    - times: observed survival times\n",
    "    - events: event indicators (1 if event occurred, 0 if censored)\n",
    "    \"\"\"\n",
    "    n = len(times)\n",
    "    concordant = 0.0\n",
    "    permissible = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # only compare if i had an event and its time is earlier than j\n",
    "            if times[i] < times[j] and events[i] == 1:\n",
    "                permissible += 1\n",
    "                if hazards[i] > hazards[j]:\n",
    "                    concordant += 1\n",
    "                elif hazards[i] == hazards[j]:\n",
    "                    concordant += 0.5\n",
    "    return concordant / permissible if permissible > 0 else 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
